<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="计算巢是阿里云开放给企业应用服务商的服务管理平台。服务商能够在计算巢上发布私有化部署服务，为其客户提供云上软件一键部署的能力；同时也支持全托管模式的服务，赋能服务商托管其客户资源。">
  <title>基于ACS集群的大模型部署文档 - Aliyun 计算巢 x Demo</title>

  <link rel="shortcut icon" href="../img/favicon.ico">

  <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.0/build/pure-min.css">
  <link rel="stylesheet" href="../css/theme.css">
  

  

  
  

  
    <script src="../search/main.js"></script>
  

  

  <script src="../js/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>

<body>
  <div class="container">
    <div class="nav">
      <div class="nav-inner">
        <div class="logo">
          <img src="./img/logo-2x.png">
        </div>
        <div class="nav-list">
          <ul>
          
              <li><a href="#acs">基于ACS集群的大模型部署文档</a></li>
              
                  <li><a href="#_1">部署说明</a></li>
                  
              
                  <li><a href="#_2">整体架构</a></li>
                  
              
                  <li><a href="#_3">计费说明</a></li>
                  
              
                  <li><a href="#ram">RAM账号所需权限</a></li>
                  
              
                  <li><a href="#_4">部署流程</a></li>
                  
              
                  <li><a href="#_5">使用说明</a></li>
                  
                      <li class="li-h3"><a href="#api">私网API访问</a></li>
                  
                      <li class="li-h3"><a href="#api_1">公网API访问</a></li>
                  
                      <li class="li-h3"><a href="#gpupod">快速更换模型、GPU规格、Pod数量</a></li>
                  
                      <li class="li-h3"><a href="#_6">手动重新部署模型</a></li>
                  
                      <li class="li-h3"><a href="#_7">进阶教程</a></li>
                  
                      <li class="li-h3"><a href="#benchmark">Benchmark</a></li>
                  
              
          
          </ul>
        </div>
      </div>
    </div>
    <div class="content theme-github">
      
      <div class="content-inner">        
        
        <h1 id="acs">基于ACS集群的大模型部署文档</h1>
<h2 id="_1">部署说明</h2>
<p>本方案通过阿里云计算巢服务实现开箱即用的大模型推理服务部署，支持以下场景：
- <strong>新建ACS集群</strong>：用户可以选择直接创建一个ACS集群，计算巢会在用户账号下一键创建ACS集群和OSS Bucket，完成了模型上传和Bucket的挂载后会自动部署模型，最后会自动创建负载均衡实现内、公网的访问。
- <strong>选择已有ACS、ACK集群</strong>：用户可以选择使用已有的ACS或者ACK集群，计算巢会在用户账号下创建OSS Bucket（也可以选择已有的Bucket），完成了模型上传和Bucket的挂载后会自动部署模型，最后会自动创建负载均衡实现内、公网的访问。</p>
<p>本方案基于以下核心组件：</p>
<ul>
<li><strong>vLLM</strong>：提供高性能并行推理能力，支持低延迟、高吞吐的LLM推理（支持Qwen、DeepSeek全系列模型）</li>
<li><strong>SGLang</strong>: SGLang 是一个适用于大语言模型和视觉语言模型的快速服务框架。</li>
<li><strong>ACS集群</strong>：提供全托管的Kubernetes环境，支持Serverless工作负载弹性伸缩</li>
<li><strong>P16EN/GU8TF GPU加速</strong>：支持多种算力规格，满足不同模型规模的推理需求</li>
</ul>
<p>部署后，用户可通过私有/公网API调用模型服务，资源利用率提升数倍，开发者无需关注底层容器编排与资源调度，仅需在计算巢控制台页面选择模型即可完成一键部署。</p>
<h2 id="_2">整体架构</h2>
<p><img alt="arch.png" src="../arch.png" /></p>
<h2 id="_3">计费说明</h2>
<table>
<thead>
<tr>
<th>资源类型</th>
<th>计费模式</th>
<th>关键配置说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>ACS集群</td>
<td>按量付费</td>
<td>根据所选GPU类型和数量计费，GU8TF/GU8TEF/P16EN规格不同价格不同</td>
</tr>
<tr>
<td>ECS跳板机</td>
<td>按量付费</td>
<td>ecs.u1-c1m2.xlarge（4C8G），用于集群管理，部署完成后可安全释放</td>
</tr>
<tr>
<td>OSS存储</td>
<td>按量付费</td>
<td>存储模型文件，建议选择与集群同地域的存储类型</td>
</tr>
<tr>
<td>NAT网关</td>
<td>按量付费</td>
<td>当开启公网访问时自动创建，按使用时长和带宽计费</td>
</tr>
</tbody>
</table>
<h2 id="ram">RAM账号所需权限</h2>
<p>部署实例需要对部分阿里云资源进行访问和创建操作。因此您的账号需要包含如下资源的权限。且需要开通ACS服务，开通后可以在ACS控制台右上角看到：
<strong>开通状态：GPU 按量付费已开通, GPU 容量预留已开通, CPU 按量付费已开通</strong>。</p>
<table>
<thead>
<tr>
<th>权限策略名称</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>AliyunECSFullAccess</td>
<td>管理云服务器服务（ECS）的权限</td>
</tr>
<tr>
<td>AliyunVPCFullAccess</td>
<td>管理专有网络（VPC）的权限</td>
</tr>
<tr>
<td>AliyunROSFullAccess</td>
<td>管理资源编排服务（ROS）的权限</td>
</tr>
<tr>
<td>AliyunCSFullAccess</td>
<td>管理容器服务（CS）的权限</td>
</tr>
<tr>
<td>AliyunComputeNestUserFullAccess</td>
<td>管理计算巢服务（ComputeNest）的用户侧权限</td>
</tr>
<tr>
<td>AliyunOSSFullAccess</td>
<td>管理网络对象存储服务（OSS）的权限</td>
</tr>
</tbody>
</table>
<p>除此之外，<strong>部署前需要联系PDSA添加GPU白名单。</strong></p>
<h2 id="_4">部署流程</h2>
<ol>
<li>
<p>单击<a href="https://computenest.console.aliyun.com/service/instance/create/cn-hangzhou?type=user&amp;ServiceName=LLM%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1-ACS%E7%89%88">部署链接</a>
。根据界面提示填写参数，可以看到对应询价明细，确认参数后点击<strong>下一步：确认订单</strong>。
    <img alt="deploy.png" src="../deploy.png" />
    这里也可以选择已有ACS集群,如下所示：
    <img alt="deploy_existing_cluster.png" src="../deploy_existing_cluster.png" /></p>
</li>
<li>
<p>点击<strong>下一步：确认订单</strong>后可以也看到价格预览，随后点击<strong>立即部署</strong>，等待部署完成。
   <img alt="price.png" src="../price.png" /></p>
</li>
<li>
<p>等待部署完成后就可以开始使用服务，进入服务实例详情查看如何私网访问指导。如果选择了<strong>支持公网访问</strong>，则能看到公网访问指导。
   <img alt="result.png" src="../result.png" /></p>
</li>
</ol>
<h2 id="_5">使用说明</h2>
<h3 id="api">私网API访问</h3>
<ol>
<li>在和服务器同一VPC内的ECS中访问概览页的<strong>私网API地址</strong>。访问示例如下：</li>
</ol>
<pre><code class="language-shell"># 私网有认证请求，流式访问，若想关闭流式访问，删除stream即可。
curl http://{$PrivateIP}:8000/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;Authorization: Bearer ${API_KEY}&quot; \
  -d '{
    &quot;model&quot;: &quot;ds&quot;,
    &quot;messages&quot;: [
      {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;给闺女写一份来自未来2035的信，同时告诉她要好好学习科技，做科技的主人，推动科技，经济发展；她现在是3年级&quot;
      }
    ],
    &quot;max_tokens&quot;: 1024,
    &quot;temperature&quot;: 0,
    &quot;top_p&quot;: 0.9,
    &quot;seed&quot;: 10,
    &quot;stream&quot;: true
  }'
</code></pre>
<h3 id="api_1">公网API访问</h3>
<ol>
<li>如果想通过公网访问API地址，部署时如果选择了<strong>支持公网访问</strong>，则直接通过公网IP访问即可，示例如下：</li>
</ol>
<pre><code class="language-shell">curl http://${PublicIp}:8000/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model&quot;: &quot;ds&quot;,
    &quot;messages&quot;: [
      {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;给闺女写一份来自未来2035的信，同时告诉她要好好学习科技，做科技的主人，推动科技，经济发展；她现在是3年级&quot;
      }
    ],
    &quot;max_tokens&quot;: 1024,
    &quot;temperature&quot;: 0,
    &quot;top_p&quot;: 0.9,
    &quot;seed&quot;: 10,
    &quot;stream&quot;: true
  }'
</code></pre>
<ol>
<li>如果未选择<strong>支持公网访问</strong>，则需要手动在集群中创建一个<code>LoadBalance</code>
，示例如下（deepseek-r1，如果是qwq-32b，labels.app需要改为qwq-32b)：</li>
</ol>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  annotations:
    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type: &quot;internet&quot;
    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-ip-version: ipv4
  labels:
    app: deepseek-r1
  name: svc-public
  namespace: llm-model
spec:
  externalTrafficPolicy: Local
  ports:
    - name: serving
      port: 8000
      protocol: TCP
      targetPort: 8000
  selector:
    app: deepseek-r1
  type: LoadBalancer
</code></pre>
<h3 id="gpupod">快速更换模型、GPU规格、Pod数量</h3>
<p><strong>计算巢支持实例变配功能，可以一键更换模型、GPU规格、Pod数量，具体参考如下说明：</strong></p>
<p>服务实例部署完成后，进入服务实例详情页面，点击右上角的<strong>修改配置</strong>按钮，选择需要修改的参数，修改后可以查看价格和变配前后的参数变化，最后点击<strong>确认</strong>。（变配GPU前请确保原所选地域与可用区有库存）
<img alt="update_instance_1.png" src="../update_instacne_1.png" /></p>
<p><img alt="update_instance_2.png" src="../update_instance_2.png" /></p>
<p><img alt="update_instance_3.png" src="../update_instance_3.png" /></p>
<p><img alt="update_instance_4.png" src="../update_instance_4.png" /></p>
<p>实例状态由<strong>变配中</strong>变为<strong>已成功</strong>表示变配成功。
<img alt="update_instance_5.png" src="../update_instance_5.png" /></p>
<h3 id="_6">手动重新部署模型</h3>
<p><strong>对于不更换模型、仅改变部署参数的情况，可以参考如下说明重新部署模型：</strong></p>
<p>通过跳板机上执行kubectl apply命令或者直接在控制台手动输入模板来重新部署。</p>
<ol>
<li>
<p>跳板机方式</p>
<ol>
<li>进入计算巢控制台服务实例的资源界面，可以看到对应的ECS跳板机，执行<strong>远程连接</strong>，选择免密登录。
   <img alt="resources.png" src="../resources.png" /></li>
<li>进入跳板机后执行命令
     <code>bash
     su root
     # 修改部署参数
     vi /model.yaml 
     # 如果需要更改模型参数，修改了model.yaml后直接执行apply命令即可
     kubectl apply -f /model.yaml</code></li>
</ol>
</li>
<li>
<p>控制台方式</p>
<ol>
<li>进入计算巢控制台，点击<strong>服务实例</strong>，点击<strong>资源</strong>，找到对应的ACS实例，点击进入。
   <img alt="acs.png" src="../acs.png" /></li>
<li>进入ACS控制台后点击<strong>工作负载</strong>，查看<strong>无状态</strong>，以qwq-32b为例：可以看到对应的Deployment。
   <img alt="qwq-deploy.png" src="../qwq-deploy.png" /></li>
<li>点击该Deployment后进入详情页面，点击编辑可以修改一些基本参数，或者点击查看yaml修改后更新。
   <img alt="modify_deploy.png" src="../modify_deploy.png" /></li>
</ol>
</li>
</ol>
<p><strong>对于更换模型的情况，可以参考如下文档：</strong></p>
<p><a href="https://help.aliyun.com/document_detail/2864595.html">ACS集群形态的LLM大模型推理镜像使用指导_PG1阿里云产品-阿里云帮助中心</a>
<a href="https://help.aliyun.com/zh/cs/user-guide/use-acs-gpu-computing-power-to-build-deepseek-full-model-reasoning-service">使用ACS GPU算力构建DeepSeek满血版模型推理服务_容器计算服务(ACS)-阿里云帮助中心</a></p>
<h3 id="_7">进阶教程</h3>
<ul>
<li>除了部署服务实例时可以选择<strong>Fluid配置</strong>，也可以后续自定义配置Fluid实现模型加速</li>
</ul>
<p>Fluid 是一种基于 Kubernetes 原生的分布式数据集编排和加速引擎，旨在优化数据密集型应用（如AI推理、大模型训练等场景）的性能。如果服务需要在弹性伸缩时快速启动，
  可以考虑部署Fluid，具体可以参考文档：<a href="https://help.aliyun.com/zh/cs/user-guide/using-acs-gpu-computing-power-to-build-a-distributed-deepseek-full-blood-version-reasoning-service">Fluid</a>。
  经测试，采用Fluid的加速，根据缓存大小，模型加载速度可以缩短至50%，在应对一些弹性伸缩的场景下，可以快速加载模型，显著提高性能。如下所示，可以仅修改具体的BucketName、ModelName和具体的JindoRuntime参数：</p>
<pre><code class="language-yaml">apiVersion: data.fluid.io/v1alpha1
kind: Dataset
metadata:
  name: llm-model
  namespace: llm-model
spec:
  placement: Shared
  mounts:
    - mountPoint: oss://${BucketName}/llm-model
      options:
        fs.oss.endpoint: oss-${RegionId}-internal.aliyuncs.com
      name: models
      path: &quot;/&quot;
      encryptOptions:
        - name: fs.oss.accessKeyId
          valueFrom:
            secretKeyRef:
              name: oss-secret
              key: akId
        - name: fs.oss.accessKeySecret
          valueFrom:
            secretKeyRef:
              name: oss-secret
              key: akSecret
---
apiVersion: data.fluid.io/v1alpha1
kind: JindoRuntime
metadata:
  name: llm-model
  namespace: llm-model
spec:
  networkmode: ContainerNetwork
  replicas: ${JindoRuntimeReplicas} # 设置副本数,根据实际的模型磁盘占用进行设置
  master:
    podMetadata:
      labels:
        alibabacloud.com/compute-class: performance
        alibabacloud.com/compute-qos: default
  worker:
    podMetadata:
      labels:
        alibabacloud.com/compute-class: performance
        alibabacloud.com/compute-qos: default
      annotations:
        kubernetes.io/resource-type: serverless
    resources:
      requests:
        cpu: 16
        memory: 128Gi
      limits:
        cpu: 16
        memory: 128Gi
  tieredstore:
    levels:
      - mediumtype: MEM
        path: /dev/shm
        volumeType: emptyDir
        quota: 128Gi
        high: &quot;0.99&quot;
        low: &quot;0.95&quot;
---
apiVersion: data.fluid.io/v1alpha1
kind: DataLoad
metadata:
  name: llm-model
  namespace: llm-model
spec:
  dataset:
    name: llm-model
    namespace: llm-model
  loadMetadata: true
</code></pre>
<h3 id="benchmark">Benchmark</h3>
<p>本服务基采用vllm自带的benchmark进行测试，采用的压测数据集：<a href="https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split/files">https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split/files</a>，
整体压测流程：</p>
<ol>
<li>创建一个Deployment，使用vllm-benchmark镜像。在容器中执行数据集下载、压测操作
   使用下面的yaml创建Deployment前需要替换部分参数</li>
</ol>
<table>
<thead>
<tr>
<th>替换参数</th>
<th>参数含义</th>
<th>参数值示例/说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>$POD_IP</code></strong></td>
<td>运行 deepseek-r1 的 Pod IP</td>
<td><code>kubectl get pod -n llm-model -l app=$(kubectl get deployment -n llm-model -l app -o jsonpath='{.items[0].spec.template.metadata.labels.app}') -o jsonpath='{.items[0].status.podIP}'</code></td>
</tr>
<tr>
<td><strong><code>$API_KEY</code></strong></td>
<td>服务认证密钥</td>
<td>在服务实例详情页中获取（形如 <code>sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</code>）</td>
</tr>
<tr>
<td><strong><code>$MODEL_PATH</code></strong></td>
<td>模型存储路径</td>
<td>QwQ-32b: <code>/llm-model/Qwen/QwQ-32B</code><br>Qwen3-32b: <code>/llm-model/Qwen/Qwen3-32B</code><br>Qwen3-235b-A22b: <code>/llm-model/Qwen/Qwen3-235B-A22B</code><br>DeepSeek-R1_671b: <code>/llm-model/deepseek-ai/DeepSeek-R1</code><br>DeepSeek-R1_32b: <code>/llm-model/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B</code><br>DeepSeek-R1_70b: <code>/llm-model/deepseek-ai/DeepSeek-R1-Distill-Llama-70B</code></td>
</tr>
<tr>
<td><strong><code>$SERVED_MODEL_NAME</code></strong></td>
<td>服务部署的模型名称</td>
<td>QwQ-32b: <code>Qwen/QwQ-32B</code><br>Qwen3-32b: <code>Qwen/Qwen3-32B</code><br>Qwen3-235b-A22b: <code>Qwen/Qwen3-235B-A22B</code><br>DeepSeek-R1_671b: <code>deepseek-ai/DeepSeek-R1</code><br>DeepSeek-R1_32b: <code>deepseek-ai/DeepSeek-R1-Distill-Qwen-32B</code><br>DeepSeek-R1_70b: <code>deepseek-ai/DeepSeek-R1-Distill-Llama-70B</code></td>
</tr>
</tbody>
</table>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-benchmark
  namespace: llm-model
  labels:
    app: vllm-benchmark
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-benchmark
  template:
    metadata:
      labels:
        app: vllm-benchmark
    spec:
      volumes:
        - name: llm-model
          persistentVolumeClaim:
            claimName: llm-model
      containers:
        - name: vllm-benchmark
          image: kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/vllm-benchmark:v1
          command:
            - &quot;sh&quot;
            - &quot;-c&quot;
            - |
              # 安装依赖
              yum install -y epel-release &amp;&amp; \
              yum install -y git git-lfs &amp;&amp; \
              git lfs install &amp;&amp;

              # 下载数据集
              git clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git /root/ShareGPT_V3_unfiltered_cleaned_split

              # 执行基准测试
              export OPENAI_API_KEY=$API_KEY
              python3 /root/vllm/benchmarks/benchmark_serving.py \
                --backend vllm \
                --model $MODEL_PATH \
                --served-model-name $SERVED_MODEL_NAME \
                --trust-remote-code \
                --dataset-name sharegpt \
                --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \
                --sonnet-input-len 1024 \
                --sonnet-output-len 6 \
                --sonnet-prefix-len 50 \
                --num-prompts 200 \
                --request-rate 1 \
                --host $POD_IP \
                --port 8000 \
                --endpoint /v1/completions \
                --save-result

              # 保持容器运行
              sleep inf
          volumeMounts:
            - mountPath: /llm-model
              name: llm-model
</code></pre>
<ol>
<li>直接在acs控制台查看容器日志或者进入容器查看容器日志
   <img alt="img.png" src="../console_log.png" /></li>
</ol>
<p>测试结果示例：
    <code>plaintext
    ============ Serving Benchmark Result ============
    Successful requests:                     200       
    Benchmark duration (s):                  272.15    
    Total input tokens:                      43390     
    Total generated tokens:                  39980     
    Request throughput (req/s):              0.73      
    Output token throughput (tok/s):         146.91    
    Total Token throughput (tok/s):          306.34    
    ---------------Time to First Token----------------
    Mean TTFT (ms):                          246.46    
    Median TTFT (ms):                        244.58    
    P99 TTFT (ms):                           342.11    
    -----Time per Output Token (excl. 1st token)------
    Mean TPOT (ms):                          130.30    
    Median TPOT (ms):                        130.12    
    P99 TPOT (ms):                           139.09    
    ---------------Inter-token Latency----------------
    Mean ITL (ms):                           129.89    
    Median ITL (ms):                         125.40    
    P99 ITL (ms):                            173.20    
    ==================================================</code></p>
        
      </div>

      <div class="copyrights">© 2009-2022 Aliyun.com 版权所有</div>
    </div>
  </div>
  
  <!--
  MkDocs version      : 1.6.1
  Docs Build Date UTC : 2025-07-31 09:42:36.248805+00:00
  -->
</body>
</html>