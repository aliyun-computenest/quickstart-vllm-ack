<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="计算巢是阿里云开放给企业应用服务商的服务管理平台。服务商能够在计算巢上发布私有化部署服务，为其客户提供云上软件一键部署的能力；同时也支持全托管模式的服务，赋能服务商托管其客户资源。">
  <title>基于单ECS实例的LLM模型部署文档 - Aliyun 计算巢 x Demo</title>

  <link rel="shortcut icon" href="../img/favicon.ico">

  <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.0/build/pure-min.css">
  <link rel="stylesheet" href="../css/theme.css">
  

  

  
  

  
    <script src="../search/main.js"></script>
  

  

  <script src="../js/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>

<body>
  <div class="container">
    <div class="nav">
      <div class="nav-inner">
        <div class="logo">
          <img src="./img/logo-2x.png">
        </div>
        <div class="nav-list">
          <ul>
          
              <li><a href="#ecsllm">基于单ECS实例的LLM模型部署文档</a></li>
              
                  <li><a href="#_1">部署说明</a></li>
                  
              
                  <li><a href="#_2">整体架构</a></li>
                  
              
                  <li><a href="#_3">计费说明</a></li>
                  
              
                  <li><a href="#ram">RAM账号所需权限</a></li>
                  
              
                  <li><a href="#_4">部署流程</a></li>
                  
              
                  <li><a href="#_5">使用说明</a></li>
                  
                      <li class="li-h3"><a href="#_6">查询模型部署参数</a></li>
                  
                      <li class="li-h3"><a href="#_7">自定义模型部署参数</a></li>
                  
                      <li class="li-h3"><a href="#api">内网API访问</a></li>
                  
                      <li class="li-h3"><a href="#api_1">公网API访问</a></li>
                  
              
                  <li><a href="#chatbox-vllm-api">使用 Chatbox 客户端配置 vLLM API 进行对话(可选)</a></li>
                  
              
                  <li><a href="#_8">性能测试</a></li>
                  
                      <li class="li-h3"><a href="#_9">压测过程(供参考)</a></li>
                  
                      <li class="li-h3"><a href="#_12">性能测试结果</a></li>
                  
              
          
          </ul>
        </div>
      </div>
    </div>
    <div class="content theme-github">
      
      <div class="content-inner">        
        
        <h1 id="ecsllm">基于单ECS实例的LLM模型部署文档</h1>
<h2 id="_1">部署说明</h2>
<p>本服务提供了基于ECS镜像与VLLM的大模型一键部署方案，10分钟即可部署使用QwQ-32B模型，30分钟即可部署使用Qwen3-235B-A22B模型。</p>
<p>本服务通过ECS镜像打包标准环境，通过Ros模版实现云资源与大模型的一键部署，开发者无需关心模型部署运行的标准环境与底层云资源编排，仅需添加几个参数即可享受主流LLM（如Qwen、DeepSeek等）的推理体验。</p>
<p>本服务支持的模型如下：
* <a href="https://www.modelscope.cn/models/Qwen/Qwen3-235B-A22B/">Qwen/Qwen3-235B-A22B</a>
* <a href="https://www.modelscope.cn/models/Qwen/Qwen3-32B">Qwen/Qwen3-32B</a>
* <a href="https://www.modelscope.cn/models/Qwen/Qwen3-8B">Qwen/Qwen3-8B</a>
* <a href="https://www.modelscope.cn/models/Qwen/QwQ-32B">Qwen/QwQ-32B</a>
* <a href="https://www.modelscope.cn/models/Qwen/Qwen2.5-32B-Instruct">Qwen/Qwen2.5-32B-Instruct</a>
* <a href="https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Llama-70B">deepseek-ai/DeepSeek-R1-Distill-Llama-70B</a>
* <a href="https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B">deepseek-ai/DeepSeek-R1-Distill-Qwen-32B</a>
* <a href="https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B">deepseek-ai/DeepSeek-R1-Distill-Qwen-7B</a></p>
<h2 id="_2">整体架构</h2>
<p><img alt="arch-ecs-one.png" src="../arch-ecs-one.png" /></p>
<h2 id="_3">计费说明</h2>
<p>本服务在阿里云上的费用主要涉及：
* 所选GPU云服务器的规格
* 节点数量
* 磁盘容量
* 公网带宽
计费方式：按量付费（小时）或包年包月
预估费用在创建实例时可实时看到。</p>
<h2 id="ram">RAM账号所需权限</h2>
<p>部署服务实例，需要对部分阿里云资源进行访问和创建操作。因此您的账号需要包含如下资源的权限。</p>
<table>
<thead>
<tr>
<th>权限策略名称</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>AliyunECSFullAccess</td>
<td>管理云服务器服务（ECS）的权限</td>
</tr>
<tr>
<td>AliyunVPCFullAccess</td>
<td>管理专有网络（VPC）的权限</td>
</tr>
<tr>
<td>AliyunROSFullAccess</td>
<td>管理资源编排服务（ROS）的权限</td>
</tr>
<tr>
<td>AliyunComputeNestUserFullAccess</td>
<td>管理计算巢服务（ComputeNest）的用户侧权限</td>
</tr>
</tbody>
</table>
<h2 id="_4">部署流程</h2>
<ol>
<li>单击<a href="https://computenest.console.aliyun.com/service/instance/create/cn-hangzhou?type=user&amp;ServiceId=service-fcfc1ea4afaf47bcbadc">部署链接</a>。选择单机版。根据界面提示填写参数，可根据需求选择是否开启公网，可以看到对应询价明细，确认参数后点击<strong>下一步：确认订单</strong>。
    <img alt="deploy-ecs-one-1.png" src="../deploy-ecs-one-1.png" />
    <img alt="deploy-ecs-one-2.png" src="../deploy-ecs-one-2.png" /></li>
<li>点击<strong>下一步：确认订单</strong>后可以看到价格预览，随后可点击<strong>立即部署</strong>，等待部署完成。(提示RAM权限不足时需要为子账号添加RAM权限)
    <img alt="price-ecs-one.png" src="../price-ecs-one.png" /></li>
<li>等待部署完成后，就可以开始使用服务了。点击服务实例名称，进入服务实例详情，使用Api调用示例即可访问服务。如果是内网访问，需保证ECS实例在同一个VPC下。
    <img alt="deploying-ecs-one.png" src="../deploying-ecs-one.png" />
    <img alt="result-ecs-one-1.png" src="../result-ecs-one-1.png" />
    <img alt="result-ecs-one-2.png" src="../result-ecs-one-2.png" /></li>
<li>ssh访问ECS实例后，执行 docker logs vllm 即可查询模型服务部署日志。当您看到下图所示结果时，表示模型服务部署成功。模型所在路径为/root/llm_model/。
    <img alt="deployed.png" src="../deployed.png" /></li>
</ol>
<h2 id="_5">使用说明</h2>
<h3 id="_6">查询模型部署参数</h3>
<ol>
<li>复制服务实例名称。到<a href="https://ros.console.aliyun.com/cn-hangzhou/stacks">资源编排控制台</a>查看对应的资源栈。
   <img alt="ros-stack-1.png" src="../ros-stack-1.png" />
   <img alt="ros-stack-2.png" src="../ros-stack-2.png" /></li>
<li>进入服务实例对应的资源栈，可以看到所开启的全部资源，并查看到模型部署过程中执行的全部脚本。
   <img alt="ros-stack-ecs-one-1.png" src="../ros-stack-ecs-one-1.png" />
   <img alt="get-shell.png" src="../get-shell.png" /></li>
</ol>
<h3 id="_7">自定义模型部署参数</h3>
<p>如果您有自定义的模型部署参数的需求，可以在部署服务实例后，按照如下操作步骤进行修改。</p>
<ol>
<li>远程连接，登入ECS实例。
   <img alt="private-ip-ecs-one-1.png" src="../private-ip-ecs-one-1.png" /></li>
<li>执行下面的命令，将模型服务停止。
    ```shell
    sudo docker stop vllm
    sudo docker rm vllm</li>
<li>请参考本文档中的 查询模型部署参数 部分，获取模型部署实际执行的脚本。</li>
<li>下面分别是vllm与sglang部署的参考脚本，您可参考参数注释自定义模型部署参数，修改实际执行的脚本。</li>
<li>vllm部署参考脚本
  ```shell
   docker run -d -t --net=host --gpus all \
   --entrypoint /bin/bash \
   --privileged \
   --ipc=host \
   --name vllm \
   -v /root:/root \
   egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-pytorch2.5.1-cuda12.4-ubuntu22.04 \
   -c "pip install --upgrade vllm==0.8.2 &amp;&amp; # 可自定义版本，如 pip install vllm==0.7.1
   export GLOO_SOCKET_IFNAME=eth0 &amp;&amp; # 采用vpc进行网络通信所需环境变量，勿删改
   export NCCL_SOCKET_IFNAME=eth0 &amp;&amp; # 采用vpc进行网络通信所需环境变量，勿删改
   vllm serve /root/llm-model/${ModelName} \
   --served-model-name ${ModelName} \
   --gpu-memory-utilization 0.98 \ # Gpu占用率，过高可能导致其他进程触发OOM。取值范围:0~1
   --max-model-len ${MaxModelLen} \ # 模型最大长度，取值范围与模型本身有关。
   --enable-chunked-prefill \
   --host=0.0.0.0 \
   --port 8000 \
   --trust-remote-code \
   --api-key "${VLLM_API_KEY}" \ # 可选，如不需要可去掉。
   --tensor-parallel-size $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}')" # 使用GPU数量，默认使用全部GPU。</li>
<li>sglang部署参考脚本
  ```shell
   #下载包含sglang的公开镜像
   docker pull egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224</li>
</ol>
<p>docker run -d -t --net=host --gpus all \
   --entrypoint /bin/bash \
   --privileged \
   --ipc=host \
   --name llm-server \
   -v /root:/root \
   egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \ 
   -c "pip install sglang==0.4.3 &amp;&amp; # 可自定义版本
   export GLOO_SOCKET_IFNAME=eth0 &amp;&amp; # 采用vpc进行网络通信所需环境变量，勿删改
   export NCCL_SOCKET_IFNAME=eth0 &amp;&amp; # 采用vpc进行网络通信所需环境变量，勿删改
   python3 -m sglang.launch_server \
   --model-path /root/llm-model/${ModelName} \
   --served-model-name ${ModelName} \
   --tp $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}')" \ # 使用GPU数量，默认使用全部GPU。
   --trust-remote-code \
   --host 0.0.0.0 \
   --port 8000 \
   --mem-fraction-static 0.9 # Gpu占用率，过高可能导致其他进程触发OOM。取值范围:0~1</p>
<h3 id="api">内网API访问</h3>
<p>复制Api调用示例，在资源标签页的ECS实例中粘贴Api调用示例即可。也可在同一VPC内的其他ECS中访问。
    <img alt="result-ecs-one-2.png" src="../result-ecs-one-2.png" />
    <img alt="private-ip-ecs-one-1.png" src="../private-ip-ecs-one-1.png" />
    <img alt="private-ip-ecs-one-2.png" src="../private-ip-ecs-one-2.png" /></p>
<h3 id="api_1">公网API访问</h3>
<p>复制Api调用示例，在本地终端中粘贴Api调用示例即可。
    <img alt="result-ecs-one-2.png" src="../result-ecs-one-2.png" />
    <img alt="public-ip-ecs-one-1.png" src="../public-ip-ecs-one-1.png" /></p>
<h2 id="chatbox-vllm-api">使用 Chatbox 客户端配置 vLLM API 进行对话(可选)</h2>
<ol>
<li>访问 Chatbox <a href="https://chatboxai.app/zh#download">下载地址</a>下载并安装客户端，本方案以 macOS M3 为例。
    <img alt="install-chatbox-1.png" src="../install-chatbox-1.png" /></li>
<li>运行并配置 vLLM API ，单击设置。
    <img alt="install-chatbox-2.png" src="../install-chatbox-2.png" /></li>
<li>在弹出的看板中按照如下表格进行配置。</li>
</ol>
<table>
<thead>
<tr>
<th>项目</th>
<th>说明</th>
<th>示例值</th>
</tr>
</thead>
<tbody>
<tr>
<td>模型提供方</td>
<td>下拉选择模型提供方。</td>
<td>添加自定义提供方</td>
</tr>
<tr>
<td>名称</td>
<td>填写定义模型提供方名称。</td>
<td>vLLM API</td>
</tr>
<tr>
<td>API 域名</td>
<td>填写模型服务调用地址。</td>
<td>http://<ECS公网IP>:8000</td>
</tr>
<tr>
<td>API 路径</td>
<td>填写 API 路径。</td>
<td>/v1/chat/completions</td>
</tr>
<tr>
<td>网络兼容性</td>
<td>点击开启改善网络兼容性</td>
<td>开启</td>
</tr>
<tr>
<td>API 密钥</td>
<td>填写模型服务调用 API 密钥。</td>
<td>部署服务实例后，在服务实例页面可获取Api_Key</td>
</tr>
<tr>
<td>模型</td>
<td>填写调用的模型。</td>
<td>Qwen/QwQ-32B</td>
</tr>
</tbody>
</table>
<ol>
<li>保存配置。在文本输入框中可以进行对话交互。输入问题你是谁？或者其他指令后，调用模型服务获得相应的响应。
    <img alt="install-chatbox-3.png" src="../install-chatbox-3.png" /></li>
</ol>
<h2 id="_8">性能测试</h2>
<h3 id="_9">压测过程(供参考)</h3>
<blockquote>
<p><strong>前提条件：</strong> 1. 无法直接测试带api-key的模型服务；2. 需要公网。</p>
</blockquote>
<h4 id="_10">重新部署模型服务</h4>
<ol>
<li>远程连接，登入ECS实例。
   <img alt="private-ip-ecs-one-1.png" src="../private-ip-ecs-one-1.png" /></li>
<li>执行下面的命令，将模型服务停止。
    ```shell
    sudo docker stop vllm
    sudo docker rm vllm</li>
<li>请参考本文档中的 查询模型部署参数 部分，获取模型部署实际执行的脚本。</li>
<li>去掉脚本中的--api-key参数，在ECS实例中执行剩余脚本。执行docker logs vllm。若结果如下图所示，则模型服务重新部署成功。
   <img alt="deployed.png" src="../deployed.png" /></li>
</ol>
<h4 id="_11">进行性能测试</h4>
<p>以QwQ-32B为例，模型服务部署完成后，ssh登录ECS实例。执行下面的命令，即可得到模型服务性能测试结果。可根据参数说明自行修改。
   ```shell
    yum install -y git-lfs
    git lfs install
    git lfs clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git
    git lfs clone https://github.com/vllm-project/vllm.git</p>
<pre><code>docker exec vllm bash -c "
pip install pandas datasets &amp;&amp;
python3 /root/vllm/benchmarks/benchmark_serving.py \
--backend vllm \
--model /root/llm-model/Qwen/QwQ-32B \
--served-model-name Qwen/QwQ-32B \
--sonnet-input-len 1024 \ # 最大输入长度
--sonnet-output-len 4096 \ # 最大输出长度
--sonnet-prefix-len 50 \ # 前缀长度
--num-prompts 400 \ # 从数据集中随机选取或按顺序处理 400 个 prompt 进行性能测试。
--request-rate 20 \ # 模拟每秒 20 个并发请求的压力测试，持续20秒，共400个请求。评估模型服务在负载下的吞吐量和延迟。
--port 8000 \
--trust-remote-code \
--dataset-name sharegpt \
--save-result \
--dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json
"
</code></pre>
<p>```</p>
<h3 id="_12">性能测试结果</h3>
<h4 id="qwen3-235b-a22b">Qwen3-235B-A22B压测结果</h4>
<p>本服务方案下，针对Qwen3-235B-A22B在GU8TF实例规格下，分别测试QPS为20情况下模型服务的推理响应性能，压测持续时间均为1分钟。</p>
<h5 id="gu8tf">GU8TF规格</h5>
<h6 id="qps2011200">QPS为20，1分钟1200个问答请求</h6>
<p><img alt="qps20-GU8TF-qwen3-235b.png" src="../qps20-GU8TF-qwen3-235b.png" /></p>
<h6 id="qps5013000">QPS为50，1分钟3000个问答请求</h6>
<p><img alt="qps50-GU8TF-qwen3-235b.png" src="../qps50-GU8TF-qwen3-235b.png" /></p>
<h4 id="qwen3-32b">Qwen3-32B压测结果</h4>
<p>本服务方案下，针对Qwen3-32B在ecs.gn7i-8x.16xlarge（8*A10）实例规格下，分别测试QPS为20情况下模型服务的推理响应性能，压测持续时间均为1分钟。</p>
<h5 id="8a10">8*A10规格</h5>
<h6 id="qps2011200_1">QPS为20，1分钟1200个问答请求</h6>
<p><img alt="qps20-8a10-qwen3-32b.png" src="../qps20-8a10-qwen3-32b.png" /></p>
<h6 id="qps5013000_1">QPS为50，1分钟3000个问答请求</h6>
<p><img alt="qps50-8a10-qwen3-32b.png" src="../qps50-8a10-qwen3-32b.png" /></p>
<h4 id="qwq-32b">QwQ-32B压测结果</h4>
<p>本服务方案下，针对QwQ-32B在4<em>A10和8</em>A10实例规格下，分别测试QPS为10、20、50情况下模型服务的推理响应性能，压测持续时间均为20s。</p>
<h5 id="8a10_1">8*A10规格</h5>
<h6 id="qps10">QPS为10</h6>
<p><img alt="img.png" src="../qps10-8a10-ecs-one.png" /></p>
<h6 id="qps20">QPS为20</h6>
<p><img alt="qps20-8a10-ecs-one.png" src="../qps20-8a10-ecs-one.png" /></p>
<h6 id="qps50">QPS为50</h6>
<p><img alt="qps50-8a10-ecs-one.png" src="../qps50-8a10-ecs-one.png" /></p>
<h5 id="4a10">4*A10规格</h5>
<h6 id="qps10_1">QPS为10</h6>
<p><img alt="qps10-4a10-ecs-one.png" src="../qps10-4a10-ecs-one.png" /></p>
<h6 id="qps20_1">QPS为20</h6>
<p><img alt="qps20-4a10-ecs-one.png" src="../qps20-4a10-ecs-one.png" /></p>
<h6 id="qps50_1">QPS为50</h6>
<p><img alt="qps50-4a10-ecs-one.png" src="../qps50-4a10-ecs-one.png" /></p>
        
      </div>

      <div class="copyrights">© 2009-2022 Aliyun.com 版权所有</div>
    </div>
  </div>
  
  <!--
  MkDocs version      : 1.6.1
  Docs Build Date UTC : 2025-07-31 09:42:36.301249+00:00
  -->
</body>
</html>