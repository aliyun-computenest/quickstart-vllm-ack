{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index-acs/","text":"\u57fa\u4e8eACS\u96c6\u7fa4\u7684\u5927\u6a21\u578b\u90e8\u7f72\u6587\u6863 \u90e8\u7f72\u8bf4\u660e \u672c\u65b9\u6848\u901a\u8fc7\u963f\u91cc\u4e91\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u73b0\u5f00\u7bb1\u5373\u7528\u7684\u5927\u6a21\u578b\u63a8\u7406\u670d\u52a1\u90e8\u7f72\uff0c\u652f\u6301\u4ee5\u4e0b\u573a\u666f\uff1a - \u65b0\u5efaACS\u96c6\u7fa4 \uff1a\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u76f4\u63a5\u521b\u5efa\u4e00\u4e2aACS\u96c6\u7fa4\uff0c\u8ba1\u7b97\u5de2\u4f1a\u5728\u7528\u6237\u8d26\u53f7\u4e0b\u4e00\u952e\u521b\u5efaACS\u96c6\u7fa4\u548cOSS Bucket\uff0c\u5b8c\u6210\u4e86\u6a21\u578b\u4e0a\u4f20\u548cBucket\u7684\u6302\u8f7d\u540e\u4f1a\u81ea\u52a8\u90e8\u7f72\u6a21\u578b\uff0c\u6700\u540e\u4f1a\u81ea\u52a8\u521b\u5efa\u8d1f\u8f7d\u5747\u8861\u5b9e\u73b0\u5185\u3001\u516c\u7f51\u7684\u8bbf\u95ee\u3002 - \u9009\u62e9\u5df2\u6709ACS\u3001ACK\u96c6\u7fa4 \uff1a\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528\u5df2\u6709\u7684ACS\u6216\u8005ACK\u96c6\u7fa4\uff0c\u8ba1\u7b97\u5de2\u4f1a\u5728\u7528\u6237\u8d26\u53f7\u4e0b\u521b\u5efaOSS Bucket\uff08\u4e5f\u53ef\u4ee5\u9009\u62e9\u5df2\u6709\u7684Bucket\uff09\uff0c\u5b8c\u6210\u4e86\u6a21\u578b\u4e0a\u4f20\u548cBucket\u7684\u6302\u8f7d\u540e\u4f1a\u81ea\u52a8\u90e8\u7f72\u6a21\u578b\uff0c\u6700\u540e\u4f1a\u81ea\u52a8\u521b\u5efa\u8d1f\u8f7d\u5747\u8861\u5b9e\u73b0\u5185\u3001\u516c\u7f51\u7684\u8bbf\u95ee\u3002 \u672c\u65b9\u6848\u57fa\u4e8e\u4ee5\u4e0b\u6838\u5fc3\u7ec4\u4ef6\uff1a vLLM \uff1a\u63d0\u4f9b\u9ad8\u6027\u80fd\u5e76\u884c\u63a8\u7406\u80fd\u529b\uff0c\u652f\u6301\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u7684LLM\u63a8\u7406\uff08\u652f\u6301Qwen\u3001DeepSeek\u5168\u7cfb\u5217\u6a21\u578b\uff09 SGLang : SGLang \u662f\u4e00\u4e2a\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u670d\u52a1\u6846\u67b6\u3002 ACS\u96c6\u7fa4 \uff1a\u63d0\u4f9b\u5168\u6258\u7ba1\u7684Kubernetes\u73af\u5883\uff0c\u652f\u6301Serverless\u5de5\u4f5c\u8d1f\u8f7d\u5f39\u6027\u4f38\u7f29 P16EN/GU8TF GPU\u52a0\u901f \uff1a\u652f\u6301\u591a\u79cd\u7b97\u529b\u89c4\u683c\uff0c\u6ee1\u8db3\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u7684\u63a8\u7406\u9700\u6c42 \u90e8\u7f72\u540e\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u79c1\u6709/\u516c\u7f51API\u8c03\u7528\u6a21\u578b\u670d\u52a1\uff0c\u8d44\u6e90\u5229\u7528\u7387\u63d0\u5347\u6570\u500d\uff0c\u5f00\u53d1\u8005\u65e0\u9700\u5173\u6ce8\u5e95\u5c42\u5bb9\u5668\u7f16\u6392\u4e0e\u8d44\u6e90\u8c03\u5ea6\uff0c\u4ec5\u9700\u5728\u8ba1\u7b97\u5de2\u63a7\u5236\u53f0\u9875\u9762\u9009\u62e9\u6a21\u578b\u5373\u53ef\u5b8c\u6210\u4e00\u952e\u90e8\u7f72\u3002 \u6574\u4f53\u67b6\u6784 \u8ba1\u8d39\u8bf4\u660e \u8d44\u6e90\u7c7b\u578b \u8ba1\u8d39\u6a21\u5f0f \u5173\u952e\u914d\u7f6e\u8bf4\u660e ACS\u96c6\u7fa4 \u6309\u91cf\u4ed8\u8d39 \u6839\u636e\u6240\u9009GPU\u7c7b\u578b\u548c\u6570\u91cf\u8ba1\u8d39\uff0cGU8TF/GU8TEF/P16EN\u89c4\u683c\u4e0d\u540c\u4ef7\u683c\u4e0d\u540c ECS\u8df3\u677f\u673a \u6309\u91cf\u4ed8\u8d39 ecs.u1-c1m2.xlarge\uff084C8G\uff09\uff0c\u7528\u4e8e\u96c6\u7fa4\u7ba1\u7406\uff0c\u90e8\u7f72\u5b8c\u6210\u540e\u53ef\u5b89\u5168\u91ca\u653e OSS\u5b58\u50a8 \u6309\u91cf\u4ed8\u8d39 \u5b58\u50a8\u6a21\u578b\u6587\u4ef6\uff0c\u5efa\u8bae\u9009\u62e9\u4e0e\u96c6\u7fa4\u540c\u5730\u57df\u7684\u5b58\u50a8\u7c7b\u578b NAT\u7f51\u5173 \u6309\u91cf\u4ed8\u8d39 \u5f53\u5f00\u542f\u516c\u7f51\u8bbf\u95ee\u65f6\u81ea\u52a8\u521b\u5efa\uff0c\u6309\u4f7f\u7528\u65f6\u957f\u548c\u5e26\u5bbd\u8ba1\u8d39 RAM\u8d26\u53f7\u6240\u9700\u6743\u9650 \u90e8\u7f72\u5b9e\u4f8b\u9700\u8981\u5bf9\u90e8\u5206\u963f\u91cc\u4e91\u8d44\u6e90\u8fdb\u884c\u8bbf\u95ee\u548c\u521b\u5efa\u64cd\u4f5c\u3002\u56e0\u6b64\u60a8\u7684\u8d26\u53f7\u9700\u8981\u5305\u542b\u5982\u4e0b\u8d44\u6e90\u7684\u6743\u9650\u3002\u4e14\u9700\u8981\u5f00\u901aACS\u670d\u52a1\uff0c\u5f00\u901a\u540e\u53ef\u4ee5\u5728ACS\u63a7\u5236\u53f0\u53f3\u4e0a\u89d2\u770b\u5230\uff1a \u5f00\u901a\u72b6\u6001\uff1aGPU \u6309\u91cf\u4ed8\u8d39\u5df2\u5f00\u901a, GPU \u5bb9\u91cf\u9884\u7559\u5df2\u5f00\u901a, CPU \u6309\u91cf\u4ed8\u8d39\u5df2\u5f00\u901a \u3002 \u6743\u9650\u7b56\u7565\u540d\u79f0 \u5907\u6ce8 AliyunECSFullAccess \u7ba1\u7406\u4e91\u670d\u52a1\u5668\u670d\u52a1\uff08ECS\uff09\u7684\u6743\u9650 AliyunVPCFullAccess \u7ba1\u7406\u4e13\u6709\u7f51\u7edc\uff08VPC\uff09\u7684\u6743\u9650 AliyunROSFullAccess \u7ba1\u7406\u8d44\u6e90\u7f16\u6392\u670d\u52a1\uff08ROS\uff09\u7684\u6743\u9650 AliyunCSFullAccess \u7ba1\u7406\u5bb9\u5668\u670d\u52a1\uff08CS\uff09\u7684\u6743\u9650 AliyunComputeNestUserFullAccess \u7ba1\u7406\u8ba1\u7b97\u5de2\u670d\u52a1\uff08ComputeNest\uff09\u7684\u7528\u6237\u4fa7\u6743\u9650 AliyunOSSFullAccess \u7ba1\u7406\u7f51\u7edc\u5bf9\u8c61\u5b58\u50a8\u670d\u52a1\uff08OSS\uff09\u7684\u6743\u9650 \u9664\u6b64\u4e4b\u5916\uff0c \u90e8\u7f72\u524d\u9700\u8981\u8054\u7cfbPDSA\u6dfb\u52a0GPU\u767d\u540d\u5355\u3002 \u90e8\u7f72\u6d41\u7a0b \u5355\u51fb \u90e8\u7f72\u94fe\u63a5 \u3002\u6839\u636e\u754c\u9762\u63d0\u793a\u586b\u5199\u53c2\u6570\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u8be2\u4ef7\u660e\u7ec6\uff0c\u786e\u8ba4\u53c2\u6570\u540e\u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u3002 \u8fd9\u91cc\u4e5f\u53ef\u4ee5\u9009\u62e9\u5df2\u6709ACS\u96c6\u7fa4,\u5982\u4e0b\u6240\u793a\uff1a \u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u540e\u53ef\u4ee5\u4e5f\u770b\u5230\u4ef7\u683c\u9884\u89c8\uff0c\u968f\u540e\u70b9\u51fb \u7acb\u5373\u90e8\u7f72 \uff0c\u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u3002 \u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u540e\u5c31\u53ef\u4ee5\u5f00\u59cb\u4f7f\u7528\u670d\u52a1\uff0c\u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u67e5\u770b\u5982\u4f55\u79c1\u7f51\u8bbf\u95ee\u6307\u5bfc\u3002\u5982\u679c\u9009\u62e9\u4e86 \u652f\u6301\u516c\u7f51\u8bbf\u95ee \uff0c\u5219\u80fd\u770b\u5230\u516c\u7f51\u8bbf\u95ee\u6307\u5bfc\u3002 \u4f7f\u7528\u8bf4\u660e \u79c1\u7f51API\u8bbf\u95ee \u5728\u548c\u670d\u52a1\u5668\u540c\u4e00VPC\u5185\u7684ECS\u4e2d\u8bbf\u95ee\u6982\u89c8\u9875\u7684 \u79c1\u7f51API\u5730\u5740 \u3002\u8bbf\u95ee\u793a\u4f8b\u5982\u4e0b\uff1a # \u79c1\u7f51\u6709\u8ba4\u8bc1\u8bf7\u6c42\uff0c\u6d41\u5f0f\u8bbf\u95ee\uff0c\u82e5\u60f3\u5173\u95ed\u6d41\u5f0f\u8bbf\u95ee\uff0c\u5220\u9664stream\u5373\u53ef\u3002 curl http://{$PrivateIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${API_KEY}\" \\ -d '{ \"model\": \"ds\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ], \"max_tokens\": 1024, \"temperature\": 0, \"top_p\": 0.9, \"seed\": 10, \"stream\": true }' \u516c\u7f51API\u8bbf\u95ee \u5982\u679c\u60f3\u901a\u8fc7\u516c\u7f51\u8bbf\u95eeAPI\u5730\u5740\uff0c\u90e8\u7f72\u65f6\u5982\u679c\u9009\u62e9\u4e86 \u652f\u6301\u516c\u7f51\u8bbf\u95ee \uff0c\u5219\u76f4\u63a5\u901a\u8fc7\u516c\u7f51IP\u8bbf\u95ee\u5373\u53ef\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a curl http://${PublicIp}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"ds\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ], \"max_tokens\": 1024, \"temperature\": 0, \"top_p\": 0.9, \"seed\": 10, \"stream\": true }' \u5982\u679c\u672a\u9009\u62e9 \u652f\u6301\u516c\u7f51\u8bbf\u95ee \uff0c\u5219\u9700\u8981\u624b\u52a8\u5728\u96c6\u7fa4\u4e2d\u521b\u5efa\u4e00\u4e2a LoadBalance \uff0c\u793a\u4f8b\u5982\u4e0b\uff08deepseek-r1\uff0c\u5982\u679c\u662fqwq-32b\uff0clabels.app\u9700\u8981\u6539\u4e3aqwq-32b)\uff1a apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type: \"internet\" service.beta.kubernetes.io/alibaba-cloud-loadbalancer-ip-version: ipv4 labels: app: deepseek-r1 name: svc-public namespace: llm-model spec: externalTrafficPolicy: Local ports: - name: serving port: 8000 protocol: TCP targetPort: 8000 selector: app: deepseek-r1 type: LoadBalancer \u5feb\u901f\u66f4\u6362\u6a21\u578b\u3001GPU\u89c4\u683c\u3001Pod\u6570\u91cf \u8ba1\u7b97\u5de2\u652f\u6301\u5b9e\u4f8b\u53d8\u914d\u529f\u80fd\uff0c\u53ef\u4ee5\u4e00\u952e\u66f4\u6362\u6a21\u578b\u3001GPU\u89c4\u683c\u3001Pod\u6570\u91cf\uff0c\u5177\u4f53\u53c2\u8003\u5982\u4e0b\u8bf4\u660e\uff1a \u670d\u52a1\u5b9e\u4f8b\u90e8\u7f72\u5b8c\u6210\u540e\uff0c\u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u9762\uff0c\u70b9\u51fb\u53f3\u4e0a\u89d2\u7684 \u4fee\u6539\u914d\u7f6e \u6309\u94ae\uff0c\u9009\u62e9\u9700\u8981\u4fee\u6539\u7684\u53c2\u6570\uff0c\u4fee\u6539\u540e\u53ef\u4ee5\u67e5\u770b\u4ef7\u683c\u548c\u53d8\u914d\u524d\u540e\u7684\u53c2\u6570\u53d8\u5316\uff0c\u6700\u540e\u70b9\u51fb \u786e\u8ba4 \u3002\uff08\u53d8\u914dGPU\u524d\u8bf7\u786e\u4fdd\u539f\u6240\u9009\u5730\u57df\u4e0e\u53ef\u7528\u533a\u6709\u5e93\u5b58\uff09 \u5b9e\u4f8b\u72b6\u6001\u7531 \u53d8\u914d\u4e2d \u53d8\u4e3a \u5df2\u6210\u529f \u8868\u793a\u53d8\u914d\u6210\u529f\u3002 \u624b\u52a8\u91cd\u65b0\u90e8\u7f72\u6a21\u578b \u5bf9\u4e8e\u4e0d\u66f4\u6362\u6a21\u578b\u3001\u4ec5\u6539\u53d8\u90e8\u7f72\u53c2\u6570\u7684\u60c5\u51b5\uff0c\u53ef\u4ee5\u53c2\u8003\u5982\u4e0b\u8bf4\u660e\u91cd\u65b0\u90e8\u7f72\u6a21\u578b\uff1a \u901a\u8fc7\u8df3\u677f\u673a\u4e0a\u6267\u884ckubectl apply\u547d\u4ee4\u6216\u8005\u76f4\u63a5\u5728\u63a7\u5236\u53f0\u624b\u52a8\u8f93\u5165\u6a21\u677f\u6765\u91cd\u65b0\u90e8\u7f72\u3002 \u8df3\u677f\u673a\u65b9\u5f0f \u8fdb\u5165\u8ba1\u7b97\u5de2\u63a7\u5236\u53f0\u670d\u52a1\u5b9e\u4f8b\u7684\u8d44\u6e90\u754c\u9762\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u7684ECS\u8df3\u677f\u673a\uff0c\u6267\u884c \u8fdc\u7a0b\u8fde\u63a5 \uff0c\u9009\u62e9\u514d\u5bc6\u767b\u5f55\u3002 \u8fdb\u5165\u8df3\u677f\u673a\u540e\u6267\u884c\u547d\u4ee4 bash su root # \u4fee\u6539\u90e8\u7f72\u53c2\u6570 vi /model.yaml # \u5982\u679c\u9700\u8981\u66f4\u6539\u6a21\u578b\u53c2\u6570\uff0c\u4fee\u6539\u4e86model.yaml\u540e\u76f4\u63a5\u6267\u884capply\u547d\u4ee4\u5373\u53ef kubectl apply -f /model.yaml \u63a7\u5236\u53f0\u65b9\u5f0f \u8fdb\u5165\u8ba1\u7b97\u5de2\u63a7\u5236\u53f0\uff0c\u70b9\u51fb \u670d\u52a1\u5b9e\u4f8b \uff0c\u70b9\u51fb \u8d44\u6e90 \uff0c\u627e\u5230\u5bf9\u5e94\u7684ACS\u5b9e\u4f8b\uff0c\u70b9\u51fb\u8fdb\u5165\u3002 \u8fdb\u5165ACS\u63a7\u5236\u53f0\u540e\u70b9\u51fb \u5de5\u4f5c\u8d1f\u8f7d \uff0c\u67e5\u770b \u65e0\u72b6\u6001 \uff0c\u4ee5qwq-32b\u4e3a\u4f8b\uff1a\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u7684Deployment\u3002 \u70b9\u51fb\u8be5Deployment\u540e\u8fdb\u5165\u8be6\u60c5\u9875\u9762\uff0c\u70b9\u51fb\u7f16\u8f91\u53ef\u4ee5\u4fee\u6539\u4e00\u4e9b\u57fa\u672c\u53c2\u6570\uff0c\u6216\u8005\u70b9\u51fb\u67e5\u770byaml\u4fee\u6539\u540e\u66f4\u65b0\u3002 \u5bf9\u4e8e\u66f4\u6362\u6a21\u578b\u7684\u60c5\u51b5\uff0c\u53ef\u4ee5\u53c2\u8003\u5982\u4e0b\u6587\u6863\uff1a ACS\u96c6\u7fa4\u5f62\u6001\u7684LLM\u5927\u6a21\u578b\u63a8\u7406\u955c\u50cf\u4f7f\u7528\u6307\u5bfc_PG1\u963f\u91cc\u4e91\u4ea7\u54c1-\u963f\u91cc\u4e91\u5e2e\u52a9\u4e2d\u5fc3 \u4f7f\u7528ACS GPU\u7b97\u529b\u6784\u5efaDeepSeek\u6ee1\u8840\u7248\u6a21\u578b\u63a8\u7406\u670d\u52a1_\u5bb9\u5668\u8ba1\u7b97\u670d\u52a1(ACS)-\u963f\u91cc\u4e91\u5e2e\u52a9\u4e2d\u5fc3 \u8fdb\u9636\u6559\u7a0b \u9664\u4e86\u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\u65f6\u53ef\u4ee5\u9009\u62e9 Fluid\u914d\u7f6e \uff0c\u4e5f\u53ef\u4ee5\u540e\u7eed\u81ea\u5b9a\u4e49\u914d\u7f6eFluid\u5b9e\u73b0\u6a21\u578b\u52a0\u901f Fluid \u662f\u4e00\u79cd\u57fa\u4e8e Kubernetes \u539f\u751f\u7684\u5206\u5e03\u5f0f\u6570\u636e\u96c6\u7f16\u6392\u548c\u52a0\u901f\u5f15\u64ce\uff0c\u65e8\u5728\u4f18\u5316\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\uff08\u5982AI\u63a8\u7406\u3001\u5927\u6a21\u578b\u8bad\u7ec3\u7b49\u573a\u666f\uff09\u7684\u6027\u80fd\u3002\u5982\u679c\u670d\u52a1\u9700\u8981\u5728\u5f39\u6027\u4f38\u7f29\u65f6\u5feb\u901f\u542f\u52a8\uff0c \u53ef\u4ee5\u8003\u8651\u90e8\u7f72Fluid\uff0c\u5177\u4f53\u53ef\u4ee5\u53c2\u8003\u6587\u6863\uff1a Fluid \u3002 \u7ecf\u6d4b\u8bd5\uff0c\u91c7\u7528Fluid\u7684\u52a0\u901f\uff0c\u6839\u636e\u7f13\u5b58\u5927\u5c0f\uff0c\u6a21\u578b\u52a0\u8f7d\u901f\u5ea6\u53ef\u4ee5\u7f29\u77ed\u81f350%\uff0c\u5728\u5e94\u5bf9\u4e00\u4e9b\u5f39\u6027\u4f38\u7f29\u7684\u573a\u666f\u4e0b\uff0c\u53ef\u4ee5\u5feb\u901f\u52a0\u8f7d\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002\u5982\u4e0b\u6240\u793a\uff0c\u53ef\u4ee5\u4ec5\u4fee\u6539\u5177\u4f53\u7684BucketName\u3001ModelName\u548c\u5177\u4f53\u7684JindoRuntime\u53c2\u6570\uff1a apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: llm-model namespace: llm-model spec: placement: Shared mounts: - mountPoint: oss://${BucketName}/llm-model options: fs.oss.endpoint: oss-${RegionId}-internal.aliyuncs.com name: models path: \"/\" encryptOptions: - name: fs.oss.accessKeyId valueFrom: secretKeyRef: name: oss-secret key: akId - name: fs.oss.accessKeySecret valueFrom: secretKeyRef: name: oss-secret key: akSecret --- apiVersion: data.fluid.io/v1alpha1 kind: JindoRuntime metadata: name: llm-model namespace: llm-model spec: networkmode: ContainerNetwork replicas: ${JindoRuntimeReplicas} # \u8bbe\u7f6e\u526f\u672c\u6570,\u6839\u636e\u5b9e\u9645\u7684\u6a21\u578b\u78c1\u76d8\u5360\u7528\u8fdb\u884c\u8bbe\u7f6e master: podMetadata: labels: alibabacloud.com/compute-class: performance alibabacloud.com/compute-qos: default worker: podMetadata: labels: alibabacloud.com/compute-class: performance alibabacloud.com/compute-qos: default annotations: kubernetes.io/resource-type: serverless resources: requests: cpu: 16 memory: 128Gi limits: cpu: 16 memory: 128Gi tieredstore: levels: - mediumtype: MEM path: /dev/shm volumeType: emptyDir quota: 128Gi high: \"0.99\" low: \"0.95\" --- apiVersion: data.fluid.io/v1alpha1 kind: DataLoad metadata: name: llm-model namespace: llm-model spec: dataset: name: llm-model namespace: llm-model loadMetadata: true Benchmark \u672c\u670d\u52a1\u57fa\u91c7\u7528vllm\u81ea\u5e26\u7684benchmark\u8fdb\u884c\u6d4b\u8bd5\uff0c\u91c7\u7528\u7684\u538b\u6d4b\u6570\u636e\u96c6\uff1a https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split/files \uff0c \u6574\u4f53\u538b\u6d4b\u6d41\u7a0b\uff1a \u521b\u5efa\u4e00\u4e2aDeployment\uff0c\u4f7f\u7528vllm-benchmark\u955c\u50cf\u3002\u5728\u5bb9\u5668\u4e2d\u6267\u884c\u6570\u636e\u96c6\u4e0b\u8f7d\u3001\u538b\u6d4b\u64cd\u4f5c \u4f7f\u7528\u4e0b\u9762\u7684yaml\u521b\u5efaDeployment\u524d\u9700\u8981\u66ff\u6362\u90e8\u5206\u53c2\u6570 \u66ff\u6362\u53c2\u6570 \u53c2\u6570\u542b\u4e49 \u53c2\u6570\u503c\u793a\u4f8b/\u8bf4\u660e $POD_IP \u8fd0\u884c deepseek-r1 \u7684 Pod IP kubectl get pod -n llm-model -l app=$(kubectl get deployment -n llm-model -l app -o jsonpath='{.items[0].spec.template.metadata.labels.app}') -o jsonpath='{.items[0].status.podIP}' $API_KEY \u670d\u52a1\u8ba4\u8bc1\u5bc6\u94a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u4e2d\u83b7\u53d6\uff08\u5f62\u5982 sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \uff09 $MODEL_PATH \u6a21\u578b\u5b58\u50a8\u8def\u5f84 QwQ-32b: /llm-model/Qwen/QwQ-32B Qwen3-32b: /llm-model/Qwen/Qwen3-32B Qwen3-235b-A22b: /llm-model/Qwen/Qwen3-235B-A22B DeepSeek-R1_671b: /llm-model/deepseek-ai/DeepSeek-R1 DeepSeek-R1_32b: /llm-model/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1_70b: /llm-model/deepseek-ai/DeepSeek-R1-Distill-Llama-70B $SERVED_MODEL_NAME \u670d\u52a1\u90e8\u7f72\u7684\u6a21\u578b\u540d\u79f0 QwQ-32b: Qwen/QwQ-32B Qwen3-32b: Qwen/Qwen3-32B Qwen3-235b-A22b: Qwen/Qwen3-235B-A22B DeepSeek-R1_671b: deepseek-ai/DeepSeek-R1 DeepSeek-R1_32b: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1_70b: deepseek-ai/DeepSeek-R1-Distill-Llama-70B apiVersion: apps/v1 kind: Deployment metadata: name: vllm-benchmark namespace: llm-model labels: app: vllm-benchmark spec: replicas: 1 selector: matchLabels: app: vllm-benchmark template: metadata: labels: app: vllm-benchmark spec: volumes: - name: llm-model persistentVolumeClaim: claimName: llm-model containers: - name: vllm-benchmark image: kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/vllm-benchmark:v1 command: - \"sh\" - \"-c\" - | # \u5b89\u88c5\u4f9d\u8d56 yum install -y epel-release && \\ yum install -y git git-lfs && \\ git lfs install && # \u4e0b\u8f7d\u6570\u636e\u96c6 git clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git /root/ShareGPT_V3_unfiltered_cleaned_split # \u6267\u884c\u57fa\u51c6\u6d4b\u8bd5 export OPENAI_API_KEY=$API_KEY python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model $MODEL_PATH \\ --served-model-name $SERVED_MODEL_NAME \\ --trust-remote-code \\ --dataset-name sharegpt \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \\ --sonnet-input-len 1024 \\ --sonnet-output-len 6 \\ --sonnet-prefix-len 50 \\ --num-prompts 200 \\ --request-rate 1 \\ --host $POD_IP \\ --port 8000 \\ --endpoint /v1/completions \\ --save-result # \u4fdd\u6301\u5bb9\u5668\u8fd0\u884c sleep inf volumeMounts: - mountPath: /llm-model name: llm-model \u76f4\u63a5\u5728acs\u63a7\u5236\u53f0\u67e5\u770b\u5bb9\u5668\u65e5\u5fd7\u6216\u8005\u8fdb\u5165\u5bb9\u5668\u67e5\u770b\u5bb9\u5668\u65e5\u5fd7 \u6d4b\u8bd5\u7ed3\u679c\u793a\u4f8b\uff1a plaintext ============ Serving Benchmark Result ============ Successful requests: 200 Benchmark duration (s): 272.15 Total input tokens: 43390 Total generated tokens: 39980 Request throughput (req/s): 0.73 Output token throughput (tok/s): 146.91 Total Token throughput (tok/s): 306.34 ---------------Time to First Token---------------- Mean TTFT (ms): 246.46 Median TTFT (ms): 244.58 P99 TTFT (ms): 342.11 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 130.30 Median TPOT (ms): 130.12 P99 TPOT (ms): 139.09 ---------------Inter-token Latency---------------- Mean ITL (ms): 129.89 Median ITL (ms): 125.40 P99 ITL (ms): 173.20 ==================================================","title":"\u57fa\u4e8eACS\u96c6\u7fa4\u7684\u5927\u6a21\u578b\u90e8\u7f72\u6587\u6863"},{"location":"index-acs/#acs","text":"","title":"\u57fa\u4e8eACS\u96c6\u7fa4\u7684\u5927\u6a21\u578b\u90e8\u7f72\u6587\u6863"},{"location":"index-acs/#_1","text":"\u672c\u65b9\u6848\u901a\u8fc7\u963f\u91cc\u4e91\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u73b0\u5f00\u7bb1\u5373\u7528\u7684\u5927\u6a21\u578b\u63a8\u7406\u670d\u52a1\u90e8\u7f72\uff0c\u652f\u6301\u4ee5\u4e0b\u573a\u666f\uff1a - \u65b0\u5efaACS\u96c6\u7fa4 \uff1a\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u76f4\u63a5\u521b\u5efa\u4e00\u4e2aACS\u96c6\u7fa4\uff0c\u8ba1\u7b97\u5de2\u4f1a\u5728\u7528\u6237\u8d26\u53f7\u4e0b\u4e00\u952e\u521b\u5efaACS\u96c6\u7fa4\u548cOSS Bucket\uff0c\u5b8c\u6210\u4e86\u6a21\u578b\u4e0a\u4f20\u548cBucket\u7684\u6302\u8f7d\u540e\u4f1a\u81ea\u52a8\u90e8\u7f72\u6a21\u578b\uff0c\u6700\u540e\u4f1a\u81ea\u52a8\u521b\u5efa\u8d1f\u8f7d\u5747\u8861\u5b9e\u73b0\u5185\u3001\u516c\u7f51\u7684\u8bbf\u95ee\u3002 - \u9009\u62e9\u5df2\u6709ACS\u3001ACK\u96c6\u7fa4 \uff1a\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528\u5df2\u6709\u7684ACS\u6216\u8005ACK\u96c6\u7fa4\uff0c\u8ba1\u7b97\u5de2\u4f1a\u5728\u7528\u6237\u8d26\u53f7\u4e0b\u521b\u5efaOSS Bucket\uff08\u4e5f\u53ef\u4ee5\u9009\u62e9\u5df2\u6709\u7684Bucket\uff09\uff0c\u5b8c\u6210\u4e86\u6a21\u578b\u4e0a\u4f20\u548cBucket\u7684\u6302\u8f7d\u540e\u4f1a\u81ea\u52a8\u90e8\u7f72\u6a21\u578b\uff0c\u6700\u540e\u4f1a\u81ea\u52a8\u521b\u5efa\u8d1f\u8f7d\u5747\u8861\u5b9e\u73b0\u5185\u3001\u516c\u7f51\u7684\u8bbf\u95ee\u3002 \u672c\u65b9\u6848\u57fa\u4e8e\u4ee5\u4e0b\u6838\u5fc3\u7ec4\u4ef6\uff1a vLLM \uff1a\u63d0\u4f9b\u9ad8\u6027\u80fd\u5e76\u884c\u63a8\u7406\u80fd\u529b\uff0c\u652f\u6301\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u7684LLM\u63a8\u7406\uff08\u652f\u6301Qwen\u3001DeepSeek\u5168\u7cfb\u5217\u6a21\u578b\uff09 SGLang : SGLang \u662f\u4e00\u4e2a\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u670d\u52a1\u6846\u67b6\u3002 ACS\u96c6\u7fa4 \uff1a\u63d0\u4f9b\u5168\u6258\u7ba1\u7684Kubernetes\u73af\u5883\uff0c\u652f\u6301Serverless\u5de5\u4f5c\u8d1f\u8f7d\u5f39\u6027\u4f38\u7f29 P16EN/GU8TF GPU\u52a0\u901f \uff1a\u652f\u6301\u591a\u79cd\u7b97\u529b\u89c4\u683c\uff0c\u6ee1\u8db3\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u7684\u63a8\u7406\u9700\u6c42 \u90e8\u7f72\u540e\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u79c1\u6709/\u516c\u7f51API\u8c03\u7528\u6a21\u578b\u670d\u52a1\uff0c\u8d44\u6e90\u5229\u7528\u7387\u63d0\u5347\u6570\u500d\uff0c\u5f00\u53d1\u8005\u65e0\u9700\u5173\u6ce8\u5e95\u5c42\u5bb9\u5668\u7f16\u6392\u4e0e\u8d44\u6e90\u8c03\u5ea6\uff0c\u4ec5\u9700\u5728\u8ba1\u7b97\u5de2\u63a7\u5236\u53f0\u9875\u9762\u9009\u62e9\u6a21\u578b\u5373\u53ef\u5b8c\u6210\u4e00\u952e\u90e8\u7f72\u3002","title":"\u90e8\u7f72\u8bf4\u660e"},{"location":"index-acs/#_2","text":"","title":"\u6574\u4f53\u67b6\u6784"},{"location":"index-acs/#_3","text":"\u8d44\u6e90\u7c7b\u578b \u8ba1\u8d39\u6a21\u5f0f \u5173\u952e\u914d\u7f6e\u8bf4\u660e ACS\u96c6\u7fa4 \u6309\u91cf\u4ed8\u8d39 \u6839\u636e\u6240\u9009GPU\u7c7b\u578b\u548c\u6570\u91cf\u8ba1\u8d39\uff0cGU8TF/GU8TEF/P16EN\u89c4\u683c\u4e0d\u540c\u4ef7\u683c\u4e0d\u540c ECS\u8df3\u677f\u673a \u6309\u91cf\u4ed8\u8d39 ecs.u1-c1m2.xlarge\uff084C8G\uff09\uff0c\u7528\u4e8e\u96c6\u7fa4\u7ba1\u7406\uff0c\u90e8\u7f72\u5b8c\u6210\u540e\u53ef\u5b89\u5168\u91ca\u653e OSS\u5b58\u50a8 \u6309\u91cf\u4ed8\u8d39 \u5b58\u50a8\u6a21\u578b\u6587\u4ef6\uff0c\u5efa\u8bae\u9009\u62e9\u4e0e\u96c6\u7fa4\u540c\u5730\u57df\u7684\u5b58\u50a8\u7c7b\u578b NAT\u7f51\u5173 \u6309\u91cf\u4ed8\u8d39 \u5f53\u5f00\u542f\u516c\u7f51\u8bbf\u95ee\u65f6\u81ea\u52a8\u521b\u5efa\uff0c\u6309\u4f7f\u7528\u65f6\u957f\u548c\u5e26\u5bbd\u8ba1\u8d39","title":"\u8ba1\u8d39\u8bf4\u660e"},{"location":"index-acs/#ram","text":"\u90e8\u7f72\u5b9e\u4f8b\u9700\u8981\u5bf9\u90e8\u5206\u963f\u91cc\u4e91\u8d44\u6e90\u8fdb\u884c\u8bbf\u95ee\u548c\u521b\u5efa\u64cd\u4f5c\u3002\u56e0\u6b64\u60a8\u7684\u8d26\u53f7\u9700\u8981\u5305\u542b\u5982\u4e0b\u8d44\u6e90\u7684\u6743\u9650\u3002\u4e14\u9700\u8981\u5f00\u901aACS\u670d\u52a1\uff0c\u5f00\u901a\u540e\u53ef\u4ee5\u5728ACS\u63a7\u5236\u53f0\u53f3\u4e0a\u89d2\u770b\u5230\uff1a \u5f00\u901a\u72b6\u6001\uff1aGPU \u6309\u91cf\u4ed8\u8d39\u5df2\u5f00\u901a, GPU \u5bb9\u91cf\u9884\u7559\u5df2\u5f00\u901a, CPU \u6309\u91cf\u4ed8\u8d39\u5df2\u5f00\u901a \u3002 \u6743\u9650\u7b56\u7565\u540d\u79f0 \u5907\u6ce8 AliyunECSFullAccess \u7ba1\u7406\u4e91\u670d\u52a1\u5668\u670d\u52a1\uff08ECS\uff09\u7684\u6743\u9650 AliyunVPCFullAccess \u7ba1\u7406\u4e13\u6709\u7f51\u7edc\uff08VPC\uff09\u7684\u6743\u9650 AliyunROSFullAccess \u7ba1\u7406\u8d44\u6e90\u7f16\u6392\u670d\u52a1\uff08ROS\uff09\u7684\u6743\u9650 AliyunCSFullAccess \u7ba1\u7406\u5bb9\u5668\u670d\u52a1\uff08CS\uff09\u7684\u6743\u9650 AliyunComputeNestUserFullAccess \u7ba1\u7406\u8ba1\u7b97\u5de2\u670d\u52a1\uff08ComputeNest\uff09\u7684\u7528\u6237\u4fa7\u6743\u9650 AliyunOSSFullAccess \u7ba1\u7406\u7f51\u7edc\u5bf9\u8c61\u5b58\u50a8\u670d\u52a1\uff08OSS\uff09\u7684\u6743\u9650 \u9664\u6b64\u4e4b\u5916\uff0c \u90e8\u7f72\u524d\u9700\u8981\u8054\u7cfbPDSA\u6dfb\u52a0GPU\u767d\u540d\u5355\u3002","title":"RAM\u8d26\u53f7\u6240\u9700\u6743\u9650"},{"location":"index-acs/#_4","text":"\u5355\u51fb \u90e8\u7f72\u94fe\u63a5 \u3002\u6839\u636e\u754c\u9762\u63d0\u793a\u586b\u5199\u53c2\u6570\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u8be2\u4ef7\u660e\u7ec6\uff0c\u786e\u8ba4\u53c2\u6570\u540e\u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u3002 \u8fd9\u91cc\u4e5f\u53ef\u4ee5\u9009\u62e9\u5df2\u6709ACS\u96c6\u7fa4,\u5982\u4e0b\u6240\u793a\uff1a \u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u540e\u53ef\u4ee5\u4e5f\u770b\u5230\u4ef7\u683c\u9884\u89c8\uff0c\u968f\u540e\u70b9\u51fb \u7acb\u5373\u90e8\u7f72 \uff0c\u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u3002 \u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u540e\u5c31\u53ef\u4ee5\u5f00\u59cb\u4f7f\u7528\u670d\u52a1\uff0c\u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u67e5\u770b\u5982\u4f55\u79c1\u7f51\u8bbf\u95ee\u6307\u5bfc\u3002\u5982\u679c\u9009\u62e9\u4e86 \u652f\u6301\u516c\u7f51\u8bbf\u95ee \uff0c\u5219\u80fd\u770b\u5230\u516c\u7f51\u8bbf\u95ee\u6307\u5bfc\u3002","title":"\u90e8\u7f72\u6d41\u7a0b"},{"location":"index-acs/#_5","text":"","title":"\u4f7f\u7528\u8bf4\u660e"},{"location":"index-acs/#api","text":"\u5728\u548c\u670d\u52a1\u5668\u540c\u4e00VPC\u5185\u7684ECS\u4e2d\u8bbf\u95ee\u6982\u89c8\u9875\u7684 \u79c1\u7f51API\u5730\u5740 \u3002\u8bbf\u95ee\u793a\u4f8b\u5982\u4e0b\uff1a # \u79c1\u7f51\u6709\u8ba4\u8bc1\u8bf7\u6c42\uff0c\u6d41\u5f0f\u8bbf\u95ee\uff0c\u82e5\u60f3\u5173\u95ed\u6d41\u5f0f\u8bbf\u95ee\uff0c\u5220\u9664stream\u5373\u53ef\u3002 curl http://{$PrivateIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${API_KEY}\" \\ -d '{ \"model\": \"ds\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ], \"max_tokens\": 1024, \"temperature\": 0, \"top_p\": 0.9, \"seed\": 10, \"stream\": true }'","title":"\u79c1\u7f51API\u8bbf\u95ee"},{"location":"index-acs/#api_1","text":"\u5982\u679c\u60f3\u901a\u8fc7\u516c\u7f51\u8bbf\u95eeAPI\u5730\u5740\uff0c\u90e8\u7f72\u65f6\u5982\u679c\u9009\u62e9\u4e86 \u652f\u6301\u516c\u7f51\u8bbf\u95ee \uff0c\u5219\u76f4\u63a5\u901a\u8fc7\u516c\u7f51IP\u8bbf\u95ee\u5373\u53ef\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a curl http://${PublicIp}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"ds\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ], \"max_tokens\": 1024, \"temperature\": 0, \"top_p\": 0.9, \"seed\": 10, \"stream\": true }' \u5982\u679c\u672a\u9009\u62e9 \u652f\u6301\u516c\u7f51\u8bbf\u95ee \uff0c\u5219\u9700\u8981\u624b\u52a8\u5728\u96c6\u7fa4\u4e2d\u521b\u5efa\u4e00\u4e2a LoadBalance \uff0c\u793a\u4f8b\u5982\u4e0b\uff08deepseek-r1\uff0c\u5982\u679c\u662fqwq-32b\uff0clabels.app\u9700\u8981\u6539\u4e3aqwq-32b)\uff1a apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type: \"internet\" service.beta.kubernetes.io/alibaba-cloud-loadbalancer-ip-version: ipv4 labels: app: deepseek-r1 name: svc-public namespace: llm-model spec: externalTrafficPolicy: Local ports: - name: serving port: 8000 protocol: TCP targetPort: 8000 selector: app: deepseek-r1 type: LoadBalancer","title":"\u516c\u7f51API\u8bbf\u95ee"},{"location":"index-acs/#gpupod","text":"\u8ba1\u7b97\u5de2\u652f\u6301\u5b9e\u4f8b\u53d8\u914d\u529f\u80fd\uff0c\u53ef\u4ee5\u4e00\u952e\u66f4\u6362\u6a21\u578b\u3001GPU\u89c4\u683c\u3001Pod\u6570\u91cf\uff0c\u5177\u4f53\u53c2\u8003\u5982\u4e0b\u8bf4\u660e\uff1a \u670d\u52a1\u5b9e\u4f8b\u90e8\u7f72\u5b8c\u6210\u540e\uff0c\u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u9762\uff0c\u70b9\u51fb\u53f3\u4e0a\u89d2\u7684 \u4fee\u6539\u914d\u7f6e \u6309\u94ae\uff0c\u9009\u62e9\u9700\u8981\u4fee\u6539\u7684\u53c2\u6570\uff0c\u4fee\u6539\u540e\u53ef\u4ee5\u67e5\u770b\u4ef7\u683c\u548c\u53d8\u914d\u524d\u540e\u7684\u53c2\u6570\u53d8\u5316\uff0c\u6700\u540e\u70b9\u51fb \u786e\u8ba4 \u3002\uff08\u53d8\u914dGPU\u524d\u8bf7\u786e\u4fdd\u539f\u6240\u9009\u5730\u57df\u4e0e\u53ef\u7528\u533a\u6709\u5e93\u5b58\uff09 \u5b9e\u4f8b\u72b6\u6001\u7531 \u53d8\u914d\u4e2d \u53d8\u4e3a \u5df2\u6210\u529f \u8868\u793a\u53d8\u914d\u6210\u529f\u3002","title":"\u5feb\u901f\u66f4\u6362\u6a21\u578b\u3001GPU\u89c4\u683c\u3001Pod\u6570\u91cf"},{"location":"index-acs/#_6","text":"\u5bf9\u4e8e\u4e0d\u66f4\u6362\u6a21\u578b\u3001\u4ec5\u6539\u53d8\u90e8\u7f72\u53c2\u6570\u7684\u60c5\u51b5\uff0c\u53ef\u4ee5\u53c2\u8003\u5982\u4e0b\u8bf4\u660e\u91cd\u65b0\u90e8\u7f72\u6a21\u578b\uff1a \u901a\u8fc7\u8df3\u677f\u673a\u4e0a\u6267\u884ckubectl apply\u547d\u4ee4\u6216\u8005\u76f4\u63a5\u5728\u63a7\u5236\u53f0\u624b\u52a8\u8f93\u5165\u6a21\u677f\u6765\u91cd\u65b0\u90e8\u7f72\u3002 \u8df3\u677f\u673a\u65b9\u5f0f \u8fdb\u5165\u8ba1\u7b97\u5de2\u63a7\u5236\u53f0\u670d\u52a1\u5b9e\u4f8b\u7684\u8d44\u6e90\u754c\u9762\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u7684ECS\u8df3\u677f\u673a\uff0c\u6267\u884c \u8fdc\u7a0b\u8fde\u63a5 \uff0c\u9009\u62e9\u514d\u5bc6\u767b\u5f55\u3002 \u8fdb\u5165\u8df3\u677f\u673a\u540e\u6267\u884c\u547d\u4ee4 bash su root # \u4fee\u6539\u90e8\u7f72\u53c2\u6570 vi /model.yaml # \u5982\u679c\u9700\u8981\u66f4\u6539\u6a21\u578b\u53c2\u6570\uff0c\u4fee\u6539\u4e86model.yaml\u540e\u76f4\u63a5\u6267\u884capply\u547d\u4ee4\u5373\u53ef kubectl apply -f /model.yaml \u63a7\u5236\u53f0\u65b9\u5f0f \u8fdb\u5165\u8ba1\u7b97\u5de2\u63a7\u5236\u53f0\uff0c\u70b9\u51fb \u670d\u52a1\u5b9e\u4f8b \uff0c\u70b9\u51fb \u8d44\u6e90 \uff0c\u627e\u5230\u5bf9\u5e94\u7684ACS\u5b9e\u4f8b\uff0c\u70b9\u51fb\u8fdb\u5165\u3002 \u8fdb\u5165ACS\u63a7\u5236\u53f0\u540e\u70b9\u51fb \u5de5\u4f5c\u8d1f\u8f7d \uff0c\u67e5\u770b \u65e0\u72b6\u6001 \uff0c\u4ee5qwq-32b\u4e3a\u4f8b\uff1a\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u7684Deployment\u3002 \u70b9\u51fb\u8be5Deployment\u540e\u8fdb\u5165\u8be6\u60c5\u9875\u9762\uff0c\u70b9\u51fb\u7f16\u8f91\u53ef\u4ee5\u4fee\u6539\u4e00\u4e9b\u57fa\u672c\u53c2\u6570\uff0c\u6216\u8005\u70b9\u51fb\u67e5\u770byaml\u4fee\u6539\u540e\u66f4\u65b0\u3002 \u5bf9\u4e8e\u66f4\u6362\u6a21\u578b\u7684\u60c5\u51b5\uff0c\u53ef\u4ee5\u53c2\u8003\u5982\u4e0b\u6587\u6863\uff1a ACS\u96c6\u7fa4\u5f62\u6001\u7684LLM\u5927\u6a21\u578b\u63a8\u7406\u955c\u50cf\u4f7f\u7528\u6307\u5bfc_PG1\u963f\u91cc\u4e91\u4ea7\u54c1-\u963f\u91cc\u4e91\u5e2e\u52a9\u4e2d\u5fc3 \u4f7f\u7528ACS GPU\u7b97\u529b\u6784\u5efaDeepSeek\u6ee1\u8840\u7248\u6a21\u578b\u63a8\u7406\u670d\u52a1_\u5bb9\u5668\u8ba1\u7b97\u670d\u52a1(ACS)-\u963f\u91cc\u4e91\u5e2e\u52a9\u4e2d\u5fc3","title":"\u624b\u52a8\u91cd\u65b0\u90e8\u7f72\u6a21\u578b"},{"location":"index-acs/#_7","text":"\u9664\u4e86\u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\u65f6\u53ef\u4ee5\u9009\u62e9 Fluid\u914d\u7f6e \uff0c\u4e5f\u53ef\u4ee5\u540e\u7eed\u81ea\u5b9a\u4e49\u914d\u7f6eFluid\u5b9e\u73b0\u6a21\u578b\u52a0\u901f Fluid \u662f\u4e00\u79cd\u57fa\u4e8e Kubernetes \u539f\u751f\u7684\u5206\u5e03\u5f0f\u6570\u636e\u96c6\u7f16\u6392\u548c\u52a0\u901f\u5f15\u64ce\uff0c\u65e8\u5728\u4f18\u5316\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\uff08\u5982AI\u63a8\u7406\u3001\u5927\u6a21\u578b\u8bad\u7ec3\u7b49\u573a\u666f\uff09\u7684\u6027\u80fd\u3002\u5982\u679c\u670d\u52a1\u9700\u8981\u5728\u5f39\u6027\u4f38\u7f29\u65f6\u5feb\u901f\u542f\u52a8\uff0c \u53ef\u4ee5\u8003\u8651\u90e8\u7f72Fluid\uff0c\u5177\u4f53\u53ef\u4ee5\u53c2\u8003\u6587\u6863\uff1a Fluid \u3002 \u7ecf\u6d4b\u8bd5\uff0c\u91c7\u7528Fluid\u7684\u52a0\u901f\uff0c\u6839\u636e\u7f13\u5b58\u5927\u5c0f\uff0c\u6a21\u578b\u52a0\u8f7d\u901f\u5ea6\u53ef\u4ee5\u7f29\u77ed\u81f350%\uff0c\u5728\u5e94\u5bf9\u4e00\u4e9b\u5f39\u6027\u4f38\u7f29\u7684\u573a\u666f\u4e0b\uff0c\u53ef\u4ee5\u5feb\u901f\u52a0\u8f7d\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002\u5982\u4e0b\u6240\u793a\uff0c\u53ef\u4ee5\u4ec5\u4fee\u6539\u5177\u4f53\u7684BucketName\u3001ModelName\u548c\u5177\u4f53\u7684JindoRuntime\u53c2\u6570\uff1a apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: llm-model namespace: llm-model spec: placement: Shared mounts: - mountPoint: oss://${BucketName}/llm-model options: fs.oss.endpoint: oss-${RegionId}-internal.aliyuncs.com name: models path: \"/\" encryptOptions: - name: fs.oss.accessKeyId valueFrom: secretKeyRef: name: oss-secret key: akId - name: fs.oss.accessKeySecret valueFrom: secretKeyRef: name: oss-secret key: akSecret --- apiVersion: data.fluid.io/v1alpha1 kind: JindoRuntime metadata: name: llm-model namespace: llm-model spec: networkmode: ContainerNetwork replicas: ${JindoRuntimeReplicas} # \u8bbe\u7f6e\u526f\u672c\u6570,\u6839\u636e\u5b9e\u9645\u7684\u6a21\u578b\u78c1\u76d8\u5360\u7528\u8fdb\u884c\u8bbe\u7f6e master: podMetadata: labels: alibabacloud.com/compute-class: performance alibabacloud.com/compute-qos: default worker: podMetadata: labels: alibabacloud.com/compute-class: performance alibabacloud.com/compute-qos: default annotations: kubernetes.io/resource-type: serverless resources: requests: cpu: 16 memory: 128Gi limits: cpu: 16 memory: 128Gi tieredstore: levels: - mediumtype: MEM path: /dev/shm volumeType: emptyDir quota: 128Gi high: \"0.99\" low: \"0.95\" --- apiVersion: data.fluid.io/v1alpha1 kind: DataLoad metadata: name: llm-model namespace: llm-model spec: dataset: name: llm-model namespace: llm-model loadMetadata: true","title":"\u8fdb\u9636\u6559\u7a0b"},{"location":"index-acs/#benchmark","text":"\u672c\u670d\u52a1\u57fa\u91c7\u7528vllm\u81ea\u5e26\u7684benchmark\u8fdb\u884c\u6d4b\u8bd5\uff0c\u91c7\u7528\u7684\u538b\u6d4b\u6570\u636e\u96c6\uff1a https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split/files \uff0c \u6574\u4f53\u538b\u6d4b\u6d41\u7a0b\uff1a \u521b\u5efa\u4e00\u4e2aDeployment\uff0c\u4f7f\u7528vllm-benchmark\u955c\u50cf\u3002\u5728\u5bb9\u5668\u4e2d\u6267\u884c\u6570\u636e\u96c6\u4e0b\u8f7d\u3001\u538b\u6d4b\u64cd\u4f5c \u4f7f\u7528\u4e0b\u9762\u7684yaml\u521b\u5efaDeployment\u524d\u9700\u8981\u66ff\u6362\u90e8\u5206\u53c2\u6570 \u66ff\u6362\u53c2\u6570 \u53c2\u6570\u542b\u4e49 \u53c2\u6570\u503c\u793a\u4f8b/\u8bf4\u660e $POD_IP \u8fd0\u884c deepseek-r1 \u7684 Pod IP kubectl get pod -n llm-model -l app=$(kubectl get deployment -n llm-model -l app -o jsonpath='{.items[0].spec.template.metadata.labels.app}') -o jsonpath='{.items[0].status.podIP}' $API_KEY \u670d\u52a1\u8ba4\u8bc1\u5bc6\u94a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u4e2d\u83b7\u53d6\uff08\u5f62\u5982 sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \uff09 $MODEL_PATH \u6a21\u578b\u5b58\u50a8\u8def\u5f84 QwQ-32b: /llm-model/Qwen/QwQ-32B Qwen3-32b: /llm-model/Qwen/Qwen3-32B Qwen3-235b-A22b: /llm-model/Qwen/Qwen3-235B-A22B DeepSeek-R1_671b: /llm-model/deepseek-ai/DeepSeek-R1 DeepSeek-R1_32b: /llm-model/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1_70b: /llm-model/deepseek-ai/DeepSeek-R1-Distill-Llama-70B $SERVED_MODEL_NAME \u670d\u52a1\u90e8\u7f72\u7684\u6a21\u578b\u540d\u79f0 QwQ-32b: Qwen/QwQ-32B Qwen3-32b: Qwen/Qwen3-32B Qwen3-235b-A22b: Qwen/Qwen3-235B-A22B DeepSeek-R1_671b: deepseek-ai/DeepSeek-R1 DeepSeek-R1_32b: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1_70b: deepseek-ai/DeepSeek-R1-Distill-Llama-70B apiVersion: apps/v1 kind: Deployment metadata: name: vllm-benchmark namespace: llm-model labels: app: vllm-benchmark spec: replicas: 1 selector: matchLabels: app: vllm-benchmark template: metadata: labels: app: vllm-benchmark spec: volumes: - name: llm-model persistentVolumeClaim: claimName: llm-model containers: - name: vllm-benchmark image: kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/vllm-benchmark:v1 command: - \"sh\" - \"-c\" - | # \u5b89\u88c5\u4f9d\u8d56 yum install -y epel-release && \\ yum install -y git git-lfs && \\ git lfs install && # \u4e0b\u8f7d\u6570\u636e\u96c6 git clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git /root/ShareGPT_V3_unfiltered_cleaned_split # \u6267\u884c\u57fa\u51c6\u6d4b\u8bd5 export OPENAI_API_KEY=$API_KEY python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model $MODEL_PATH \\ --served-model-name $SERVED_MODEL_NAME \\ --trust-remote-code \\ --dataset-name sharegpt \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \\ --sonnet-input-len 1024 \\ --sonnet-output-len 6 \\ --sonnet-prefix-len 50 \\ --num-prompts 200 \\ --request-rate 1 \\ --host $POD_IP \\ --port 8000 \\ --endpoint /v1/completions \\ --save-result # \u4fdd\u6301\u5bb9\u5668\u8fd0\u884c sleep inf volumeMounts: - mountPath: /llm-model name: llm-model \u76f4\u63a5\u5728acs\u63a7\u5236\u53f0\u67e5\u770b\u5bb9\u5668\u65e5\u5fd7\u6216\u8005\u8fdb\u5165\u5bb9\u5668\u67e5\u770b\u5bb9\u5668\u65e5\u5fd7 \u6d4b\u8bd5\u7ed3\u679c\u793a\u4f8b\uff1a plaintext ============ Serving Benchmark Result ============ Successful requests: 200 Benchmark duration (s): 272.15 Total input tokens: 43390 Total generated tokens: 39980 Request throughput (req/s): 0.73 Output token throughput (tok/s): 146.91 Total Token throughput (tok/s): 306.34 ---------------Time to First Token---------------- Mean TTFT (ms): 246.46 Median TTFT (ms): 244.58 P99 TTFT (ms): 342.11 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 130.30 Median TPOT (ms): 130.12 P99 TPOT (ms): 139.09 ---------------Inter-token Latency---------------- Mean ITL (ms): 129.89 Median ITL (ms): 125.40 P99 ITL (ms): 173.20 ==================================================","title":"Benchmark"},{"location":"index-ecs-one-en/","text":"LLM model deployment document based on single ECS instance Deployment Instructions This service provides a one-click deployment solution for large models based on ECS mirroring and VLLM. The QwQ-32B model can be deployed in 10 minutes and the Qwen3-235B-A22B model can be deployed in 30 minutes. This service uses ECS mirror packaging standard environments and realizes one-click deployment of cloud resources and large models through Ros templates. Developers do not need to care about the standard environment for model deployment and operation and the underlying cloud resource orchestration. They only need to add a few parameters to enjoy the inference experience of mainstream LLMs (such as Qwen, DeepSeek, etc.). The models supported by this service are as follows: * Qwen/Qwen3-235B-A22B * Qwen/Qwen3-32B * Qwen/Qwen3-8B * Qwen/QwQ-32B * Qwen/Qwen2.5-32B-Instruct * deepseek-ai/DeepSeek-R1-Distill-Llama-70B * deepseek-ai/DeepSeek-R1-Distill-Qwen-32B * deepseek-ai/DeepSeek-R1-Distill-Qwen-7B Overall architecture User/Client Layer Access Method: Users can connect via Private VPC or Public Network. Security Measures: Utilizes an API Key for secure access. Response Mode: Supports streaming returns for real-time data processing. Service Layer Large-Scale Services: This layer provides large model services. Standard API Interface: It offers standardized API calls for easy integration and usage. Recommendation Engine: The service features a recommendation engine powered by VLLM services. Infrastructure Layer Network: It features a Virtual Private Cloud (VPC) network for secure data transmission. Computational Resources: This includes ECS image and ECS GPU instances for scalable computing power. ECS Image Contents: The layer manages the ECS image, which comprises the environment for running VLLM, as well as large model files. Billing instructions The cost of this service on Alibaba Cloud mainly involves: * Specifications of the selected GPU cloud server * Number of nodes * Disk capacity * Public network bandwidth Billing method: pay by volume (hours) or annual monthly payment Estimated costs are visible in real time when creating an instance. Required permissions for RAM account To deploy service instances, some Alibaba Cloud resources need to be accessed and created.Therefore, your account needs permissions to include the following resources. Permission Policy Name Notes AliyunECSFullAccess Permissions to manage cloud server services (ECS) AliyunVPCFullAccess Permissions to manage proprietary networks (VPCs) AliyunROSFullAccess Permissions to manage resource orchestration services (ROS) AliyunComputeNestUserFullAccess Manage user-side permissions for ComputeNest Deployment Process Click [Deployment Link] .Select standalone version.Fill in the parameters according to the prompts on the interface, and you can choose whether to turn on the public network according to your needs. You can see the corresponding inquiry details. After confirming the parameters, click Next: Confirm the order**. Click Next: Confirm the order ** and you can see the price preview. Then you can click Deploy now** and wait for the deployment to complete.(If the RAM permission is insufficient, you need to add RAM permissions to the sub-account) After the deployment is completed, you can start using the service.Click on the service instance name to enter the service instance details, and use the Api to call the sample to access the service.If it is an intranet access, you must ensure that the ECS instance is under the same VPC. After ssh accesses the ECS instance, execute docker logs vllm to query the model service deployment log.When you see the result shown in the figure below, it means that the model service is deployed successfully.The path where the model is located is /root/llm_model/. Instructions for use Query model deployment parameters Copy the service instance name.Go to [Resource Orchestration Console] (https://ros.console.aliyun.com/cn-hangzhou/stacks) to view the corresponding resource stack. Enter the resource stack corresponding to the service instance, you can see all the resources opened and view all the scripts executed during the model deployment process. Customize model deployment parameters If you have the requirement for custom model deployment parameters, you can modify it after deploying the service instance as follows. Remotely connect and log in to the ECS instance. Execute the following command to stop the model service. ```shell sudo docker stop vllm sudo docker rm vllm Please refer to the Query Model Deployment Parameters section in this document to obtain the scripts that the model deployment actually executes. The following are the reference scripts for vllm and sglang deployments. You can refer to the parameters to annotate custom model deployment parameters and modify the actual executed script. vllm deployment reference script ```shell docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name vllm \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-pytorch2.5.1-cuda12.4-ubuntu22.04 \\ -c \"pip install --upgrade vllm==0.8.2 && # Customizable version, such as pip install vllm==0.7.1 export GLOO_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify export NCCL_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify vllm serve /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --gpu-memory-utilization 0.98 \\ # Gpu occupancy rate, too high may cause other processes to trigger OOM.Value range: 0~1 --max-model-len ${MaxModelLen} \\ # The maximum length of the model, the value range is related to the model itself. --enable-chunked-prefill \\ --host=0.0.0.0 \\ --port 8000 \\ --trust-remote-code \\ --api-key \"${VLLM_API_KEY}\" \\ # Optional, can be removed if not required. --tensor-parallel-size $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}')\" # Number of GPUs used, all GPUs are used by default. sglang deployment reference script ```shell #Download a public image containing sglang docker pull egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 Docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name llm-server \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install sglang==0.4.3 && # Customizable version export GLOO_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify export NCCL_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify python3 -m sglang.launch_server \\ --model-path /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --tp $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}')\" \\ # Number of GPUs used, all GPUs are used by default. --trust-remote-code \\ --host 0.0.0.0 \\ --port 8000 \\ --mem-fraction-static 0.9 # Gpu occupancy rate, too high may cause other processes to trigger OOM.Value range: 0~1 Intranet API access Copy the Api call example and paste the Api call example in the ECS instance of the resource tab.It can also be accessed in other ECS within the same VPC. Public Internet API access Copy the Api call example and paste the Api call example in the local terminal. Configure the vLLM API using the Chatbox client for conversation (optional) Visit Chatbox [Download Address] (https://chatboxai.app/zh#download) to download and install the client. This solution takes macOS M3 as an example. Run and configure the vLLM API and click Settings. Configure in the pop-up kanban according to the following table. Project Description Example Value Model Provider Pull down to select the model provider. Add a custom provider Name Fill in the name of the model provider. vLLM API API Domain Name Fill in the model service call address. http:// :8000 API Path Fill in the API Path. /v1/chat/completes Network Compatibility Click to enable to improve network compatibility Open API Key Fill in the Model Service Call API Key. After deploying the service instance, you can get Api_Key on the service instance page Model Fill in the called model. Qwen/QwQ-32B Save the configuration.The conversation can be performed in the text input box.Enter the question Who are you?Or after other instructions, the model service is called to obtain the corresponding response. Performance Testing Pressure test process (for reference) **Prerequisites: ** 1. It is impossible to directly test model services with API-key; 2. Public network is required. Redeploy the model service Remotely connect and log in to the ECS instance. Execute the following command to stop the model service. ```shell sudo docker stop vllm sudo docker rm vllm Please refer to the Query Model Deployment Parameters section in this document to obtain the scripts that the model deployment actually executes. Remove the --api-key parameter in the script and execute the remaining scripts in the ECS instance.Execute docker logs vllm.If the result is shown in the figure below, the model service is redeployed successfully. Perform performance testing Taking QwQ-32B as an example, after the model service is deployed, ssh logs into the ECS instance.Execute the following command to get the model service performance test results.You can modify it yourself according to the parameter description. ```shell yum install -y git-lfs git lfs install git lfs clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git git lfs clone https://github.com/vllm-project/vllm.git docker exec vllm bash -c \" pip install pandas datasets && python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model /root/llm-model/Qwen/QwQ-32B \\ --served-model-name Qwen/QwQ-32B \\ --sonnet-input-len 1024 \\ # Maximum input length --sonnet-output-len 4096 \\ # Maximum output length --sonnet-prefix-len 50 \\ # Prefix length --num-prompts 400 \\ # Randomly select or process 400 prompts from the dataset for performance testing. --request-rate 20 \\ # Simulate a stress test of 20 concurrent requests per second, lasting 20 seconds, with a total of 400 requests.Evaluate the throughput and latency of the model service under load. --port 8000 \\ --trust-remote-code \\ --dataset-name sharegpt \\ --save-result \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \" ``` Qwen3-235B-A22B stress test results Under this service plan, for Qwen3-235B-A22B, the inference response performance of the model service was tested under the GU8TF instance specification with a QPS of 20, and the stress test duration was 1 minute. GU8TF specification QPS is 20, 1200 question and answer requests in 1 minute QPS is 50, 3000 Q&A requests per minute Qwen3-32B stress test results Under this service plan, for Qwen3-32B, the inference response performance of the model service is tested under the ecs.gn7i-8x.16xlarge (8*A10) and GU8TF instance specifications, respectively, with a QPS of 20. The stress test duration is 1 minute. 8*A10 specification QPS is 20, 1200 Q&A requests per minute QPS is 50, 3000 Q&A requests per minute QwQ-32B stress test results Under this service plan, the inference response performance of the model service under the 4 A10 and 8 A10 instance specifications were tested respectively for the inference response performance of the model service with a QPS of 10, 20 and 50, and the pressure measurement duration was 20s. 8*A10 specifications QPS is 10 QPS is 20 QPS is 50 4*A10 specifications QPS is 10 QPS is 20 QPS is 50","title":"LLM model deployment document based on single ECS instance"},{"location":"index-ecs-one-en/#llm-model-deployment-document-based-on-single-ecs-instance","text":"","title":"LLM model deployment document based on single ECS instance"},{"location":"index-ecs-one-en/#deployment-instructions","text":"This service provides a one-click deployment solution for large models based on ECS mirroring and VLLM. The QwQ-32B model can be deployed in 10 minutes and the Qwen3-235B-A22B model can be deployed in 30 minutes. This service uses ECS mirror packaging standard environments and realizes one-click deployment of cloud resources and large models through Ros templates. Developers do not need to care about the standard environment for model deployment and operation and the underlying cloud resource orchestration. They only need to add a few parameters to enjoy the inference experience of mainstream LLMs (such as Qwen, DeepSeek, etc.). The models supported by this service are as follows: * Qwen/Qwen3-235B-A22B * Qwen/Qwen3-32B * Qwen/Qwen3-8B * Qwen/QwQ-32B * Qwen/Qwen2.5-32B-Instruct * deepseek-ai/DeepSeek-R1-Distill-Llama-70B * deepseek-ai/DeepSeek-R1-Distill-Qwen-32B * deepseek-ai/DeepSeek-R1-Distill-Qwen-7B","title":"Deployment Instructions"},{"location":"index-ecs-one-en/#overall-architecture","text":"","title":"Overall architecture"},{"location":"index-ecs-one-en/#userclient-layer","text":"Access Method: Users can connect via Private VPC or Public Network. Security Measures: Utilizes an API Key for secure access. Response Mode: Supports streaming returns for real-time data processing.","title":"User/Client Layer"},{"location":"index-ecs-one-en/#service-layer","text":"Large-Scale Services: This layer provides large model services. Standard API Interface: It offers standardized API calls for easy integration and usage. Recommendation Engine: The service features a recommendation engine powered by VLLM services.","title":"Service Layer"},{"location":"index-ecs-one-en/#infrastructure-layer","text":"Network: It features a Virtual Private Cloud (VPC) network for secure data transmission. Computational Resources: This includes ECS image and ECS GPU instances for scalable computing power. ECS Image Contents: The layer manages the ECS image, which comprises the environment for running VLLM, as well as large model files.","title":"Infrastructure Layer"},{"location":"index-ecs-one-en/#billing-instructions","text":"The cost of this service on Alibaba Cloud mainly involves: * Specifications of the selected GPU cloud server * Number of nodes * Disk capacity * Public network bandwidth Billing method: pay by volume (hours) or annual monthly payment Estimated costs are visible in real time when creating an instance.","title":"Billing instructions"},{"location":"index-ecs-one-en/#required-permissions-for-ram-account","text":"To deploy service instances, some Alibaba Cloud resources need to be accessed and created.Therefore, your account needs permissions to include the following resources. Permission Policy Name Notes AliyunECSFullAccess Permissions to manage cloud server services (ECS) AliyunVPCFullAccess Permissions to manage proprietary networks (VPCs) AliyunROSFullAccess Permissions to manage resource orchestration services (ROS) AliyunComputeNestUserFullAccess Manage user-side permissions for ComputeNest","title":"Required permissions for RAM account"},{"location":"index-ecs-one-en/#deployment-process","text":"Click [Deployment Link] .Select standalone version.Fill in the parameters according to the prompts on the interface, and you can choose whether to turn on the public network according to your needs. You can see the corresponding inquiry details. After confirming the parameters, click Next: Confirm the order**. Click Next: Confirm the order ** and you can see the price preview. Then you can click Deploy now** and wait for the deployment to complete.(If the RAM permission is insufficient, you need to add RAM permissions to the sub-account) After the deployment is completed, you can start using the service.Click on the service instance name to enter the service instance details, and use the Api to call the sample to access the service.If it is an intranet access, you must ensure that the ECS instance is under the same VPC. After ssh accesses the ECS instance, execute docker logs vllm to query the model service deployment log.When you see the result shown in the figure below, it means that the model service is deployed successfully.The path where the model is located is /root/llm_model/.","title":"Deployment Process"},{"location":"index-ecs-one-en/#instructions-for-use","text":"","title":"Instructions for use"},{"location":"index-ecs-one-en/#query-model-deployment-parameters","text":"Copy the service instance name.Go to [Resource Orchestration Console] (https://ros.console.aliyun.com/cn-hangzhou/stacks) to view the corresponding resource stack. Enter the resource stack corresponding to the service instance, you can see all the resources opened and view all the scripts executed during the model deployment process.","title":"Query model deployment parameters"},{"location":"index-ecs-one-en/#customize-model-deployment-parameters","text":"If you have the requirement for custom model deployment parameters, you can modify it after deploying the service instance as follows. Remotely connect and log in to the ECS instance. Execute the following command to stop the model service. ```shell sudo docker stop vllm sudo docker rm vllm Please refer to the Query Model Deployment Parameters section in this document to obtain the scripts that the model deployment actually executes. The following are the reference scripts for vllm and sglang deployments. You can refer to the parameters to annotate custom model deployment parameters and modify the actual executed script. vllm deployment reference script ```shell docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name vllm \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-pytorch2.5.1-cuda12.4-ubuntu22.04 \\ -c \"pip install --upgrade vllm==0.8.2 && # Customizable version, such as pip install vllm==0.7.1 export GLOO_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify export NCCL_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify vllm serve /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --gpu-memory-utilization 0.98 \\ # Gpu occupancy rate, too high may cause other processes to trigger OOM.Value range: 0~1 --max-model-len ${MaxModelLen} \\ # The maximum length of the model, the value range is related to the model itself. --enable-chunked-prefill \\ --host=0.0.0.0 \\ --port 8000 \\ --trust-remote-code \\ --api-key \"${VLLM_API_KEY}\" \\ # Optional, can be removed if not required. --tensor-parallel-size $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}')\" # Number of GPUs used, all GPUs are used by default. sglang deployment reference script ```shell #Download a public image containing sglang docker pull egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 Docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name llm-server \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install sglang==0.4.3 && # Customizable version export GLOO_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify export NCCL_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify python3 -m sglang.launch_server \\ --model-path /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --tp $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}')\" \\ # Number of GPUs used, all GPUs are used by default. --trust-remote-code \\ --host 0.0.0.0 \\ --port 8000 \\ --mem-fraction-static 0.9 # Gpu occupancy rate, too high may cause other processes to trigger OOM.Value range: 0~1","title":"Customize model deployment parameters"},{"location":"index-ecs-one-en/#intranet-api-access","text":"Copy the Api call example and paste the Api call example in the ECS instance of the resource tab.It can also be accessed in other ECS within the same VPC.","title":"Intranet API access"},{"location":"index-ecs-one-en/#public-internet-api-access","text":"Copy the Api call example and paste the Api call example in the local terminal.","title":"Public Internet API access"},{"location":"index-ecs-one-en/#configure-the-vllm-api-using-the-chatbox-client-for-conversation-optional","text":"Visit Chatbox [Download Address] (https://chatboxai.app/zh#download) to download and install the client. This solution takes macOS M3 as an example. Run and configure the vLLM API and click Settings. Configure in the pop-up kanban according to the following table. Project Description Example Value Model Provider Pull down to select the model provider. Add a custom provider Name Fill in the name of the model provider. vLLM API API Domain Name Fill in the model service call address. http:// :8000 API Path Fill in the API Path. /v1/chat/completes Network Compatibility Click to enable to improve network compatibility Open API Key Fill in the Model Service Call API Key. After deploying the service instance, you can get Api_Key on the service instance page Model Fill in the called model. Qwen/QwQ-32B Save the configuration.The conversation can be performed in the text input box.Enter the question Who are you?Or after other instructions, the model service is called to obtain the corresponding response.","title":"Configure the vLLM API using the Chatbox client for conversation (optional)"},{"location":"index-ecs-one-en/#performance-testing","text":"","title":"Performance Testing"},{"location":"index-ecs-one-en/#pressure-test-process-for-reference","text":"**Prerequisites: ** 1. It is impossible to directly test model services with API-key; 2. Public network is required.","title":"Pressure test process (for reference)"},{"location":"index-ecs-one-en/#redeploy-the-model-service","text":"Remotely connect and log in to the ECS instance. Execute the following command to stop the model service. ```shell sudo docker stop vllm sudo docker rm vllm Please refer to the Query Model Deployment Parameters section in this document to obtain the scripts that the model deployment actually executes. Remove the --api-key parameter in the script and execute the remaining scripts in the ECS instance.Execute docker logs vllm.If the result is shown in the figure below, the model service is redeployed successfully.","title":"Redeploy the model service"},{"location":"index-ecs-one-en/#perform-performance-testing","text":"Taking QwQ-32B as an example, after the model service is deployed, ssh logs into the ECS instance.Execute the following command to get the model service performance test results.You can modify it yourself according to the parameter description. ```shell yum install -y git-lfs git lfs install git lfs clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git git lfs clone https://github.com/vllm-project/vllm.git docker exec vllm bash -c \" pip install pandas datasets && python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model /root/llm-model/Qwen/QwQ-32B \\ --served-model-name Qwen/QwQ-32B \\ --sonnet-input-len 1024 \\ # Maximum input length --sonnet-output-len 4096 \\ # Maximum output length --sonnet-prefix-len 50 \\ # Prefix length --num-prompts 400 \\ # Randomly select or process 400 prompts from the dataset for performance testing. --request-rate 20 \\ # Simulate a stress test of 20 concurrent requests per second, lasting 20 seconds, with a total of 400 requests.Evaluate the throughput and latency of the model service under load. --port 8000 \\ --trust-remote-code \\ --dataset-name sharegpt \\ --save-result \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \" ```","title":"Perform performance testing"},{"location":"index-ecs-one-en/#qwen3-235b-a22b-stress-test-results","text":"Under this service plan, for Qwen3-235B-A22B, the inference response performance of the model service was tested under the GU8TF instance specification with a QPS of 20, and the stress test duration was 1 minute.","title":"Qwen3-235B-A22B stress test results"},{"location":"index-ecs-one-en/#gu8tf-specification","text":"","title":"GU8TF specification"},{"location":"index-ecs-one-en/#qps-is-20-1200-question-and-answer-requests-in-1-minute","text":"","title":"QPS is 20, 1200 question and answer requests in 1 minute"},{"location":"index-ecs-one-en/#qps-is-50-3000-qa-requests-per-minute","text":"","title":"QPS is 50, 3000 Q&amp;A requests per minute"},{"location":"index-ecs-one-en/#qwen3-32b-stress-test-results","text":"Under this service plan, for Qwen3-32B, the inference response performance of the model service is tested under the ecs.gn7i-8x.16xlarge (8*A10) and GU8TF instance specifications, respectively, with a QPS of 20. The stress test duration is 1 minute.","title":"Qwen3-32B stress test results"},{"location":"index-ecs-one-en/#8a10-specification","text":"","title":"8*A10 specification"},{"location":"index-ecs-one-en/#qps-is-20-1200-qa-requests-per-minute","text":"","title":"QPS is 20, 1200 Q&amp;A requests per minute"},{"location":"index-ecs-one-en/#qps-is-50-3000-qa-requests-per-minute_1","text":"","title":"QPS is 50, 3000 Q&amp;A requests per minute"},{"location":"index-ecs-one-en/#qwq-32b-stress-test-results","text":"Under this service plan, the inference response performance of the model service under the 4 A10 and 8 A10 instance specifications were tested respectively for the inference response performance of the model service with a QPS of 10, 20 and 50, and the pressure measurement duration was 20s.","title":"QwQ-32B stress test results"},{"location":"index-ecs-one-en/#8a10-specifications","text":"","title":"8*A10 specifications"},{"location":"index-ecs-one-en/#qps-is-10","text":"","title":"QPS is 10"},{"location":"index-ecs-one-en/#qps-is-20","text":"","title":"QPS is 20"},{"location":"index-ecs-one-en/#qps-is-50","text":"","title":"QPS is 50"},{"location":"index-ecs-one-en/#4a10-specifications","text":"","title":"4*A10 specifications"},{"location":"index-ecs-one-en/#qps-is-10_1","text":"","title":"QPS is 10"},{"location":"index-ecs-one-en/#qps-is-20_1","text":"","title":"QPS is 20"},{"location":"index-ecs-one-en/#qps-is-50_1","text":"","title":"QPS is 50"},{"location":"index-ecs-one/","text":"\u57fa\u4e8e\u5355ECS\u5b9e\u4f8b\u7684LLM\u6a21\u578b\u90e8\u7f72\u6587\u6863 \u90e8\u7f72\u8bf4\u660e \u672c\u670d\u52a1\u63d0\u4f9b\u4e86\u57fa\u4e8eECS\u955c\u50cf\u4e0eVLLM\u7684\u5927\u6a21\u578b\u4e00\u952e\u90e8\u7f72\u65b9\u6848\uff0c10\u5206\u949f\u5373\u53ef\u90e8\u7f72\u4f7f\u7528QwQ-32B\u6a21\u578b\uff0c30\u5206\u949f\u5373\u53ef\u90e8\u7f72\u4f7f\u7528Qwen3-235B-A22B\u6a21\u578b\u3002 \u672c\u670d\u52a1\u901a\u8fc7ECS\u955c\u50cf\u6253\u5305\u6807\u51c6\u73af\u5883\uff0c\u901a\u8fc7Ros\u6a21\u7248\u5b9e\u73b0\u4e91\u8d44\u6e90\u4e0e\u5927\u6a21\u578b\u7684\u4e00\u952e\u90e8\u7f72\uff0c\u5f00\u53d1\u8005\u65e0\u9700\u5173\u5fc3\u6a21\u578b\u90e8\u7f72\u8fd0\u884c\u7684\u6807\u51c6\u73af\u5883\u4e0e\u5e95\u5c42\u4e91\u8d44\u6e90\u7f16\u6392\uff0c\u4ec5\u9700\u6dfb\u52a0\u51e0\u4e2a\u53c2\u6570\u5373\u53ef\u4eab\u53d7\u4e3b\u6d41LLM\uff08\u5982Qwen\u3001DeepSeek\u7b49\uff09\u7684\u63a8\u7406\u4f53\u9a8c\u3002 \u672c\u670d\u52a1\u652f\u6301\u7684\u6a21\u578b\u5982\u4e0b\uff1a * Qwen/Qwen3-235B-A22B * Qwen/Qwen3-32B * Qwen/Qwen3-8B * Qwen/QwQ-32B * Qwen/Qwen2.5-32B-Instruct * deepseek-ai/DeepSeek-R1-Distill-Llama-70B * deepseek-ai/DeepSeek-R1-Distill-Qwen-32B * deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \u6574\u4f53\u67b6\u6784 \u8ba1\u8d39\u8bf4\u660e \u672c\u670d\u52a1\u5728\u963f\u91cc\u4e91\u4e0a\u7684\u8d39\u7528\u4e3b\u8981\u6d89\u53ca\uff1a * \u6240\u9009GPU\u4e91\u670d\u52a1\u5668\u7684\u89c4\u683c * \u8282\u70b9\u6570\u91cf * \u78c1\u76d8\u5bb9\u91cf * \u516c\u7f51\u5e26\u5bbd \u8ba1\u8d39\u65b9\u5f0f\uff1a\u6309\u91cf\u4ed8\u8d39\uff08\u5c0f\u65f6\uff09\u6216\u5305\u5e74\u5305\u6708 \u9884\u4f30\u8d39\u7528\u5728\u521b\u5efa\u5b9e\u4f8b\u65f6\u53ef\u5b9e\u65f6\u770b\u5230\u3002 RAM\u8d26\u53f7\u6240\u9700\u6743\u9650 \u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\uff0c\u9700\u8981\u5bf9\u90e8\u5206\u963f\u91cc\u4e91\u8d44\u6e90\u8fdb\u884c\u8bbf\u95ee\u548c\u521b\u5efa\u64cd\u4f5c\u3002\u56e0\u6b64\u60a8\u7684\u8d26\u53f7\u9700\u8981\u5305\u542b\u5982\u4e0b\u8d44\u6e90\u7684\u6743\u9650\u3002 \u6743\u9650\u7b56\u7565\u540d\u79f0 \u5907\u6ce8 AliyunECSFullAccess \u7ba1\u7406\u4e91\u670d\u52a1\u5668\u670d\u52a1\uff08ECS\uff09\u7684\u6743\u9650 AliyunVPCFullAccess \u7ba1\u7406\u4e13\u6709\u7f51\u7edc\uff08VPC\uff09\u7684\u6743\u9650 AliyunROSFullAccess \u7ba1\u7406\u8d44\u6e90\u7f16\u6392\u670d\u52a1\uff08ROS\uff09\u7684\u6743\u9650 AliyunComputeNestUserFullAccess \u7ba1\u7406\u8ba1\u7b97\u5de2\u670d\u52a1\uff08ComputeNest\uff09\u7684\u7528\u6237\u4fa7\u6743\u9650 \u90e8\u7f72\u6d41\u7a0b \u5355\u51fb \u90e8\u7f72\u94fe\u63a5 \u3002\u9009\u62e9\u5355\u673a\u7248\u3002\u6839\u636e\u754c\u9762\u63d0\u793a\u586b\u5199\u53c2\u6570\uff0c\u53ef\u6839\u636e\u9700\u6c42\u9009\u62e9\u662f\u5426\u5f00\u542f\u516c\u7f51\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u8be2\u4ef7\u660e\u7ec6\uff0c\u786e\u8ba4\u53c2\u6570\u540e\u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u3002 \u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u540e\u53ef\u4ee5\u770b\u5230\u4ef7\u683c\u9884\u89c8\uff0c\u968f\u540e\u53ef\u70b9\u51fb \u7acb\u5373\u90e8\u7f72 \uff0c\u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u3002(\u63d0\u793aRAM\u6743\u9650\u4e0d\u8db3\u65f6\u9700\u8981\u4e3a\u5b50\u8d26\u53f7\u6dfb\u52a0RAM\u6743\u9650) \u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u540e\uff0c\u5c31\u53ef\u4ee5\u5f00\u59cb\u4f7f\u7528\u670d\u52a1\u4e86\u3002\u70b9\u51fb\u670d\u52a1\u5b9e\u4f8b\u540d\u79f0\uff0c\u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\uff0c\u4f7f\u7528Api\u8c03\u7528\u793a\u4f8b\u5373\u53ef\u8bbf\u95ee\u670d\u52a1\u3002\u5982\u679c\u662f\u5185\u7f51\u8bbf\u95ee\uff0c\u9700\u4fdd\u8bc1ECS\u5b9e\u4f8b\u5728\u540c\u4e00\u4e2aVPC\u4e0b\u3002 ssh\u8bbf\u95eeECS\u5b9e\u4f8b\u540e\uff0c\u6267\u884c docker logs vllm \u5373\u53ef\u67e5\u8be2\u6a21\u578b\u670d\u52a1\u90e8\u7f72\u65e5\u5fd7\u3002\u5f53\u60a8\u770b\u5230\u4e0b\u56fe\u6240\u793a\u7ed3\u679c\u65f6\uff0c\u8868\u793a\u6a21\u578b\u670d\u52a1\u90e8\u7f72\u6210\u529f\u3002\u6a21\u578b\u6240\u5728\u8def\u5f84\u4e3a/root/llm_model/\u3002 \u4f7f\u7528\u8bf4\u660e \u67e5\u8be2\u6a21\u578b\u90e8\u7f72\u53c2\u6570 \u590d\u5236\u670d\u52a1\u5b9e\u4f8b\u540d\u79f0\u3002\u5230 \u8d44\u6e90\u7f16\u6392\u63a7\u5236\u53f0 \u67e5\u770b\u5bf9\u5e94\u7684\u8d44\u6e90\u6808\u3002 \u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u5bf9\u5e94\u7684\u8d44\u6e90\u6808\uff0c\u53ef\u4ee5\u770b\u5230\u6240\u5f00\u542f\u7684\u5168\u90e8\u8d44\u6e90\uff0c\u5e76\u67e5\u770b\u5230\u6a21\u578b\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u6267\u884c\u7684\u5168\u90e8\u811a\u672c\u3002 \u81ea\u5b9a\u4e49\u6a21\u578b\u90e8\u7f72\u53c2\u6570 \u5982\u679c\u60a8\u6709\u81ea\u5b9a\u4e49\u7684\u6a21\u578b\u90e8\u7f72\u53c2\u6570\u7684\u9700\u6c42\uff0c\u53ef\u4ee5\u5728\u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\u540e\uff0c\u6309\u7167\u5982\u4e0b\u64cd\u4f5c\u6b65\u9aa4\u8fdb\u884c\u4fee\u6539\u3002 \u8fdc\u7a0b\u8fde\u63a5\uff0c\u767b\u5165ECS\u5b9e\u4f8b\u3002 \u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5c06\u6a21\u578b\u670d\u52a1\u505c\u6b62\u3002 ```shell sudo docker stop vllm sudo docker rm vllm \u8bf7\u53c2\u8003\u672c\u6587\u6863\u4e2d\u7684 \u67e5\u8be2\u6a21\u578b\u90e8\u7f72\u53c2\u6570 \u90e8\u5206\uff0c\u83b7\u53d6\u6a21\u578b\u90e8\u7f72\u5b9e\u9645\u6267\u884c\u7684\u811a\u672c\u3002 \u4e0b\u9762\u5206\u522b\u662fvllm\u4e0esglang\u90e8\u7f72\u7684\u53c2\u8003\u811a\u672c\uff0c\u60a8\u53ef\u53c2\u8003\u53c2\u6570\u6ce8\u91ca\u81ea\u5b9a\u4e49\u6a21\u578b\u90e8\u7f72\u53c2\u6570\uff0c\u4fee\u6539\u5b9e\u9645\u6267\u884c\u7684\u811a\u672c\u3002 vllm\u90e8\u7f72\u53c2\u8003\u811a\u672c ```shell docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name vllm \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-pytorch2.5.1-cuda12.4-ubuntu22.04 \\ -c \"pip install --upgrade vllm==0.8.2 && # \u53ef\u81ea\u5b9a\u4e49\u7248\u672c\uff0c\u5982 pip install vllm==0.7.1 export GLOO_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 export NCCL_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 vllm serve /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --gpu-memory-utilization 0.98 \\ # Gpu\u5360\u7528\u7387\uff0c\u8fc7\u9ad8\u53ef\u80fd\u5bfc\u81f4\u5176\u4ed6\u8fdb\u7a0b\u89e6\u53d1OOM\u3002\u53d6\u503c\u8303\u56f4:0~1 --max-model-len ${MaxModelLen} \\ # \u6a21\u578b\u6700\u5927\u957f\u5ea6\uff0c\u53d6\u503c\u8303\u56f4\u4e0e\u6a21\u578b\u672c\u8eab\u6709\u5173\u3002 --enable-chunked-prefill \\ --host=0.0.0.0 \\ --port 8000 \\ --trust-remote-code \\ --api-key \"${VLLM_API_KEY}\" \\ # \u53ef\u9009\uff0c\u5982\u4e0d\u9700\u8981\u53ef\u53bb\u6389\u3002 --tensor-parallel-size $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}')\" # \u4f7f\u7528GPU\u6570\u91cf\uff0c\u9ed8\u8ba4\u4f7f\u7528\u5168\u90e8GPU\u3002 sglang\u90e8\u7f72\u53c2\u8003\u811a\u672c ```shell #\u4e0b\u8f7d\u5305\u542bsglang\u7684\u516c\u5f00\u955c\u50cf docker pull egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name llm-server \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install sglang==0.4.3 && # \u53ef\u81ea\u5b9a\u4e49\u7248\u672c export GLOO_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 export NCCL_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 python3 -m sglang.launch_server \\ --model-path /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --tp $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}')\" \\ # \u4f7f\u7528GPU\u6570\u91cf\uff0c\u9ed8\u8ba4\u4f7f\u7528\u5168\u90e8GPU\u3002 --trust-remote-code \\ --host 0.0.0.0 \\ --port 8000 \\ --mem-fraction-static 0.9 # Gpu\u5360\u7528\u7387\uff0c\u8fc7\u9ad8\u53ef\u80fd\u5bfc\u81f4\u5176\u4ed6\u8fdb\u7a0b\u89e6\u53d1OOM\u3002\u53d6\u503c\u8303\u56f4:0~1 \u5185\u7f51API\u8bbf\u95ee \u590d\u5236Api\u8c03\u7528\u793a\u4f8b\uff0c\u5728\u8d44\u6e90\u6807\u7b7e\u9875\u7684ECS\u5b9e\u4f8b\u4e2d\u7c98\u8d34Api\u8c03\u7528\u793a\u4f8b\u5373\u53ef\u3002\u4e5f\u53ef\u5728\u540c\u4e00VPC\u5185\u7684\u5176\u4ed6ECS\u4e2d\u8bbf\u95ee\u3002 \u516c\u7f51API\u8bbf\u95ee \u590d\u5236Api\u8c03\u7528\u793a\u4f8b\uff0c\u5728\u672c\u5730\u7ec8\u7aef\u4e2d\u7c98\u8d34Api\u8c03\u7528\u793a\u4f8b\u5373\u53ef\u3002 \u4f7f\u7528 Chatbox \u5ba2\u6237\u7aef\u914d\u7f6e vLLM API \u8fdb\u884c\u5bf9\u8bdd(\u53ef\u9009) \u8bbf\u95ee Chatbox \u4e0b\u8f7d\u5730\u5740 \u4e0b\u8f7d\u5e76\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u672c\u65b9\u6848\u4ee5 macOS M3 \u4e3a\u4f8b\u3002 \u8fd0\u884c\u5e76\u914d\u7f6e vLLM API \uff0c\u5355\u51fb\u8bbe\u7f6e\u3002 \u5728\u5f39\u51fa\u7684\u770b\u677f\u4e2d\u6309\u7167\u5982\u4e0b\u8868\u683c\u8fdb\u884c\u914d\u7f6e\u3002 \u9879\u76ee \u8bf4\u660e \u793a\u4f8b\u503c \u6a21\u578b\u63d0\u4f9b\u65b9 \u4e0b\u62c9\u9009\u62e9\u6a21\u578b\u63d0\u4f9b\u65b9\u3002 \u6dfb\u52a0\u81ea\u5b9a\u4e49\u63d0\u4f9b\u65b9 \u540d\u79f0 \u586b\u5199\u5b9a\u4e49\u6a21\u578b\u63d0\u4f9b\u65b9\u540d\u79f0\u3002 vLLM API API \u57df\u540d \u586b\u5199\u6a21\u578b\u670d\u52a1\u8c03\u7528\u5730\u5740\u3002 http:// :8000 API \u8def\u5f84 \u586b\u5199 API \u8def\u5f84\u3002 /v1/chat/completions \u7f51\u7edc\u517c\u5bb9\u6027 \u70b9\u51fb\u5f00\u542f\u6539\u5584\u7f51\u7edc\u517c\u5bb9\u6027 \u5f00\u542f API \u5bc6\u94a5 \u586b\u5199\u6a21\u578b\u670d\u52a1\u8c03\u7528 API \u5bc6\u94a5\u3002 \u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\u540e\uff0c\u5728\u670d\u52a1\u5b9e\u4f8b\u9875\u9762\u53ef\u83b7\u53d6Api_Key \u6a21\u578b \u586b\u5199\u8c03\u7528\u7684\u6a21\u578b\u3002 Qwen/QwQ-32B \u4fdd\u5b58\u914d\u7f6e\u3002\u5728\u6587\u672c\u8f93\u5165\u6846\u4e2d\u53ef\u4ee5\u8fdb\u884c\u5bf9\u8bdd\u4ea4\u4e92\u3002\u8f93\u5165\u95ee\u9898\u4f60\u662f\u8c01\uff1f\u6216\u8005\u5176\u4ed6\u6307\u4ee4\u540e\uff0c\u8c03\u7528\u6a21\u578b\u670d\u52a1\u83b7\u5f97\u76f8\u5e94\u7684\u54cd\u5e94\u3002 \u6027\u80fd\u6d4b\u8bd5 \u538b\u6d4b\u8fc7\u7a0b(\u4f9b\u53c2\u8003) \u524d\u63d0\u6761\u4ef6\uff1a 1. \u65e0\u6cd5\u76f4\u63a5\u6d4b\u8bd5\u5e26api-key\u7684\u6a21\u578b\u670d\u52a1\uff1b2. \u9700\u8981\u516c\u7f51\u3002 \u91cd\u65b0\u90e8\u7f72\u6a21\u578b\u670d\u52a1 \u8fdc\u7a0b\u8fde\u63a5\uff0c\u767b\u5165ECS\u5b9e\u4f8b\u3002 \u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5c06\u6a21\u578b\u670d\u52a1\u505c\u6b62\u3002 ```shell sudo docker stop vllm sudo docker rm vllm \u8bf7\u53c2\u8003\u672c\u6587\u6863\u4e2d\u7684 \u67e5\u8be2\u6a21\u578b\u90e8\u7f72\u53c2\u6570 \u90e8\u5206\uff0c\u83b7\u53d6\u6a21\u578b\u90e8\u7f72\u5b9e\u9645\u6267\u884c\u7684\u811a\u672c\u3002 \u53bb\u6389\u811a\u672c\u4e2d\u7684--api-key\u53c2\u6570\uff0c\u5728ECS\u5b9e\u4f8b\u4e2d\u6267\u884c\u5269\u4f59\u811a\u672c\u3002\u6267\u884cdocker logs vllm\u3002\u82e5\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u6a21\u578b\u670d\u52a1\u91cd\u65b0\u90e8\u7f72\u6210\u529f\u3002 \u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5 \u4ee5QwQ-32B\u4e3a\u4f8b\uff0c\u6a21\u578b\u670d\u52a1\u90e8\u7f72\u5b8c\u6210\u540e\uff0cssh\u767b\u5f55ECS\u5b9e\u4f8b\u3002\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5373\u53ef\u5f97\u5230\u6a21\u578b\u670d\u52a1\u6027\u80fd\u6d4b\u8bd5\u7ed3\u679c\u3002\u53ef\u6839\u636e\u53c2\u6570\u8bf4\u660e\u81ea\u884c\u4fee\u6539\u3002 ```shell yum install -y git-lfs git lfs install git lfs clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git git lfs clone https://github.com/vllm-project/vllm.git docker exec vllm bash -c \" pip install pandas datasets && python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model /root/llm-model/Qwen/QwQ-32B \\ --served-model-name Qwen/QwQ-32B \\ --sonnet-input-len 1024 \\ # \u6700\u5927\u8f93\u5165\u957f\u5ea6 --sonnet-output-len 4096 \\ # \u6700\u5927\u8f93\u51fa\u957f\u5ea6 --sonnet-prefix-len 50 \\ # \u524d\u7f00\u957f\u5ea6 --num-prompts 400 \\ # \u4ece\u6570\u636e\u96c6\u4e2d\u968f\u673a\u9009\u53d6\u6216\u6309\u987a\u5e8f\u5904\u7406 400 \u4e2a prompt \u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002 --request-rate 20 \\ # \u6a21\u62df\u6bcf\u79d2 20 \u4e2a\u5e76\u53d1\u8bf7\u6c42\u7684\u538b\u529b\u6d4b\u8bd5\uff0c\u6301\u7eed20\u79d2\uff0c\u5171400\u4e2a\u8bf7\u6c42\u3002\u8bc4\u4f30\u6a21\u578b\u670d\u52a1\u5728\u8d1f\u8f7d\u4e0b\u7684\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u3002 --port 8000 \\ --trust-remote-code \\ --dataset-name sharegpt \\ --save-result \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \" ``` \u6027\u80fd\u6d4b\u8bd5\u7ed3\u679c Qwen3-235B-A22B\u538b\u6d4b\u7ed3\u679c \u672c\u670d\u52a1\u65b9\u6848\u4e0b\uff0c\u9488\u5bf9Qwen3-235B-A22B\u5728GU8TF\u5b9e\u4f8b\u89c4\u683c\u4e0b\uff0c\u5206\u522b\u6d4b\u8bd5QPS\u4e3a20\u60c5\u51b5\u4e0b\u6a21\u578b\u670d\u52a1\u7684\u63a8\u7406\u54cd\u5e94\u6027\u80fd\uff0c\u538b\u6d4b\u6301\u7eed\u65f6\u95f4\u5747\u4e3a1\u5206\u949f\u3002 GU8TF\u89c4\u683c QPS\u4e3a20\uff0c1\u5206\u949f1200\u4e2a\u95ee\u7b54\u8bf7\u6c42 QPS\u4e3a50\uff0c1\u5206\u949f3000\u4e2a\u95ee\u7b54\u8bf7\u6c42 Qwen3-32B\u538b\u6d4b\u7ed3\u679c \u672c\u670d\u52a1\u65b9\u6848\u4e0b\uff0c\u9488\u5bf9Qwen3-32B\u5728ecs.gn7i-8x.16xlarge\uff088*A10\uff09\u5b9e\u4f8b\u89c4\u683c\u4e0b\uff0c\u5206\u522b\u6d4b\u8bd5QPS\u4e3a20\u60c5\u51b5\u4e0b\u6a21\u578b\u670d\u52a1\u7684\u63a8\u7406\u54cd\u5e94\u6027\u80fd\uff0c\u538b\u6d4b\u6301\u7eed\u65f6\u95f4\u5747\u4e3a1\u5206\u949f\u3002 8*A10\u89c4\u683c QPS\u4e3a20\uff0c1\u5206\u949f1200\u4e2a\u95ee\u7b54\u8bf7\u6c42 QPS\u4e3a50\uff0c1\u5206\u949f3000\u4e2a\u95ee\u7b54\u8bf7\u6c42 QwQ-32B\u538b\u6d4b\u7ed3\u679c \u672c\u670d\u52a1\u65b9\u6848\u4e0b\uff0c\u9488\u5bf9QwQ-32B\u57284 A10\u548c8 A10\u5b9e\u4f8b\u89c4\u683c\u4e0b\uff0c\u5206\u522b\u6d4b\u8bd5QPS\u4e3a10\u300120\u300150\u60c5\u51b5\u4e0b\u6a21\u578b\u670d\u52a1\u7684\u63a8\u7406\u54cd\u5e94\u6027\u80fd\uff0c\u538b\u6d4b\u6301\u7eed\u65f6\u95f4\u5747\u4e3a20s\u3002 8*A10\u89c4\u683c QPS\u4e3a10 QPS\u4e3a20 QPS\u4e3a50 4*A10\u89c4\u683c QPS\u4e3a10 QPS\u4e3a20 QPS\u4e3a50","title":"\u57fa\u4e8e\u5355ECS\u5b9e\u4f8b\u7684LLM\u6a21\u578b\u90e8\u7f72\u6587\u6863"},{"location":"index-ecs-one/#ecsllm","text":"","title":"\u57fa\u4e8e\u5355ECS\u5b9e\u4f8b\u7684LLM\u6a21\u578b\u90e8\u7f72\u6587\u6863"},{"location":"index-ecs-one/#_1","text":"\u672c\u670d\u52a1\u63d0\u4f9b\u4e86\u57fa\u4e8eECS\u955c\u50cf\u4e0eVLLM\u7684\u5927\u6a21\u578b\u4e00\u952e\u90e8\u7f72\u65b9\u6848\uff0c10\u5206\u949f\u5373\u53ef\u90e8\u7f72\u4f7f\u7528QwQ-32B\u6a21\u578b\uff0c30\u5206\u949f\u5373\u53ef\u90e8\u7f72\u4f7f\u7528Qwen3-235B-A22B\u6a21\u578b\u3002 \u672c\u670d\u52a1\u901a\u8fc7ECS\u955c\u50cf\u6253\u5305\u6807\u51c6\u73af\u5883\uff0c\u901a\u8fc7Ros\u6a21\u7248\u5b9e\u73b0\u4e91\u8d44\u6e90\u4e0e\u5927\u6a21\u578b\u7684\u4e00\u952e\u90e8\u7f72\uff0c\u5f00\u53d1\u8005\u65e0\u9700\u5173\u5fc3\u6a21\u578b\u90e8\u7f72\u8fd0\u884c\u7684\u6807\u51c6\u73af\u5883\u4e0e\u5e95\u5c42\u4e91\u8d44\u6e90\u7f16\u6392\uff0c\u4ec5\u9700\u6dfb\u52a0\u51e0\u4e2a\u53c2\u6570\u5373\u53ef\u4eab\u53d7\u4e3b\u6d41LLM\uff08\u5982Qwen\u3001DeepSeek\u7b49\uff09\u7684\u63a8\u7406\u4f53\u9a8c\u3002 \u672c\u670d\u52a1\u652f\u6301\u7684\u6a21\u578b\u5982\u4e0b\uff1a * Qwen/Qwen3-235B-A22B * Qwen/Qwen3-32B * Qwen/Qwen3-8B * Qwen/QwQ-32B * Qwen/Qwen2.5-32B-Instruct * deepseek-ai/DeepSeek-R1-Distill-Llama-70B * deepseek-ai/DeepSeek-R1-Distill-Qwen-32B * deepseek-ai/DeepSeek-R1-Distill-Qwen-7B","title":"\u90e8\u7f72\u8bf4\u660e"},{"location":"index-ecs-one/#_2","text":"","title":"\u6574\u4f53\u67b6\u6784"},{"location":"index-ecs-one/#_3","text":"\u672c\u670d\u52a1\u5728\u963f\u91cc\u4e91\u4e0a\u7684\u8d39\u7528\u4e3b\u8981\u6d89\u53ca\uff1a * \u6240\u9009GPU\u4e91\u670d\u52a1\u5668\u7684\u89c4\u683c * \u8282\u70b9\u6570\u91cf * \u78c1\u76d8\u5bb9\u91cf * \u516c\u7f51\u5e26\u5bbd \u8ba1\u8d39\u65b9\u5f0f\uff1a\u6309\u91cf\u4ed8\u8d39\uff08\u5c0f\u65f6\uff09\u6216\u5305\u5e74\u5305\u6708 \u9884\u4f30\u8d39\u7528\u5728\u521b\u5efa\u5b9e\u4f8b\u65f6\u53ef\u5b9e\u65f6\u770b\u5230\u3002","title":"\u8ba1\u8d39\u8bf4\u660e"},{"location":"index-ecs-one/#ram","text":"\u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\uff0c\u9700\u8981\u5bf9\u90e8\u5206\u963f\u91cc\u4e91\u8d44\u6e90\u8fdb\u884c\u8bbf\u95ee\u548c\u521b\u5efa\u64cd\u4f5c\u3002\u56e0\u6b64\u60a8\u7684\u8d26\u53f7\u9700\u8981\u5305\u542b\u5982\u4e0b\u8d44\u6e90\u7684\u6743\u9650\u3002 \u6743\u9650\u7b56\u7565\u540d\u79f0 \u5907\u6ce8 AliyunECSFullAccess \u7ba1\u7406\u4e91\u670d\u52a1\u5668\u670d\u52a1\uff08ECS\uff09\u7684\u6743\u9650 AliyunVPCFullAccess \u7ba1\u7406\u4e13\u6709\u7f51\u7edc\uff08VPC\uff09\u7684\u6743\u9650 AliyunROSFullAccess \u7ba1\u7406\u8d44\u6e90\u7f16\u6392\u670d\u52a1\uff08ROS\uff09\u7684\u6743\u9650 AliyunComputeNestUserFullAccess \u7ba1\u7406\u8ba1\u7b97\u5de2\u670d\u52a1\uff08ComputeNest\uff09\u7684\u7528\u6237\u4fa7\u6743\u9650","title":"RAM\u8d26\u53f7\u6240\u9700\u6743\u9650"},{"location":"index-ecs-one/#_4","text":"\u5355\u51fb \u90e8\u7f72\u94fe\u63a5 \u3002\u9009\u62e9\u5355\u673a\u7248\u3002\u6839\u636e\u754c\u9762\u63d0\u793a\u586b\u5199\u53c2\u6570\uff0c\u53ef\u6839\u636e\u9700\u6c42\u9009\u62e9\u662f\u5426\u5f00\u542f\u516c\u7f51\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u8be2\u4ef7\u660e\u7ec6\uff0c\u786e\u8ba4\u53c2\u6570\u540e\u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u3002 \u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u540e\u53ef\u4ee5\u770b\u5230\u4ef7\u683c\u9884\u89c8\uff0c\u968f\u540e\u53ef\u70b9\u51fb \u7acb\u5373\u90e8\u7f72 \uff0c\u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u3002(\u63d0\u793aRAM\u6743\u9650\u4e0d\u8db3\u65f6\u9700\u8981\u4e3a\u5b50\u8d26\u53f7\u6dfb\u52a0RAM\u6743\u9650) \u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u540e\uff0c\u5c31\u53ef\u4ee5\u5f00\u59cb\u4f7f\u7528\u670d\u52a1\u4e86\u3002\u70b9\u51fb\u670d\u52a1\u5b9e\u4f8b\u540d\u79f0\uff0c\u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\uff0c\u4f7f\u7528Api\u8c03\u7528\u793a\u4f8b\u5373\u53ef\u8bbf\u95ee\u670d\u52a1\u3002\u5982\u679c\u662f\u5185\u7f51\u8bbf\u95ee\uff0c\u9700\u4fdd\u8bc1ECS\u5b9e\u4f8b\u5728\u540c\u4e00\u4e2aVPC\u4e0b\u3002 ssh\u8bbf\u95eeECS\u5b9e\u4f8b\u540e\uff0c\u6267\u884c docker logs vllm \u5373\u53ef\u67e5\u8be2\u6a21\u578b\u670d\u52a1\u90e8\u7f72\u65e5\u5fd7\u3002\u5f53\u60a8\u770b\u5230\u4e0b\u56fe\u6240\u793a\u7ed3\u679c\u65f6\uff0c\u8868\u793a\u6a21\u578b\u670d\u52a1\u90e8\u7f72\u6210\u529f\u3002\u6a21\u578b\u6240\u5728\u8def\u5f84\u4e3a/root/llm_model/\u3002","title":"\u90e8\u7f72\u6d41\u7a0b"},{"location":"index-ecs-one/#_5","text":"","title":"\u4f7f\u7528\u8bf4\u660e"},{"location":"index-ecs-one/#_6","text":"\u590d\u5236\u670d\u52a1\u5b9e\u4f8b\u540d\u79f0\u3002\u5230 \u8d44\u6e90\u7f16\u6392\u63a7\u5236\u53f0 \u67e5\u770b\u5bf9\u5e94\u7684\u8d44\u6e90\u6808\u3002 \u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u5bf9\u5e94\u7684\u8d44\u6e90\u6808\uff0c\u53ef\u4ee5\u770b\u5230\u6240\u5f00\u542f\u7684\u5168\u90e8\u8d44\u6e90\uff0c\u5e76\u67e5\u770b\u5230\u6a21\u578b\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u6267\u884c\u7684\u5168\u90e8\u811a\u672c\u3002","title":"\u67e5\u8be2\u6a21\u578b\u90e8\u7f72\u53c2\u6570"},{"location":"index-ecs-one/#_7","text":"\u5982\u679c\u60a8\u6709\u81ea\u5b9a\u4e49\u7684\u6a21\u578b\u90e8\u7f72\u53c2\u6570\u7684\u9700\u6c42\uff0c\u53ef\u4ee5\u5728\u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\u540e\uff0c\u6309\u7167\u5982\u4e0b\u64cd\u4f5c\u6b65\u9aa4\u8fdb\u884c\u4fee\u6539\u3002 \u8fdc\u7a0b\u8fde\u63a5\uff0c\u767b\u5165ECS\u5b9e\u4f8b\u3002 \u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5c06\u6a21\u578b\u670d\u52a1\u505c\u6b62\u3002 ```shell sudo docker stop vllm sudo docker rm vllm \u8bf7\u53c2\u8003\u672c\u6587\u6863\u4e2d\u7684 \u67e5\u8be2\u6a21\u578b\u90e8\u7f72\u53c2\u6570 \u90e8\u5206\uff0c\u83b7\u53d6\u6a21\u578b\u90e8\u7f72\u5b9e\u9645\u6267\u884c\u7684\u811a\u672c\u3002 \u4e0b\u9762\u5206\u522b\u662fvllm\u4e0esglang\u90e8\u7f72\u7684\u53c2\u8003\u811a\u672c\uff0c\u60a8\u53ef\u53c2\u8003\u53c2\u6570\u6ce8\u91ca\u81ea\u5b9a\u4e49\u6a21\u578b\u90e8\u7f72\u53c2\u6570\uff0c\u4fee\u6539\u5b9e\u9645\u6267\u884c\u7684\u811a\u672c\u3002 vllm\u90e8\u7f72\u53c2\u8003\u811a\u672c ```shell docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name vllm \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-pytorch2.5.1-cuda12.4-ubuntu22.04 \\ -c \"pip install --upgrade vllm==0.8.2 && # \u53ef\u81ea\u5b9a\u4e49\u7248\u672c\uff0c\u5982 pip install vllm==0.7.1 export GLOO_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 export NCCL_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 vllm serve /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --gpu-memory-utilization 0.98 \\ # Gpu\u5360\u7528\u7387\uff0c\u8fc7\u9ad8\u53ef\u80fd\u5bfc\u81f4\u5176\u4ed6\u8fdb\u7a0b\u89e6\u53d1OOM\u3002\u53d6\u503c\u8303\u56f4:0~1 --max-model-len ${MaxModelLen} \\ # \u6a21\u578b\u6700\u5927\u957f\u5ea6\uff0c\u53d6\u503c\u8303\u56f4\u4e0e\u6a21\u578b\u672c\u8eab\u6709\u5173\u3002 --enable-chunked-prefill \\ --host=0.0.0.0 \\ --port 8000 \\ --trust-remote-code \\ --api-key \"${VLLM_API_KEY}\" \\ # \u53ef\u9009\uff0c\u5982\u4e0d\u9700\u8981\u53ef\u53bb\u6389\u3002 --tensor-parallel-size $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}')\" # \u4f7f\u7528GPU\u6570\u91cf\uff0c\u9ed8\u8ba4\u4f7f\u7528\u5168\u90e8GPU\u3002 sglang\u90e8\u7f72\u53c2\u8003\u811a\u672c ```shell #\u4e0b\u8f7d\u5305\u542bsglang\u7684\u516c\u5f00\u955c\u50cf docker pull egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name llm-server \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install sglang==0.4.3 && # \u53ef\u81ea\u5b9a\u4e49\u7248\u672c export GLOO_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 export NCCL_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 python3 -m sglang.launch_server \\ --model-path /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --tp $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}')\" \\ # \u4f7f\u7528GPU\u6570\u91cf\uff0c\u9ed8\u8ba4\u4f7f\u7528\u5168\u90e8GPU\u3002 --trust-remote-code \\ --host 0.0.0.0 \\ --port 8000 \\ --mem-fraction-static 0.9 # Gpu\u5360\u7528\u7387\uff0c\u8fc7\u9ad8\u53ef\u80fd\u5bfc\u81f4\u5176\u4ed6\u8fdb\u7a0b\u89e6\u53d1OOM\u3002\u53d6\u503c\u8303\u56f4:0~1","title":"\u81ea\u5b9a\u4e49\u6a21\u578b\u90e8\u7f72\u53c2\u6570"},{"location":"index-ecs-one/#api","text":"\u590d\u5236Api\u8c03\u7528\u793a\u4f8b\uff0c\u5728\u8d44\u6e90\u6807\u7b7e\u9875\u7684ECS\u5b9e\u4f8b\u4e2d\u7c98\u8d34Api\u8c03\u7528\u793a\u4f8b\u5373\u53ef\u3002\u4e5f\u53ef\u5728\u540c\u4e00VPC\u5185\u7684\u5176\u4ed6ECS\u4e2d\u8bbf\u95ee\u3002","title":"\u5185\u7f51API\u8bbf\u95ee"},{"location":"index-ecs-one/#api_1","text":"\u590d\u5236Api\u8c03\u7528\u793a\u4f8b\uff0c\u5728\u672c\u5730\u7ec8\u7aef\u4e2d\u7c98\u8d34Api\u8c03\u7528\u793a\u4f8b\u5373\u53ef\u3002","title":"\u516c\u7f51API\u8bbf\u95ee"},{"location":"index-ecs-one/#chatbox-vllm-api","text":"\u8bbf\u95ee Chatbox \u4e0b\u8f7d\u5730\u5740 \u4e0b\u8f7d\u5e76\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u672c\u65b9\u6848\u4ee5 macOS M3 \u4e3a\u4f8b\u3002 \u8fd0\u884c\u5e76\u914d\u7f6e vLLM API \uff0c\u5355\u51fb\u8bbe\u7f6e\u3002 \u5728\u5f39\u51fa\u7684\u770b\u677f\u4e2d\u6309\u7167\u5982\u4e0b\u8868\u683c\u8fdb\u884c\u914d\u7f6e\u3002 \u9879\u76ee \u8bf4\u660e \u793a\u4f8b\u503c \u6a21\u578b\u63d0\u4f9b\u65b9 \u4e0b\u62c9\u9009\u62e9\u6a21\u578b\u63d0\u4f9b\u65b9\u3002 \u6dfb\u52a0\u81ea\u5b9a\u4e49\u63d0\u4f9b\u65b9 \u540d\u79f0 \u586b\u5199\u5b9a\u4e49\u6a21\u578b\u63d0\u4f9b\u65b9\u540d\u79f0\u3002 vLLM API API \u57df\u540d \u586b\u5199\u6a21\u578b\u670d\u52a1\u8c03\u7528\u5730\u5740\u3002 http:// :8000 API \u8def\u5f84 \u586b\u5199 API \u8def\u5f84\u3002 /v1/chat/completions \u7f51\u7edc\u517c\u5bb9\u6027 \u70b9\u51fb\u5f00\u542f\u6539\u5584\u7f51\u7edc\u517c\u5bb9\u6027 \u5f00\u542f API \u5bc6\u94a5 \u586b\u5199\u6a21\u578b\u670d\u52a1\u8c03\u7528 API \u5bc6\u94a5\u3002 \u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\u540e\uff0c\u5728\u670d\u52a1\u5b9e\u4f8b\u9875\u9762\u53ef\u83b7\u53d6Api_Key \u6a21\u578b \u586b\u5199\u8c03\u7528\u7684\u6a21\u578b\u3002 Qwen/QwQ-32B \u4fdd\u5b58\u914d\u7f6e\u3002\u5728\u6587\u672c\u8f93\u5165\u6846\u4e2d\u53ef\u4ee5\u8fdb\u884c\u5bf9\u8bdd\u4ea4\u4e92\u3002\u8f93\u5165\u95ee\u9898\u4f60\u662f\u8c01\uff1f\u6216\u8005\u5176\u4ed6\u6307\u4ee4\u540e\uff0c\u8c03\u7528\u6a21\u578b\u670d\u52a1\u83b7\u5f97\u76f8\u5e94\u7684\u54cd\u5e94\u3002","title":"\u4f7f\u7528 Chatbox \u5ba2\u6237\u7aef\u914d\u7f6e vLLM API \u8fdb\u884c\u5bf9\u8bdd(\u53ef\u9009)"},{"location":"index-ecs-one/#_8","text":"","title":"\u6027\u80fd\u6d4b\u8bd5"},{"location":"index-ecs-one/#_9","text":"\u524d\u63d0\u6761\u4ef6\uff1a 1. \u65e0\u6cd5\u76f4\u63a5\u6d4b\u8bd5\u5e26api-key\u7684\u6a21\u578b\u670d\u52a1\uff1b2. \u9700\u8981\u516c\u7f51\u3002","title":"\u538b\u6d4b\u8fc7\u7a0b(\u4f9b\u53c2\u8003)"},{"location":"index-ecs-one/#_10","text":"\u8fdc\u7a0b\u8fde\u63a5\uff0c\u767b\u5165ECS\u5b9e\u4f8b\u3002 \u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5c06\u6a21\u578b\u670d\u52a1\u505c\u6b62\u3002 ```shell sudo docker stop vllm sudo docker rm vllm \u8bf7\u53c2\u8003\u672c\u6587\u6863\u4e2d\u7684 \u67e5\u8be2\u6a21\u578b\u90e8\u7f72\u53c2\u6570 \u90e8\u5206\uff0c\u83b7\u53d6\u6a21\u578b\u90e8\u7f72\u5b9e\u9645\u6267\u884c\u7684\u811a\u672c\u3002 \u53bb\u6389\u811a\u672c\u4e2d\u7684--api-key\u53c2\u6570\uff0c\u5728ECS\u5b9e\u4f8b\u4e2d\u6267\u884c\u5269\u4f59\u811a\u672c\u3002\u6267\u884cdocker logs vllm\u3002\u82e5\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u6a21\u578b\u670d\u52a1\u91cd\u65b0\u90e8\u7f72\u6210\u529f\u3002","title":"\u91cd\u65b0\u90e8\u7f72\u6a21\u578b\u670d\u52a1"},{"location":"index-ecs-one/#_11","text":"\u4ee5QwQ-32B\u4e3a\u4f8b\uff0c\u6a21\u578b\u670d\u52a1\u90e8\u7f72\u5b8c\u6210\u540e\uff0cssh\u767b\u5f55ECS\u5b9e\u4f8b\u3002\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5373\u53ef\u5f97\u5230\u6a21\u578b\u670d\u52a1\u6027\u80fd\u6d4b\u8bd5\u7ed3\u679c\u3002\u53ef\u6839\u636e\u53c2\u6570\u8bf4\u660e\u81ea\u884c\u4fee\u6539\u3002 ```shell yum install -y git-lfs git lfs install git lfs clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git git lfs clone https://github.com/vllm-project/vllm.git docker exec vllm bash -c \" pip install pandas datasets && python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model /root/llm-model/Qwen/QwQ-32B \\ --served-model-name Qwen/QwQ-32B \\ --sonnet-input-len 1024 \\ # \u6700\u5927\u8f93\u5165\u957f\u5ea6 --sonnet-output-len 4096 \\ # \u6700\u5927\u8f93\u51fa\u957f\u5ea6 --sonnet-prefix-len 50 \\ # \u524d\u7f00\u957f\u5ea6 --num-prompts 400 \\ # \u4ece\u6570\u636e\u96c6\u4e2d\u968f\u673a\u9009\u53d6\u6216\u6309\u987a\u5e8f\u5904\u7406 400 \u4e2a prompt \u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002 --request-rate 20 \\ # \u6a21\u62df\u6bcf\u79d2 20 \u4e2a\u5e76\u53d1\u8bf7\u6c42\u7684\u538b\u529b\u6d4b\u8bd5\uff0c\u6301\u7eed20\u79d2\uff0c\u5171400\u4e2a\u8bf7\u6c42\u3002\u8bc4\u4f30\u6a21\u578b\u670d\u52a1\u5728\u8d1f\u8f7d\u4e0b\u7684\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u3002 --port 8000 \\ --trust-remote-code \\ --dataset-name sharegpt \\ --save-result \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \" ```","title":"\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5"},{"location":"index-ecs-one/#_12","text":"","title":"\u6027\u80fd\u6d4b\u8bd5\u7ed3\u679c"},{"location":"index-ecs-one/#qwen3-235b-a22b","text":"\u672c\u670d\u52a1\u65b9\u6848\u4e0b\uff0c\u9488\u5bf9Qwen3-235B-A22B\u5728GU8TF\u5b9e\u4f8b\u89c4\u683c\u4e0b\uff0c\u5206\u522b\u6d4b\u8bd5QPS\u4e3a20\u60c5\u51b5\u4e0b\u6a21\u578b\u670d\u52a1\u7684\u63a8\u7406\u54cd\u5e94\u6027\u80fd\uff0c\u538b\u6d4b\u6301\u7eed\u65f6\u95f4\u5747\u4e3a1\u5206\u949f\u3002","title":"Qwen3-235B-A22B\u538b\u6d4b\u7ed3\u679c"},{"location":"index-ecs-one/#gu8tf","text":"","title":"GU8TF\u89c4\u683c"},{"location":"index-ecs-one/#qps2011200","text":"","title":"QPS\u4e3a20\uff0c1\u5206\u949f1200\u4e2a\u95ee\u7b54\u8bf7\u6c42"},{"location":"index-ecs-one/#qps5013000","text":"","title":"QPS\u4e3a50\uff0c1\u5206\u949f3000\u4e2a\u95ee\u7b54\u8bf7\u6c42"},{"location":"index-ecs-one/#qwen3-32b","text":"\u672c\u670d\u52a1\u65b9\u6848\u4e0b\uff0c\u9488\u5bf9Qwen3-32B\u5728ecs.gn7i-8x.16xlarge\uff088*A10\uff09\u5b9e\u4f8b\u89c4\u683c\u4e0b\uff0c\u5206\u522b\u6d4b\u8bd5QPS\u4e3a20\u60c5\u51b5\u4e0b\u6a21\u578b\u670d\u52a1\u7684\u63a8\u7406\u54cd\u5e94\u6027\u80fd\uff0c\u538b\u6d4b\u6301\u7eed\u65f6\u95f4\u5747\u4e3a1\u5206\u949f\u3002","title":"Qwen3-32B\u538b\u6d4b\u7ed3\u679c"},{"location":"index-ecs-one/#8a10","text":"","title":"8*A10\u89c4\u683c"},{"location":"index-ecs-one/#qps2011200_1","text":"","title":"QPS\u4e3a20\uff0c1\u5206\u949f1200\u4e2a\u95ee\u7b54\u8bf7\u6c42"},{"location":"index-ecs-one/#qps5013000_1","text":"","title":"QPS\u4e3a50\uff0c1\u5206\u949f3000\u4e2a\u95ee\u7b54\u8bf7\u6c42"},{"location":"index-ecs-one/#qwq-32b","text":"\u672c\u670d\u52a1\u65b9\u6848\u4e0b\uff0c\u9488\u5bf9QwQ-32B\u57284 A10\u548c8 A10\u5b9e\u4f8b\u89c4\u683c\u4e0b\uff0c\u5206\u522b\u6d4b\u8bd5QPS\u4e3a10\u300120\u300150\u60c5\u51b5\u4e0b\u6a21\u578b\u670d\u52a1\u7684\u63a8\u7406\u54cd\u5e94\u6027\u80fd\uff0c\u538b\u6d4b\u6301\u7eed\u65f6\u95f4\u5747\u4e3a20s\u3002","title":"QwQ-32B\u538b\u6d4b\u7ed3\u679c"},{"location":"index-ecs-one/#8a10_1","text":"","title":"8*A10\u89c4\u683c"},{"location":"index-ecs-one/#qps10","text":"","title":"QPS\u4e3a10"},{"location":"index-ecs-one/#qps20","text":"","title":"QPS\u4e3a20"},{"location":"index-ecs-one/#qps50","text":"","title":"QPS\u4e3a50"},{"location":"index-ecs-one/#4a10","text":"","title":"4*A10\u89c4\u683c"},{"location":"index-ecs-one/#qps10_1","text":"","title":"QPS\u4e3a10"},{"location":"index-ecs-one/#qps20_1","text":"","title":"QPS\u4e3a20"},{"location":"index-ecs-one/#qps50_1","text":"","title":"QPS\u4e3a50"},{"location":"index-ecs-two-en/","text":"DeepSeek-R1 and V3 model deployment documents based on dual ECS instances Deployment Instructions This service provides a one-click deployment solution for large models based on ECS mirroring + Vllm + Ray. DeepSeek-R1 full-blood version and DeepSeek-V3 models can be deployed in 30 minutes through dual ECS instances. This service uses ECS mirror packaging standard environments and realizes one-click deployment of cloud resources and large models through Ros templates. Developers do not need to care about the standard environment for model deployment and operation and the underlying cloud resource orchestration. They only need to add a few parameters to enjoy the inference experience of DeepSeek-R1 full-blooded version and DeepSeek-V3. Under the scheme provided by this service, the average token per request is calculated by 10kb, and two GU8TF-specification ECS instances can be used. The number of concurrent requests per second (QPS) that can be supported by the DeepSeek-R1 full-blood version theory is about 75, and the DeepSeek-V3 is about 67. The models supported by this service are as follows: * deepseek-ai/DeepSeek-R1 * deepseek-ai/DeepSeek-V3 Overall architecture User/Client Layer Access Method\uff1aVPC Internal Access, Public Network Access and API Key Access Response Method: Streaming Return Service Layer Large Model Service: Provide Standard API Interface for Invocation VLLM Service: Tensor Parallel Size 8 and Pipeline Parallel Size 2 Infrastructure Layer Network and Resources: Ray Cluster, Dual ECS GPU Instances, ECS Image and VPC Network ECS Image: VLLM Standard Operation Environment and Large Model Files Billing instructions The cost of this service on Alibaba Cloud mainly involves: * Specifications of the selected GPU cloud server * Number of nodes * Disk capacity * Public network bandwidth Billing method: pay by volume (hours) or annual monthly payment Estimated costs are visible in real time when creating an instance. Required permissions for RAM account To deploy service instances, some Alibaba Cloud resources need to be accessed and created.Therefore, your account needs permissions to include the following resources. Permission Policy Name Notes AliyunECSFullAccess Permissions to manage cloud server services (ECS) AliyunVPCFullAccess Permissions to manage proprietary networks (VPCs) AliyunROSFullAccess Permissions to manage resource orchestration services (ROS) AliyunComputeNestUserFullAccess Manage user-side permissions for ComputeNest Deployment Process Click [Deployment Link] .Select the dual-machine version and confirm that the GU8TF example specification has been applied for.Fill in the parameters according to the prompts on the interface, and you can choose whether to turn on the public network according to your needs. You can see the corresponding inquiry details. After confirming the parameters, click Next: Confirm the order**. Click Next: Confirm the order ** and you can see the price preview. Then you can click Deploy now** and wait for the deployment to complete.(If the RAM permission is insufficient, you need to add RAM permissions to the sub-account) After the deployment is completed, you can start using the service.Click on the service instance name to enter the service instance details, and use the Api to call the sample to access the service.If it is an intranet access, you must ensure that the ECS instance is under the same VPC. After ssh accesses the ECS instance, execute docker logs vllm to query the model service deployment log.When you see the result shown in the figure below, it means that the model service is deployed successfully.The path where the model is located is /root/llm_model/. Instructions for use Query model deployment parameters Copy the service instance name.Go to [Resource Orchestration Console] (https://ros.console.aliyun.com/cn-hangzhou/stacks) to view the corresponding resource stack. Enter the resource stack corresponding to the service instance, you can see all the resources opened and view all the scripts executed during the model deployment process. Customize model deployment parameters If you have the requirement for custom model deployment parameters, you can modify it after deploying the service instance as follows.Currently, two deployment methods are provided: vllm and sglang. Remote connection, log in to the master node and the worker node respectively (the two instances are named llm-xxxx-master and llm-xxxx-worker respectively). Execute the following command to stop the model services in both nodes. ```shell sudo docker stop vllm sudo docker rm vllm Please refer to the Query Model Deployment Parameters section in this document to obtain the scripts that the model deployment is actually executed in the master node and worker node. The following are the reference scripts for vllm and sglang deployments. You can refer to the parameters to annotate custom model deployment parameters and modify the actual executed script.After modification, execute the master node script first, and then execute the worker node script after success. vllm deployment master node reference script ```shell docker run -t -d \\ --entrypoint /bin/bash \\ --name=vllm \\ --ipc=host \\ --cap-add=SYS_PTRACE \\ --network=host \\ --gpus all \\ --privileged \\ --ulimit memlock=-1 \\ --ulimit stack=67108864 \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install --upgrade vllm==0.8.2 && # Customizable version, such as pip install vllm==0.7.1. Must be consistent with the worker node. export NCCL_IB_DISABLE=0 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_DEBUG=INFO && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_NET_GDR_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_P2P_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_IB_GID_INDEX=1 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export GLOO_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify export NCCL_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify ray start --head --dashboard-host 0.0.0.0 --port=6379 && tail -f /dev/null\" vllm deployment worker node reference script ```shell docker run -t -d \\ --entrypoint /bin/bash \\ --name=vllm \\ --ipc=host \\ --cap-add=SYS_PTRACE \\ --network=host \\ --gpus all \\ --privileged \\ --ulimit memlock=-1 \\ --ulimit stack=67108864 \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install --upgrade vllm==0.8.2 && # Customizable versions, such as pip install vllm==0.7.1. Must be consistent with the master node. export NCCL_IB_DISABLE=0 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_DEBUG=INFO && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_NET_GDR_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_P2P_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_IB_GID_INDEX=1 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export GLOO_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify export NCCL_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify ray start --address='${HEAD_NODE_ADDRESS}:6379' && # Fill in the intranet IP address of the master node. vllm serve /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --gpu-memory-utilization 0.98 \\ # Gpu occupancy rate, too high may cause other processes to trigger OOM.Value range: 0~1 --max-model-len ${MaxModelLen} \\ # The maximum length of the model, the value range is related to the model itself. --enable-chunked-prefill \\ --host=0.0.0.0 \\ --port 8000 \\ --trust-remote-code \\ --api-key \"${VLLM_API_KEY}\" \\ --tensor-parallel-size $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}') \\ # The number of GPUs used by a single node, and the default is to use all GPUs of a single ECS instance. --pipeline-parallel-size 2\" # The number of streamlines parallel, recommended to set it to the total number of nodes. sglang deployment master node reference script ```shell docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name llm-server \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install sglang==0.4.3 && # Customizable version, must be consistent with the worker node export NCCL_IB_DISABLE=0 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_DEBUG=INFO && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_NET_GDR_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_P2P_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_IB_GID_INDEX=1 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export GLOO_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify export NCCL_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify python3 -m sglang.launch_server \\ --model-path /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --tp 16 \\ # Currently sglang does not support streamline parallelism, and all GPUs in two ECS instances are used by default. --dist-init-addr ${HEAD_NODE_ADDRESS}:20000 # Fill in the intranet IP address of the master node. --nnodes 2 # The node finally uses two ECS instances by default. --node-rank 0 # Node sequence number, default is 0. --trust-remote-code \\ --host 0.0.0.0 \\ --port 8000 \\ --mem-fraction-static 0.9 # Gpu occupancy rate, too high may cause other processes to trigger OOM.Value range: 0~1 sglang deployment worker node reference script ```shell docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name llm-server \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install sglang==0.4.3 && # Customizable version, must be consistent with the master node export NCCL_IB_DISABLE=0 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_DEBUG=INFO && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_NET_GDR_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_P2P_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_IB_GID_INDEX=1 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export GLOO_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify export NCCL_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify python3 -m sglang.launch_server \\ --model-path /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --tp 16 \\ # Currently sglang does not support streamline parallelism, and all GPUs in two ECS instances are used by default. --dist-init-addr ${HEAD_NODE_ADDRESS}:20000 # Fill in the intranet IP address of the master node. --nnodes 2 # The node finally uses two ECS instances by default. --node-rank 1 # Node number, default is 1. --trust-remote-code \\ --host 0.0.0.0 \\ --port 8000 \\ --mem-fraction-static 0.9 # Gpu occupancy rate, too high may cause other processes to trigger OOM.Value range: 0~1 Intranet API access Copy the Api call example and paste the Api call example in the ECS instance of the resource tab.It can also be accessed in other ECS within the same VPC. Public Internet API access Copy the Api call example and paste the Api call example in the local terminal. Configure the vLLM API using the Chatbox client for conversation (optional) Visit Chatbox [Download Address] (https://chatboxai.app/zh#download) to download and install the client. This solution takes macOS M3 as an example. Run and configure the vLLM API and click Settings. Configure in the pop-up kanban according to the following table. Project Description Example Value Model Provider Pull down to select the model provider. Add a custom provider Name Fill in the name of the model provider. vLLM API API Domain Name Fill in the model service call address. http:// :8000 API Path Fill in the API Path. /v1/chat/completes Network Compatibility Click to enable to improve network compatibility Open API Key Fill in the Model Service Call API Key. After deploying the service instance, you can get Api_Key on the service instance page Model Fill in the called model. deepseek-ai/DeepSeek-R1 Save the configuration.The conversation can be performed in the text input box.Enter the question Who are you?Or after other instructions, the model service is called to obtain the corresponding response. Performance Testing Pressure test process (for reference) **Prerequisites: ** 1. It is impossible to directly test model services with API-key; 2. Public network is required. Redeploy the model service Remote connection and log in to the worker node (named llm-xxxx-worker). Execute the following command to stop the model service. ```shell sudo docker stop vllm sudo docker rm vllm Please refer to the Query Model Deployment Parameters section in this document to obtain the scripts that the worker node model deployment actually executes. Remove the --api-key parameter in the script and execute the remaining scripts in the ECS instance.Execute docker logs vllm.If the result is shown in the figure below, the model service is redeployed successfully. Perform performance testing Taking Deepseek-R1 as an example, after the model service is deployed, ssh logs into the ECS instance.Execute the following command to get the model service performance test results.You can modify it yourself according to the parameter description. ```shell yum install -y git-lfs git lfs install git lfs clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git git lfs clone https://github.com/vllm-project/vllm.git docker exec vllm bash -c \" pip install pandas datasets && python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model /root/llm-model/deepseek-ai/DeepSeek-R1 \\ --served-model-name deepseek-ai/DeepSeek-R1 \\ --sonnet-input-len 1024 \\ # Maximum input length --sonnet-output-len 4096 \\ # Maximum output length --sonnet-prefix-len 50 \\ # Prefix length --num-prompts 400 \\ # Randomly select or process 400 prompts from the dataset for performance testing. --request-rate 20 \\ # Simulate a stress test of 20 concurrent requests per second, lasting 20 seconds, with a total of 400 requests.Evaluate the throughput and latency of the model service under load. --port 8000 \\ --trust-remote-code \\ --dataset-name sharegpt \\ --save-result \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \" ``` Model service performance test Under this service plan, the inference response performance of the model service with a QPS of 75 and 60 was tested respectively for Deepseek-R1 and V3, and the pressure measurement duration was 20s. Deepseek-R1 QPS is 75 Deepseek-V3 QPS is 60","title":"DeepSeek-R1 and V3 model deployment documents based on dual ECS instances"},{"location":"index-ecs-two-en/#deepseek-r1-and-v3-model-deployment-documents-based-on-dual-ecs-instances","text":"","title":"DeepSeek-R1 and V3 model deployment documents based on dual ECS instances"},{"location":"index-ecs-two-en/#deployment-instructions","text":"This service provides a one-click deployment solution for large models based on ECS mirroring + Vllm + Ray. DeepSeek-R1 full-blood version and DeepSeek-V3 models can be deployed in 30 minutes through dual ECS instances. This service uses ECS mirror packaging standard environments and realizes one-click deployment of cloud resources and large models through Ros templates. Developers do not need to care about the standard environment for model deployment and operation and the underlying cloud resource orchestration. They only need to add a few parameters to enjoy the inference experience of DeepSeek-R1 full-blooded version and DeepSeek-V3. Under the scheme provided by this service, the average token per request is calculated by 10kb, and two GU8TF-specification ECS instances can be used. The number of concurrent requests per second (QPS) that can be supported by the DeepSeek-R1 full-blood version theory is about 75, and the DeepSeek-V3 is about 67. The models supported by this service are as follows: * deepseek-ai/DeepSeek-R1 * deepseek-ai/DeepSeek-V3","title":"Deployment Instructions"},{"location":"index-ecs-two-en/#overall-architecture","text":"","title":"Overall architecture"},{"location":"index-ecs-two-en/#userclient-layer","text":"Access Method\uff1aVPC Internal Access, Public Network Access and API Key Access Response Method: Streaming Return","title":"User/Client Layer"},{"location":"index-ecs-two-en/#service-layer","text":"Large Model Service: Provide Standard API Interface for Invocation VLLM Service: Tensor Parallel Size 8 and Pipeline Parallel Size 2","title":"Service Layer"},{"location":"index-ecs-two-en/#infrastructure-layer","text":"Network and Resources: Ray Cluster, Dual ECS GPU Instances, ECS Image and VPC Network ECS Image: VLLM Standard Operation Environment and Large Model Files","title":"Infrastructure Layer"},{"location":"index-ecs-two-en/#billing-instructions","text":"The cost of this service on Alibaba Cloud mainly involves: * Specifications of the selected GPU cloud server * Number of nodes * Disk capacity * Public network bandwidth Billing method: pay by volume (hours) or annual monthly payment Estimated costs are visible in real time when creating an instance.","title":"Billing instructions"},{"location":"index-ecs-two-en/#required-permissions-for-ram-account","text":"To deploy service instances, some Alibaba Cloud resources need to be accessed and created.Therefore, your account needs permissions to include the following resources. Permission Policy Name Notes AliyunECSFullAccess Permissions to manage cloud server services (ECS) AliyunVPCFullAccess Permissions to manage proprietary networks (VPCs) AliyunROSFullAccess Permissions to manage resource orchestration services (ROS) AliyunComputeNestUserFullAccess Manage user-side permissions for ComputeNest","title":"Required permissions for RAM account"},{"location":"index-ecs-two-en/#deployment-process","text":"Click [Deployment Link] .Select the dual-machine version and confirm that the GU8TF example specification has been applied for.Fill in the parameters according to the prompts on the interface, and you can choose whether to turn on the public network according to your needs. You can see the corresponding inquiry details. After confirming the parameters, click Next: Confirm the order**. Click Next: Confirm the order ** and you can see the price preview. Then you can click Deploy now** and wait for the deployment to complete.(If the RAM permission is insufficient, you need to add RAM permissions to the sub-account) After the deployment is completed, you can start using the service.Click on the service instance name to enter the service instance details, and use the Api to call the sample to access the service.If it is an intranet access, you must ensure that the ECS instance is under the same VPC. After ssh accesses the ECS instance, execute docker logs vllm to query the model service deployment log.When you see the result shown in the figure below, it means that the model service is deployed successfully.The path where the model is located is /root/llm_model/.","title":"Deployment Process"},{"location":"index-ecs-two-en/#instructions-for-use","text":"","title":"Instructions for use"},{"location":"index-ecs-two-en/#query-model-deployment-parameters","text":"Copy the service instance name.Go to [Resource Orchestration Console] (https://ros.console.aliyun.com/cn-hangzhou/stacks) to view the corresponding resource stack. Enter the resource stack corresponding to the service instance, you can see all the resources opened and view all the scripts executed during the model deployment process.","title":"Query model deployment parameters"},{"location":"index-ecs-two-en/#customize-model-deployment-parameters","text":"If you have the requirement for custom model deployment parameters, you can modify it after deploying the service instance as follows.Currently, two deployment methods are provided: vllm and sglang. Remote connection, log in to the master node and the worker node respectively (the two instances are named llm-xxxx-master and llm-xxxx-worker respectively). Execute the following command to stop the model services in both nodes. ```shell sudo docker stop vllm sudo docker rm vllm Please refer to the Query Model Deployment Parameters section in this document to obtain the scripts that the model deployment is actually executed in the master node and worker node. The following are the reference scripts for vllm and sglang deployments. You can refer to the parameters to annotate custom model deployment parameters and modify the actual executed script.After modification, execute the master node script first, and then execute the worker node script after success. vllm deployment master node reference script ```shell docker run -t -d \\ --entrypoint /bin/bash \\ --name=vllm \\ --ipc=host \\ --cap-add=SYS_PTRACE \\ --network=host \\ --gpus all \\ --privileged \\ --ulimit memlock=-1 \\ --ulimit stack=67108864 \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install --upgrade vllm==0.8.2 && # Customizable version, such as pip install vllm==0.7.1. Must be consistent with the worker node. export NCCL_IB_DISABLE=0 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_DEBUG=INFO && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_NET_GDR_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_P2P_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_IB_GID_INDEX=1 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export GLOO_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify export NCCL_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify ray start --head --dashboard-host 0.0.0.0 --port=6379 && tail -f /dev/null\" vllm deployment worker node reference script ```shell docker run -t -d \\ --entrypoint /bin/bash \\ --name=vllm \\ --ipc=host \\ --cap-add=SYS_PTRACE \\ --network=host \\ --gpus all \\ --privileged \\ --ulimit memlock=-1 \\ --ulimit stack=67108864 \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install --upgrade vllm==0.8.2 && # Customizable versions, such as pip install vllm==0.7.1. Must be consistent with the master node. export NCCL_IB_DISABLE=0 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_DEBUG=INFO && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_NET_GDR_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_P2P_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_IB_GID_INDEX=1 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export GLOO_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify export NCCL_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify ray start --address='${HEAD_NODE_ADDRESS}:6379' && # Fill in the intranet IP address of the master node. vllm serve /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --gpu-memory-utilization 0.98 \\ # Gpu occupancy rate, too high may cause other processes to trigger OOM.Value range: 0~1 --max-model-len ${MaxModelLen} \\ # The maximum length of the model, the value range is related to the model itself. --enable-chunked-prefill \\ --host=0.0.0.0 \\ --port 8000 \\ --trust-remote-code \\ --api-key \"${VLLM_API_KEY}\" \\ --tensor-parallel-size $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}') \\ # The number of GPUs used by a single node, and the default is to use all GPUs of a single ECS instance. --pipeline-parallel-size 2\" # The number of streamlines parallel, recommended to set it to the total number of nodes. sglang deployment master node reference script ```shell docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name llm-server \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install sglang==0.4.3 && # Customizable version, must be consistent with the worker node export NCCL_IB_DISABLE=0 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_DEBUG=INFO && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_NET_GDR_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_P2P_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_IB_GID_INDEX=1 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export GLOO_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify export NCCL_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify python3 -m sglang.launch_server \\ --model-path /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --tp 16 \\ # Currently sglang does not support streamline parallelism, and all GPUs in two ECS instances are used by default. --dist-init-addr ${HEAD_NODE_ADDRESS}:20000 # Fill in the intranet IP address of the master node. --nnodes 2 # The node finally uses two ECS instances by default. --node-rank 0 # Node sequence number, default is 0. --trust-remote-code \\ --host 0.0.0.0 \\ --port 8000 \\ --mem-fraction-static 0.9 # Gpu occupancy rate, too high may cause other processes to trigger OOM.Value range: 0~1 sglang deployment worker node reference script ```shell docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name llm-server \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install sglang==0.4.3 && # Customizable version, must be consistent with the master node export NCCL_IB_DISABLE=0 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_DEBUG=INFO && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_NET_GDR_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_P2P_LEVEL=5 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export NCCL_IB_GID_INDEX=1 && # The environment variables required for high-speed network communication using elastic RDMA are not recommended to change them export GLOO_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify export NCCL_SOCKET_IFNAME=eth0 && # Use vpc for network communication environment variables, do not delete and modify python3 -m sglang.launch_server \\ --model-path /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --tp 16 \\ # Currently sglang does not support streamline parallelism, and all GPUs in two ECS instances are used by default. --dist-init-addr ${HEAD_NODE_ADDRESS}:20000 # Fill in the intranet IP address of the master node. --nnodes 2 # The node finally uses two ECS instances by default. --node-rank 1 # Node number, default is 1. --trust-remote-code \\ --host 0.0.0.0 \\ --port 8000 \\ --mem-fraction-static 0.9 # Gpu occupancy rate, too high may cause other processes to trigger OOM.Value range: 0~1","title":"Customize model deployment parameters"},{"location":"index-ecs-two-en/#intranet-api-access","text":"Copy the Api call example and paste the Api call example in the ECS instance of the resource tab.It can also be accessed in other ECS within the same VPC.","title":"Intranet API access"},{"location":"index-ecs-two-en/#public-internet-api-access","text":"Copy the Api call example and paste the Api call example in the local terminal.","title":"Public Internet API access"},{"location":"index-ecs-two-en/#configure-the-vllm-api-using-the-chatbox-client-for-conversation-optional","text":"Visit Chatbox [Download Address] (https://chatboxai.app/zh#download) to download and install the client. This solution takes macOS M3 as an example. Run and configure the vLLM API and click Settings. Configure in the pop-up kanban according to the following table. Project Description Example Value Model Provider Pull down to select the model provider. Add a custom provider Name Fill in the name of the model provider. vLLM API API Domain Name Fill in the model service call address. http:// :8000 API Path Fill in the API Path. /v1/chat/completes Network Compatibility Click to enable to improve network compatibility Open API Key Fill in the Model Service Call API Key. After deploying the service instance, you can get Api_Key on the service instance page Model Fill in the called model. deepseek-ai/DeepSeek-R1 Save the configuration.The conversation can be performed in the text input box.Enter the question Who are you?Or after other instructions, the model service is called to obtain the corresponding response.","title":"Configure the vLLM API using the Chatbox client for conversation (optional)"},{"location":"index-ecs-two-en/#performance-testing","text":"","title":"Performance Testing"},{"location":"index-ecs-two-en/#pressure-test-process-for-reference","text":"**Prerequisites: ** 1. It is impossible to directly test model services with API-key; 2. Public network is required.","title":"Pressure test process (for reference)"},{"location":"index-ecs-two-en/#redeploy-the-model-service","text":"Remote connection and log in to the worker node (named llm-xxxx-worker). Execute the following command to stop the model service. ```shell sudo docker stop vllm sudo docker rm vllm Please refer to the Query Model Deployment Parameters section in this document to obtain the scripts that the worker node model deployment actually executes. Remove the --api-key parameter in the script and execute the remaining scripts in the ECS instance.Execute docker logs vllm.If the result is shown in the figure below, the model service is redeployed successfully.","title":"Redeploy the model service"},{"location":"index-ecs-two-en/#perform-performance-testing","text":"Taking Deepseek-R1 as an example, after the model service is deployed, ssh logs into the ECS instance.Execute the following command to get the model service performance test results.You can modify it yourself according to the parameter description. ```shell yum install -y git-lfs git lfs install git lfs clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git git lfs clone https://github.com/vllm-project/vllm.git docker exec vllm bash -c \" pip install pandas datasets && python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model /root/llm-model/deepseek-ai/DeepSeek-R1 \\ --served-model-name deepseek-ai/DeepSeek-R1 \\ --sonnet-input-len 1024 \\ # Maximum input length --sonnet-output-len 4096 \\ # Maximum output length --sonnet-prefix-len 50 \\ # Prefix length --num-prompts 400 \\ # Randomly select or process 400 prompts from the dataset for performance testing. --request-rate 20 \\ # Simulate a stress test of 20 concurrent requests per second, lasting 20 seconds, with a total of 400 requests.Evaluate the throughput and latency of the model service under load. --port 8000 \\ --trust-remote-code \\ --dataset-name sharegpt \\ --save-result \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \" ```","title":"Perform performance testing"},{"location":"index-ecs-two-en/#model-service-performance-test","text":"Under this service plan, the inference response performance of the model service with a QPS of 75 and 60 was tested respectively for Deepseek-R1 and V3, and the pressure measurement duration was 20s.","title":"Model service performance test"},{"location":"index-ecs-two-en/#deepseek-r1","text":"","title":"Deepseek-R1"},{"location":"index-ecs-two-en/#qps-is-75","text":"","title":"QPS is 75"},{"location":"index-ecs-two-en/#deepseek-v3","text":"","title":"Deepseek-V3"},{"location":"index-ecs-two-en/#qps-is-60","text":"","title":"QPS is 60"},{"location":"index-ecs-two/","text":"\u57fa\u4e8e\u53ccECS\u5b9e\u4f8b\u7684DeepSeek-R1\u548cV3\u6a21\u578b\u90e8\u7f72\u6587\u6863 \u90e8\u7f72\u8bf4\u660e \u672c\u670d\u52a1\u63d0\u4f9b\u4e86\u57fa\u4e8eECS\u955c\u50cf+Vllm+Ray\u7684\u5927\u6a21\u578b\u4e00\u952e\u90e8\u7f72\u65b9\u6848\uff0c30\u5206\u949f\u5373\u53ef\u901a\u8fc7\u53ccECS\u5b9e\u4f8b\u90e8\u7f72\u4f7f\u7528DeepSeek-R1\u6ee1\u8840\u7248\u548cDeepSeek-V3\u6a21\u578b\u3002 \u672c\u670d\u52a1\u901a\u8fc7ECS\u955c\u50cf\u6253\u5305\u6807\u51c6\u73af\u5883\uff0c\u901a\u8fc7Ros\u6a21\u7248\u5b9e\u73b0\u4e91\u8d44\u6e90\u4e0e\u5927\u6a21\u578b\u7684\u4e00\u952e\u90e8\u7f72\uff0c\u5f00\u53d1\u8005\u65e0\u9700\u5173\u5fc3\u6a21\u578b\u90e8\u7f72\u8fd0\u884c\u7684\u6807\u51c6\u73af\u5883\u4e0e\u5e95\u5c42\u4e91\u8d44\u6e90\u7f16\u6392\uff0c\u4ec5\u9700\u6dfb\u52a0\u51e0\u4e2a\u53c2\u6570\u5373\u53ef\u4eab\u53d7DeepSeek-R1\u6ee1\u8840\u7248\u548cDeepSeek-V3\u7684\u63a8\u7406\u4f53\u9a8c\u3002 \u672c\u670d\u52a1\u63d0\u4f9b\u7684\u65b9\u6848\u4e0b\uff0c\u4ee5\u5e73\u5747\u6bcf\u6b21\u8bf7\u6c42\u7684token\u4e3a10kb\u8ba1\u7b97\uff0c\u91c7\u7528\u4e24\u53f0GU8TF\u89c4\u683c\u7684ECS\u5b9e\u4f8b\uff0cDeepSeek-R1\u6ee1\u8840\u7248\u7406\u8bba\u53ef\u652f\u6301\u7684\u6bcf\u79d2\u5e76\u53d1\u8bf7\u6c42\u6570(QPS)\u7ea6\u4e3a75\uff0cDeepSeek-V3\u7ea6\u4e3a67\u3002 \u672c\u670d\u52a1\u652f\u6301\u7684\u6a21\u578b\u5982\u4e0b\uff1a * deepseek-ai/DeepSeek-R1 * deepseek-ai/DeepSeek-V3 \u6574\u4f53\u67b6\u6784 \u8ba1\u8d39\u8bf4\u660e \u672c\u670d\u52a1\u5728\u963f\u91cc\u4e91\u4e0a\u7684\u8d39\u7528\u4e3b\u8981\u6d89\u53ca\uff1a * \u6240\u9009GPU\u4e91\u670d\u52a1\u5668\u7684\u89c4\u683c * \u8282\u70b9\u6570\u91cf * \u78c1\u76d8\u5bb9\u91cf * \u516c\u7f51\u5e26\u5bbd \u8ba1\u8d39\u65b9\u5f0f\uff1a\u6309\u91cf\u4ed8\u8d39\uff08\u5c0f\u65f6\uff09\u6216\u5305\u5e74\u5305\u6708 \u9884\u4f30\u8d39\u7528\u5728\u521b\u5efa\u5b9e\u4f8b\u65f6\u53ef\u5b9e\u65f6\u770b\u5230\u3002 RAM\u8d26\u53f7\u6240\u9700\u6743\u9650 \u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\uff0c\u9700\u8981\u5bf9\u90e8\u5206\u963f\u91cc\u4e91\u8d44\u6e90\u8fdb\u884c\u8bbf\u95ee\u548c\u521b\u5efa\u64cd\u4f5c\u3002\u56e0\u6b64\u60a8\u7684\u8d26\u53f7\u9700\u8981\u5305\u542b\u5982\u4e0b\u8d44\u6e90\u7684\u6743\u9650\u3002 \u6743\u9650\u7b56\u7565\u540d\u79f0 \u5907\u6ce8 AliyunECSFullAccess \u7ba1\u7406\u4e91\u670d\u52a1\u5668\u670d\u52a1\uff08ECS\uff09\u7684\u6743\u9650 AliyunVPCFullAccess \u7ba1\u7406\u4e13\u6709\u7f51\u7edc\uff08VPC\uff09\u7684\u6743\u9650 AliyunROSFullAccess \u7ba1\u7406\u8d44\u6e90\u7f16\u6392\u670d\u52a1\uff08ROS\uff09\u7684\u6743\u9650 AliyunComputeNestUserFullAccess \u7ba1\u7406\u8ba1\u7b97\u5de2\u670d\u52a1\uff08ComputeNest\uff09\u7684\u7528\u6237\u4fa7\u6743\u9650 \u90e8\u7f72\u6d41\u7a0b \u5355\u51fb \u90e8\u7f72\u94fe\u63a5 \u3002\u9009\u62e9\u53cc\u673a\u7248\uff0c\u5e76\u786e\u8ba4\u5df2\u7533\u8bf7GU8TF\u5b9e\u4f8b\u89c4\u683c\u3002\u6839\u636e\u754c\u9762\u63d0\u793a\u586b\u5199\u53c2\u6570\uff0c\u53ef\u6839\u636e\u9700\u6c42\u9009\u62e9\u662f\u5426\u5f00\u542f\u516c\u7f51\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u8be2\u4ef7\u660e\u7ec6\uff0c\u786e\u8ba4\u53c2\u6570\u540e\u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u3002 \u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u540e\u53ef\u4ee5\u770b\u5230\u4ef7\u683c\u9884\u89c8\uff0c\u968f\u540e\u53ef\u70b9\u51fb \u7acb\u5373\u90e8\u7f72 \uff0c\u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u3002(\u63d0\u793aRAM\u6743\u9650\u4e0d\u8db3\u65f6\u9700\u8981\u4e3a\u5b50\u8d26\u53f7\u6dfb\u52a0RAM\u6743\u9650) \u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u540e\uff0c\u5c31\u53ef\u4ee5\u5f00\u59cb\u4f7f\u7528\u670d\u52a1\u4e86\u3002\u70b9\u51fb\u670d\u52a1\u5b9e\u4f8b\u540d\u79f0\uff0c\u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\uff0c\u4f7f\u7528Api\u8c03\u7528\u793a\u4f8b\u5373\u53ef\u8bbf\u95ee\u670d\u52a1\u3002\u5982\u679c\u662f\u5185\u7f51\u8bbf\u95ee\uff0c\u9700\u4fdd\u8bc1ECS\u5b9e\u4f8b\u5728\u540c\u4e00\u4e2aVPC\u4e0b\u3002 ssh\u8bbf\u95eeECS\u5b9e\u4f8b\u540e\uff0c\u6267\u884c docker logs vllm \u5373\u53ef\u67e5\u8be2\u6a21\u578b\u670d\u52a1\u90e8\u7f72\u65e5\u5fd7\u3002\u5f53\u60a8\u770b\u5230\u4e0b\u56fe\u6240\u793a\u7ed3\u679c\u65f6\uff0c\u8868\u793a\u6a21\u578b\u670d\u52a1\u90e8\u7f72\u6210\u529f\u3002\u6a21\u578b\u6240\u5728\u8def\u5f84\u4e3a/root/llm_model/\u3002 \u4f7f\u7528\u8bf4\u660e \u67e5\u8be2\u6a21\u578b\u90e8\u7f72\u53c2\u6570 \u590d\u5236\u670d\u52a1\u5b9e\u4f8b\u540d\u79f0\u3002\u5230 \u8d44\u6e90\u7f16\u6392\u63a7\u5236\u53f0 \u67e5\u770b\u5bf9\u5e94\u7684\u8d44\u6e90\u6808\u3002 \u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u5bf9\u5e94\u7684\u8d44\u6e90\u6808\uff0c\u53ef\u4ee5\u770b\u5230\u6240\u5f00\u542f\u7684\u5168\u90e8\u8d44\u6e90\uff0c\u5e76\u67e5\u770b\u5230\u6a21\u578b\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u6267\u884c\u7684\u5168\u90e8\u811a\u672c\u3002 \u81ea\u5b9a\u4e49\u6a21\u578b\u90e8\u7f72\u53c2\u6570 \u5982\u679c\u60a8\u6709\u81ea\u5b9a\u4e49\u7684\u6a21\u578b\u90e8\u7f72\u53c2\u6570\u7684\u9700\u6c42\uff0c\u53ef\u4ee5\u5728\u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\u540e\uff0c\u6309\u7167\u5982\u4e0b\u64cd\u4f5c\u6b65\u9aa4\u8fdb\u884c\u4fee\u6539\u3002\u5f53\u524d\u63d0\u4f9bvllm\u548csglang\u4e24\u79cd\u90e8\u7f72\u65b9\u5f0f\u3002 \u8fdc\u7a0b\u8fde\u63a5\uff0c\u5206\u522b\u767b\u5165master\u8282\u70b9\u548cworker\u8282\u70b9\uff08\u4e24\u53f0\u5b9e\u4f8b\u5206\u522b\u547d\u540d\u4e3allm-xxxx-master\u548cllm-xxxx-worker\uff09\u3002 \u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5c06\u4e24\u4e2a\u8282\u70b9\u5185\u7684\u6a21\u578b\u670d\u52a1\u90fd\u505c\u6b62\u3002 ```shell sudo docker stop vllm sudo docker rm vllm \u8bf7\u53c2\u8003\u672c\u6587\u6863\u4e2d\u7684 \u67e5\u8be2\u6a21\u578b\u90e8\u7f72\u53c2\u6570 \u90e8\u5206\uff0c\u83b7\u53d6master\u8282\u70b9\u548cworker\u8282\u70b9\u4e2d\u6a21\u578b\u90e8\u7f72\u5b9e\u9645\u6267\u884c\u7684\u811a\u672c\u3002 \u4e0b\u9762\u5206\u522b\u662fvllm\u4e0esglang\u90e8\u7f72\u7684\u53c2\u8003\u811a\u672c\uff0c\u60a8\u53ef\u53c2\u8003\u53c2\u6570\u6ce8\u91ca\u81ea\u5b9a\u4e49\u6a21\u578b\u90e8\u7f72\u53c2\u6570\uff0c\u4fee\u6539\u5b9e\u9645\u6267\u884c\u7684\u811a\u672c\u3002\u4fee\u6539\u540e\uff0c\u5148\u6267\u884cmaster\u8282\u70b9\u811a\u672c\uff0c\u6210\u529f\u540e\uff0c\u518d\u6267\u884cworker\u8282\u70b9\u811a\u672c\u5373\u53ef\u3002 vllm\u90e8\u7f72master\u8282\u70b9\u53c2\u8003\u811a\u672c ```shell docker run -t -d \\ --entrypoint /bin/bash \\ --name=vllm \\ --ipc=host \\ --cap-add=SYS_PTRACE \\ --network=host \\ --gpus all \\ --privileged \\ --ulimit memlock=-1 \\ --ulimit stack=67108864 \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install --upgrade vllm==0.8.2 && # \u53ef\u81ea\u5b9a\u4e49\u7248\u672c\uff0c\u5982 pip install vllm==0.7.1\u3002\u5fc5\u987b\u4e0eworker\u8282\u70b9\u4fdd\u6301\u4e00\u81f4\u3002 export NCCL_IB_DISABLE=0 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_DEBUG=INFO && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_NET_GDR_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_P2P_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_IB_GID_INDEX=1 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export GLOO_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 export NCCL_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 ray start --head --dashboard-host 0.0.0.0 --port=6379 && tail -f /dev/null\" vllm\u90e8\u7f72worker\u8282\u70b9\u53c2\u8003\u811a\u672c ```shell docker run -t -d \\ --entrypoint /bin/bash \\ --name=vllm \\ --ipc=host \\ --cap-add=SYS_PTRACE \\ --network=host \\ --gpus all \\ --privileged \\ --ulimit memlock=-1 \\ --ulimit stack=67108864 \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install --upgrade vllm==0.8.2 && # \u53ef\u81ea\u5b9a\u4e49\u7248\u672c\uff0c\u5982 pip install vllm==0.7.1\u3002\u5fc5\u987b\u4e0emaster\u8282\u70b9\u4fdd\u6301\u4e00\u81f4\u3002 export NCCL_IB_DISABLE=0 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_DEBUG=INFO && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_NET_GDR_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_P2P_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_IB_GID_INDEX=1 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export GLOO_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 export NCCL_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 ray start --address='${HEAD_NODE_ADDRESS}:6379' && # \u586b\u5199master\u8282\u70b9\u7684\u5185\u7f51IP\u5730\u5740\u3002 vllm serve /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --gpu-memory-utilization 0.98 \\ # Gpu\u5360\u7528\u7387\uff0c\u8fc7\u9ad8\u53ef\u80fd\u5bfc\u81f4\u5176\u4ed6\u8fdb\u7a0b\u89e6\u53d1OOM\u3002\u53d6\u503c\u8303\u56f4:0~1 --max-model-len ${MaxModelLen} \\ # \u6a21\u578b\u6700\u5927\u957f\u5ea6\uff0c\u53d6\u503c\u8303\u56f4\u4e0e\u6a21\u578b\u672c\u8eab\u6709\u5173\u3002 --enable-chunked-prefill \\ --host=0.0.0.0 \\ --port 8000 \\ --trust-remote-code \\ --api-key \"${VLLM_API_KEY}\" \\ --tensor-parallel-size $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}') \\ # \u5355\u8282\u70b9\u4f7f\u7528GPU\u6570\u91cf\uff0c\u9ed8\u8ba4\u4f7f\u7528\u5355\u53f0ECS\u5b9e\u4f8b\u7684\u5168\u90e8GPU\u3002 --pipeline-parallel-size 2\" # \u6d41\u7ebf\u5e76\u884c\u6570\uff0c\u63a8\u8350\u8bbe\u7f6e\u4e3a\u8282\u70b9\u603b\u6570\u3002 sglang\u90e8\u7f72master\u8282\u70b9\u53c2\u8003\u811a\u672c ```shell docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name llm-server \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install sglang==0.4.3 && # \u53ef\u81ea\u5b9a\u4e49\u7248\u672c\uff0c\u5fc5\u987b\u4e0eworker\u8282\u70b9\u4fdd\u6301\u4e00\u81f4 export NCCL_IB_DISABLE=0 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_DEBUG=INFO && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_NET_GDR_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_P2P_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_IB_GID_INDEX=1 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export GLOO_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 export NCCL_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 python3 -m sglang.launch_server \\ --model-path /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --tp 16 \\ # \u5f53\u524dsglang\u4e0d\u652f\u6301\u6d41\u7ebf\u5e76\u884c\uff0c\u9ed8\u8ba4\u4f7f\u7528\u4e24\u53f0ECS\u5b9e\u4f8b\u4e2d\u5168\u90e8GPU\u3002 --dist-init-addr ${HEAD_NODE_ADDRESS}:20000 # \u586b\u5199master\u8282\u70b9\u7684\u5185\u7f51IP\u5730\u5740\u3002 --nnodes 2 # \u8282\u70b9\u603b\u7b97\uff0c\u9ed8\u8ba4\u4f7f\u7528\u4e24\u53f0ECS\u5b9e\u4f8b\u3002 --node-rank 0 # \u8282\u70b9\u5e8f\u53f7\uff0c\u9ed8\u8ba4\u4e3a0\u3002 --trust-remote-code \\ --host 0.0.0.0 \\ --port 8000 \\ --mem-fraction-static 0.9 # Gpu\u5360\u7528\u7387\uff0c\u8fc7\u9ad8\u53ef\u80fd\u5bfc\u81f4\u5176\u4ed6\u8fdb\u7a0b\u89e6\u53d1OOM\u3002\u53d6\u503c\u8303\u56f4:0~1 sglang\u90e8\u7f72worker\u8282\u70b9\u53c2\u8003\u811a\u672c ```shell docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name llm-server \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install sglang==0.4.3 && # \u53ef\u81ea\u5b9a\u4e49\u7248\u672c\uff0c\u5fc5\u987b\u4e0emaster\u8282\u70b9\u4fdd\u6301\u4e00\u81f4 export NCCL_IB_DISABLE=0 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_DEBUG=INFO && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_NET_GDR_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_P2P_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_IB_GID_INDEX=1 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export GLOO_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 export NCCL_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 python3 -m sglang.launch_server \\ --model-path /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --tp 16 \\ # \u5f53\u524dsglang\u4e0d\u652f\u6301\u6d41\u7ebf\u5e76\u884c\uff0c\u9ed8\u8ba4\u4f7f\u7528\u4e24\u53f0ECS\u5b9e\u4f8b\u4e2d\u5168\u90e8GPU\u3002 --dist-init-addr ${HEAD_NODE_ADDRESS}:20000 # \u586b\u5199master\u8282\u70b9\u7684\u5185\u7f51IP\u5730\u5740\u3002 --nnodes 2 # \u8282\u70b9\u603b\u7b97\uff0c\u9ed8\u8ba4\u4f7f\u7528\u4e24\u53f0ECS\u5b9e\u4f8b\u3002 --node-rank 1 # \u8282\u70b9\u5e8f\u53f7\uff0c\u9ed8\u8ba4\u4e3a1\u3002 --trust-remote-code \\ --host 0.0.0.0 \\ --port 8000 \\ --mem-fraction-static 0.9 # Gpu\u5360\u7528\u7387\uff0c\u8fc7\u9ad8\u53ef\u80fd\u5bfc\u81f4\u5176\u4ed6\u8fdb\u7a0b\u89e6\u53d1OOM\u3002\u53d6\u503c\u8303\u56f4:0~1 \u5185\u7f51API\u8bbf\u95ee \u590d\u5236Api\u8c03\u7528\u793a\u4f8b\uff0c\u5728\u8d44\u6e90\u6807\u7b7e\u9875\u7684ECS\u5b9e\u4f8b\u4e2d\u7c98\u8d34Api\u8c03\u7528\u793a\u4f8b\u5373\u53ef\u3002\u4e5f\u53ef\u5728\u540c\u4e00VPC\u5185\u7684\u5176\u4ed6ECS\u4e2d\u8bbf\u95ee\u3002 \u516c\u7f51API\u8bbf\u95ee \u590d\u5236Api\u8c03\u7528\u793a\u4f8b\uff0c\u5728\u672c\u5730\u7ec8\u7aef\u4e2d\u7c98\u8d34Api\u8c03\u7528\u793a\u4f8b\u5373\u53ef\u3002 \u4f7f\u7528 Chatbox \u5ba2\u6237\u7aef\u914d\u7f6e vLLM API \u8fdb\u884c\u5bf9\u8bdd(\u53ef\u9009) \u8bbf\u95ee Chatbox \u4e0b\u8f7d\u5730\u5740 \u4e0b\u8f7d\u5e76\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u672c\u65b9\u6848\u4ee5 macOS M3 \u4e3a\u4f8b\u3002 \u8fd0\u884c\u5e76\u914d\u7f6e vLLM API \uff0c\u5355\u51fb\u8bbe\u7f6e\u3002 \u5728\u5f39\u51fa\u7684\u770b\u677f\u4e2d\u6309\u7167\u5982\u4e0b\u8868\u683c\u8fdb\u884c\u914d\u7f6e\u3002 \u9879\u76ee \u8bf4\u660e \u793a\u4f8b\u503c \u6a21\u578b\u63d0\u4f9b\u65b9 \u4e0b\u62c9\u9009\u62e9\u6a21\u578b\u63d0\u4f9b\u65b9\u3002 \u6dfb\u52a0\u81ea\u5b9a\u4e49\u63d0\u4f9b\u65b9 \u540d\u79f0 \u586b\u5199\u5b9a\u4e49\u6a21\u578b\u63d0\u4f9b\u65b9\u540d\u79f0\u3002 vLLM API API \u57df\u540d \u586b\u5199\u6a21\u578b\u670d\u52a1\u8c03\u7528\u5730\u5740\u3002 http:// :8000 API \u8def\u5f84 \u586b\u5199 API \u8def\u5f84\u3002 /v1/chat/completions \u7f51\u7edc\u517c\u5bb9\u6027 \u70b9\u51fb\u5f00\u542f\u6539\u5584\u7f51\u7edc\u517c\u5bb9\u6027 \u5f00\u542f API \u5bc6\u94a5 \u586b\u5199\u6a21\u578b\u670d\u52a1\u8c03\u7528 API \u5bc6\u94a5\u3002 \u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\u540e\uff0c\u5728\u670d\u52a1\u5b9e\u4f8b\u9875\u9762\u53ef\u83b7\u53d6Api_Key \u6a21\u578b \u586b\u5199\u8c03\u7528\u7684\u6a21\u578b\u3002 deepseek-ai/DeepSeek-R1 \u4fdd\u5b58\u914d\u7f6e\u3002\u5728\u6587\u672c\u8f93\u5165\u6846\u4e2d\u53ef\u4ee5\u8fdb\u884c\u5bf9\u8bdd\u4ea4\u4e92\u3002\u8f93\u5165\u95ee\u9898\u4f60\u662f\u8c01\uff1f\u6216\u8005\u5176\u4ed6\u6307\u4ee4\u540e\uff0c\u8c03\u7528\u6a21\u578b\u670d\u52a1\u83b7\u5f97\u76f8\u5e94\u7684\u54cd\u5e94\u3002 \u6027\u80fd\u6d4b\u8bd5 \u538b\u6d4b\u8fc7\u7a0b(\u4f9b\u53c2\u8003) \u524d\u63d0\u6761\u4ef6\uff1a 1. \u65e0\u6cd5\u76f4\u63a5\u6d4b\u8bd5\u5e26api-key\u7684\u6a21\u578b\u670d\u52a1\uff1b2. \u9700\u8981\u516c\u7f51\u3002 \u91cd\u65b0\u90e8\u7f72\u6a21\u578b\u670d\u52a1 \u8fdc\u7a0b\u8fde\u63a5\uff0c\u767b\u5165worker\u8282\u70b9\uff08\u547d\u540d\u4e3allm-xxxx-worker\uff09\u3002 \u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5c06\u6a21\u578b\u670d\u52a1\u505c\u6b62\u3002 ```shell sudo docker stop vllm sudo docker rm vllm \u8bf7\u53c2\u8003\u672c\u6587\u6863\u4e2d\u7684 \u67e5\u8be2\u6a21\u578b\u90e8\u7f72\u53c2\u6570 \u90e8\u5206\uff0c\u83b7\u53d6worker\u8282\u70b9\u6a21\u578b\u90e8\u7f72\u5b9e\u9645\u6267\u884c\u7684\u811a\u672c\u3002 \u53bb\u6389\u811a\u672c\u4e2d\u7684--api-key\u53c2\u6570\uff0c\u5728ECS\u5b9e\u4f8b\u4e2d\u6267\u884c\u5269\u4f59\u811a\u672c\u3002\u6267\u884cdocker logs vllm\u3002\u82e5\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u6a21\u578b\u670d\u52a1\u91cd\u65b0\u90e8\u7f72\u6210\u529f\u3002 \u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5 \u4ee5Deepseek-R1\u4e3a\u4f8b\uff0c\u6a21\u578b\u670d\u52a1\u90e8\u7f72\u5b8c\u6210\u540e\uff0cssh\u767b\u5f55ECS\u5b9e\u4f8b\u3002\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5373\u53ef\u5f97\u5230\u6a21\u578b\u670d\u52a1\u6027\u80fd\u6d4b\u8bd5\u7ed3\u679c\u3002\u53ef\u6839\u636e\u53c2\u6570\u8bf4\u660e\u81ea\u884c\u4fee\u6539\u3002 ```shell yum install -y git-lfs git lfs install git lfs clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git git lfs clone https://github.com/vllm-project/vllm.git docker exec vllm bash -c \" pip install pandas datasets && python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model /root/llm-model/deepseek-ai/DeepSeek-R1 \\ --served-model-name deepseek-ai/DeepSeek-R1 \\ --sonnet-input-len 1024 \\ # \u6700\u5927\u8f93\u5165\u957f\u5ea6 --sonnet-output-len 4096 \\ # \u6700\u5927\u8f93\u51fa\u957f\u5ea6 --sonnet-prefix-len 50 \\ # \u524d\u7f00\u957f\u5ea6 --num-prompts 400 \\ # \u4ece\u6570\u636e\u96c6\u4e2d\u968f\u673a\u9009\u53d6\u6216\u6309\u987a\u5e8f\u5904\u7406 400 \u4e2a prompt \u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002 --request-rate 20 \\ # \u6a21\u62df\u6bcf\u79d2 20 \u4e2a\u5e76\u53d1\u8bf7\u6c42\u7684\u538b\u529b\u6d4b\u8bd5\uff0c\u6301\u7eed20\u79d2\uff0c\u5171400\u4e2a\u8bf7\u6c42\u3002\u8bc4\u4f30\u6a21\u578b\u670d\u52a1\u5728\u8d1f\u8f7d\u4e0b\u7684\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u3002 --port 8000 \\ --trust-remote-code \\ --dataset-name sharegpt \\ --save-result \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \" ``` \u6027\u80fd\u6d4b\u8bd5\u7ed3\u679c \u672c\u670d\u52a1\u65b9\u6848\u4e0b\uff0c\u9488\u5bf9Deepseek-R1\u548cV3\uff0c\u5206\u522b\u6d4b\u8bd5QPS\u4e3a75\u548c60\u60c5\u51b5\u4e0b\u6a21\u578b\u670d\u52a1\u7684\u63a8\u7406\u54cd\u5e94\u6027\u80fd\uff0c\u538b\u6d4b\u6301\u7eed\u65f6\u95f4\u5747\u4e3a20s\u3002 Deepseek-R1 QPS\u4e3a75 Deepseek-V3 QPS\u4e3a60","title":"\u57fa\u4e8e\u53ccECS\u5b9e\u4f8b\u7684DeepSeek-R1\u548cV3\u6a21\u578b\u90e8\u7f72\u6587\u6863"},{"location":"index-ecs-two/#ecsdeepseek-r1v3","text":"","title":"\u57fa\u4e8e\u53ccECS\u5b9e\u4f8b\u7684DeepSeek-R1\u548cV3\u6a21\u578b\u90e8\u7f72\u6587\u6863"},{"location":"index-ecs-two/#_1","text":"\u672c\u670d\u52a1\u63d0\u4f9b\u4e86\u57fa\u4e8eECS\u955c\u50cf+Vllm+Ray\u7684\u5927\u6a21\u578b\u4e00\u952e\u90e8\u7f72\u65b9\u6848\uff0c30\u5206\u949f\u5373\u53ef\u901a\u8fc7\u53ccECS\u5b9e\u4f8b\u90e8\u7f72\u4f7f\u7528DeepSeek-R1\u6ee1\u8840\u7248\u548cDeepSeek-V3\u6a21\u578b\u3002 \u672c\u670d\u52a1\u901a\u8fc7ECS\u955c\u50cf\u6253\u5305\u6807\u51c6\u73af\u5883\uff0c\u901a\u8fc7Ros\u6a21\u7248\u5b9e\u73b0\u4e91\u8d44\u6e90\u4e0e\u5927\u6a21\u578b\u7684\u4e00\u952e\u90e8\u7f72\uff0c\u5f00\u53d1\u8005\u65e0\u9700\u5173\u5fc3\u6a21\u578b\u90e8\u7f72\u8fd0\u884c\u7684\u6807\u51c6\u73af\u5883\u4e0e\u5e95\u5c42\u4e91\u8d44\u6e90\u7f16\u6392\uff0c\u4ec5\u9700\u6dfb\u52a0\u51e0\u4e2a\u53c2\u6570\u5373\u53ef\u4eab\u53d7DeepSeek-R1\u6ee1\u8840\u7248\u548cDeepSeek-V3\u7684\u63a8\u7406\u4f53\u9a8c\u3002 \u672c\u670d\u52a1\u63d0\u4f9b\u7684\u65b9\u6848\u4e0b\uff0c\u4ee5\u5e73\u5747\u6bcf\u6b21\u8bf7\u6c42\u7684token\u4e3a10kb\u8ba1\u7b97\uff0c\u91c7\u7528\u4e24\u53f0GU8TF\u89c4\u683c\u7684ECS\u5b9e\u4f8b\uff0cDeepSeek-R1\u6ee1\u8840\u7248\u7406\u8bba\u53ef\u652f\u6301\u7684\u6bcf\u79d2\u5e76\u53d1\u8bf7\u6c42\u6570(QPS)\u7ea6\u4e3a75\uff0cDeepSeek-V3\u7ea6\u4e3a67\u3002 \u672c\u670d\u52a1\u652f\u6301\u7684\u6a21\u578b\u5982\u4e0b\uff1a * deepseek-ai/DeepSeek-R1 * deepseek-ai/DeepSeek-V3","title":"\u90e8\u7f72\u8bf4\u660e"},{"location":"index-ecs-two/#_2","text":"","title":"\u6574\u4f53\u67b6\u6784"},{"location":"index-ecs-two/#_3","text":"\u672c\u670d\u52a1\u5728\u963f\u91cc\u4e91\u4e0a\u7684\u8d39\u7528\u4e3b\u8981\u6d89\u53ca\uff1a * \u6240\u9009GPU\u4e91\u670d\u52a1\u5668\u7684\u89c4\u683c * \u8282\u70b9\u6570\u91cf * \u78c1\u76d8\u5bb9\u91cf * \u516c\u7f51\u5e26\u5bbd \u8ba1\u8d39\u65b9\u5f0f\uff1a\u6309\u91cf\u4ed8\u8d39\uff08\u5c0f\u65f6\uff09\u6216\u5305\u5e74\u5305\u6708 \u9884\u4f30\u8d39\u7528\u5728\u521b\u5efa\u5b9e\u4f8b\u65f6\u53ef\u5b9e\u65f6\u770b\u5230\u3002","title":"\u8ba1\u8d39\u8bf4\u660e"},{"location":"index-ecs-two/#ram","text":"\u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\uff0c\u9700\u8981\u5bf9\u90e8\u5206\u963f\u91cc\u4e91\u8d44\u6e90\u8fdb\u884c\u8bbf\u95ee\u548c\u521b\u5efa\u64cd\u4f5c\u3002\u56e0\u6b64\u60a8\u7684\u8d26\u53f7\u9700\u8981\u5305\u542b\u5982\u4e0b\u8d44\u6e90\u7684\u6743\u9650\u3002 \u6743\u9650\u7b56\u7565\u540d\u79f0 \u5907\u6ce8 AliyunECSFullAccess \u7ba1\u7406\u4e91\u670d\u52a1\u5668\u670d\u52a1\uff08ECS\uff09\u7684\u6743\u9650 AliyunVPCFullAccess \u7ba1\u7406\u4e13\u6709\u7f51\u7edc\uff08VPC\uff09\u7684\u6743\u9650 AliyunROSFullAccess \u7ba1\u7406\u8d44\u6e90\u7f16\u6392\u670d\u52a1\uff08ROS\uff09\u7684\u6743\u9650 AliyunComputeNestUserFullAccess \u7ba1\u7406\u8ba1\u7b97\u5de2\u670d\u52a1\uff08ComputeNest\uff09\u7684\u7528\u6237\u4fa7\u6743\u9650","title":"RAM\u8d26\u53f7\u6240\u9700\u6743\u9650"},{"location":"index-ecs-two/#_4","text":"\u5355\u51fb \u90e8\u7f72\u94fe\u63a5 \u3002\u9009\u62e9\u53cc\u673a\u7248\uff0c\u5e76\u786e\u8ba4\u5df2\u7533\u8bf7GU8TF\u5b9e\u4f8b\u89c4\u683c\u3002\u6839\u636e\u754c\u9762\u63d0\u793a\u586b\u5199\u53c2\u6570\uff0c\u53ef\u6839\u636e\u9700\u6c42\u9009\u62e9\u662f\u5426\u5f00\u542f\u516c\u7f51\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u8be2\u4ef7\u660e\u7ec6\uff0c\u786e\u8ba4\u53c2\u6570\u540e\u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u3002 \u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u540e\u53ef\u4ee5\u770b\u5230\u4ef7\u683c\u9884\u89c8\uff0c\u968f\u540e\u53ef\u70b9\u51fb \u7acb\u5373\u90e8\u7f72 \uff0c\u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u3002(\u63d0\u793aRAM\u6743\u9650\u4e0d\u8db3\u65f6\u9700\u8981\u4e3a\u5b50\u8d26\u53f7\u6dfb\u52a0RAM\u6743\u9650) \u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u540e\uff0c\u5c31\u53ef\u4ee5\u5f00\u59cb\u4f7f\u7528\u670d\u52a1\u4e86\u3002\u70b9\u51fb\u670d\u52a1\u5b9e\u4f8b\u540d\u79f0\uff0c\u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\uff0c\u4f7f\u7528Api\u8c03\u7528\u793a\u4f8b\u5373\u53ef\u8bbf\u95ee\u670d\u52a1\u3002\u5982\u679c\u662f\u5185\u7f51\u8bbf\u95ee\uff0c\u9700\u4fdd\u8bc1ECS\u5b9e\u4f8b\u5728\u540c\u4e00\u4e2aVPC\u4e0b\u3002 ssh\u8bbf\u95eeECS\u5b9e\u4f8b\u540e\uff0c\u6267\u884c docker logs vllm \u5373\u53ef\u67e5\u8be2\u6a21\u578b\u670d\u52a1\u90e8\u7f72\u65e5\u5fd7\u3002\u5f53\u60a8\u770b\u5230\u4e0b\u56fe\u6240\u793a\u7ed3\u679c\u65f6\uff0c\u8868\u793a\u6a21\u578b\u670d\u52a1\u90e8\u7f72\u6210\u529f\u3002\u6a21\u578b\u6240\u5728\u8def\u5f84\u4e3a/root/llm_model/\u3002","title":"\u90e8\u7f72\u6d41\u7a0b"},{"location":"index-ecs-two/#_5","text":"","title":"\u4f7f\u7528\u8bf4\u660e"},{"location":"index-ecs-two/#_6","text":"\u590d\u5236\u670d\u52a1\u5b9e\u4f8b\u540d\u79f0\u3002\u5230 \u8d44\u6e90\u7f16\u6392\u63a7\u5236\u53f0 \u67e5\u770b\u5bf9\u5e94\u7684\u8d44\u6e90\u6808\u3002 \u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u5bf9\u5e94\u7684\u8d44\u6e90\u6808\uff0c\u53ef\u4ee5\u770b\u5230\u6240\u5f00\u542f\u7684\u5168\u90e8\u8d44\u6e90\uff0c\u5e76\u67e5\u770b\u5230\u6a21\u578b\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u6267\u884c\u7684\u5168\u90e8\u811a\u672c\u3002","title":"\u67e5\u8be2\u6a21\u578b\u90e8\u7f72\u53c2\u6570"},{"location":"index-ecs-two/#_7","text":"\u5982\u679c\u60a8\u6709\u81ea\u5b9a\u4e49\u7684\u6a21\u578b\u90e8\u7f72\u53c2\u6570\u7684\u9700\u6c42\uff0c\u53ef\u4ee5\u5728\u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\u540e\uff0c\u6309\u7167\u5982\u4e0b\u64cd\u4f5c\u6b65\u9aa4\u8fdb\u884c\u4fee\u6539\u3002\u5f53\u524d\u63d0\u4f9bvllm\u548csglang\u4e24\u79cd\u90e8\u7f72\u65b9\u5f0f\u3002 \u8fdc\u7a0b\u8fde\u63a5\uff0c\u5206\u522b\u767b\u5165master\u8282\u70b9\u548cworker\u8282\u70b9\uff08\u4e24\u53f0\u5b9e\u4f8b\u5206\u522b\u547d\u540d\u4e3allm-xxxx-master\u548cllm-xxxx-worker\uff09\u3002 \u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5c06\u4e24\u4e2a\u8282\u70b9\u5185\u7684\u6a21\u578b\u670d\u52a1\u90fd\u505c\u6b62\u3002 ```shell sudo docker stop vllm sudo docker rm vllm \u8bf7\u53c2\u8003\u672c\u6587\u6863\u4e2d\u7684 \u67e5\u8be2\u6a21\u578b\u90e8\u7f72\u53c2\u6570 \u90e8\u5206\uff0c\u83b7\u53d6master\u8282\u70b9\u548cworker\u8282\u70b9\u4e2d\u6a21\u578b\u90e8\u7f72\u5b9e\u9645\u6267\u884c\u7684\u811a\u672c\u3002 \u4e0b\u9762\u5206\u522b\u662fvllm\u4e0esglang\u90e8\u7f72\u7684\u53c2\u8003\u811a\u672c\uff0c\u60a8\u53ef\u53c2\u8003\u53c2\u6570\u6ce8\u91ca\u81ea\u5b9a\u4e49\u6a21\u578b\u90e8\u7f72\u53c2\u6570\uff0c\u4fee\u6539\u5b9e\u9645\u6267\u884c\u7684\u811a\u672c\u3002\u4fee\u6539\u540e\uff0c\u5148\u6267\u884cmaster\u8282\u70b9\u811a\u672c\uff0c\u6210\u529f\u540e\uff0c\u518d\u6267\u884cworker\u8282\u70b9\u811a\u672c\u5373\u53ef\u3002 vllm\u90e8\u7f72master\u8282\u70b9\u53c2\u8003\u811a\u672c ```shell docker run -t -d \\ --entrypoint /bin/bash \\ --name=vllm \\ --ipc=host \\ --cap-add=SYS_PTRACE \\ --network=host \\ --gpus all \\ --privileged \\ --ulimit memlock=-1 \\ --ulimit stack=67108864 \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install --upgrade vllm==0.8.2 && # \u53ef\u81ea\u5b9a\u4e49\u7248\u672c\uff0c\u5982 pip install vllm==0.7.1\u3002\u5fc5\u987b\u4e0eworker\u8282\u70b9\u4fdd\u6301\u4e00\u81f4\u3002 export NCCL_IB_DISABLE=0 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_DEBUG=INFO && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_NET_GDR_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_P2P_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_IB_GID_INDEX=1 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export GLOO_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 export NCCL_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 ray start --head --dashboard-host 0.0.0.0 --port=6379 && tail -f /dev/null\" vllm\u90e8\u7f72worker\u8282\u70b9\u53c2\u8003\u811a\u672c ```shell docker run -t -d \\ --entrypoint /bin/bash \\ --name=vllm \\ --ipc=host \\ --cap-add=SYS_PTRACE \\ --network=host \\ --gpus all \\ --privileged \\ --ulimit memlock=-1 \\ --ulimit stack=67108864 \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install --upgrade vllm==0.8.2 && # \u53ef\u81ea\u5b9a\u4e49\u7248\u672c\uff0c\u5982 pip install vllm==0.7.1\u3002\u5fc5\u987b\u4e0emaster\u8282\u70b9\u4fdd\u6301\u4e00\u81f4\u3002 export NCCL_IB_DISABLE=0 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_DEBUG=INFO && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_NET_GDR_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_P2P_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_IB_GID_INDEX=1 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export GLOO_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 export NCCL_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 ray start --address='${HEAD_NODE_ADDRESS}:6379' && # \u586b\u5199master\u8282\u70b9\u7684\u5185\u7f51IP\u5730\u5740\u3002 vllm serve /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --gpu-memory-utilization 0.98 \\ # Gpu\u5360\u7528\u7387\uff0c\u8fc7\u9ad8\u53ef\u80fd\u5bfc\u81f4\u5176\u4ed6\u8fdb\u7a0b\u89e6\u53d1OOM\u3002\u53d6\u503c\u8303\u56f4:0~1 --max-model-len ${MaxModelLen} \\ # \u6a21\u578b\u6700\u5927\u957f\u5ea6\uff0c\u53d6\u503c\u8303\u56f4\u4e0e\u6a21\u578b\u672c\u8eab\u6709\u5173\u3002 --enable-chunked-prefill \\ --host=0.0.0.0 \\ --port 8000 \\ --trust-remote-code \\ --api-key \"${VLLM_API_KEY}\" \\ --tensor-parallel-size $(nvidia-smi --query-gpu=index --format=csv,noheader | wc -l | awk '{print $1}') \\ # \u5355\u8282\u70b9\u4f7f\u7528GPU\u6570\u91cf\uff0c\u9ed8\u8ba4\u4f7f\u7528\u5355\u53f0ECS\u5b9e\u4f8b\u7684\u5168\u90e8GPU\u3002 --pipeline-parallel-size 2\" # \u6d41\u7ebf\u5e76\u884c\u6570\uff0c\u63a8\u8350\u8bbe\u7f6e\u4e3a\u8282\u70b9\u603b\u6570\u3002 sglang\u90e8\u7f72master\u8282\u70b9\u53c2\u8003\u811a\u672c ```shell docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name llm-server \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install sglang==0.4.3 && # \u53ef\u81ea\u5b9a\u4e49\u7248\u672c\uff0c\u5fc5\u987b\u4e0eworker\u8282\u70b9\u4fdd\u6301\u4e00\u81f4 export NCCL_IB_DISABLE=0 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_DEBUG=INFO && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_NET_GDR_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_P2P_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_IB_GID_INDEX=1 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export GLOO_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 export NCCL_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 python3 -m sglang.launch_server \\ --model-path /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --tp 16 \\ # \u5f53\u524dsglang\u4e0d\u652f\u6301\u6d41\u7ebf\u5e76\u884c\uff0c\u9ed8\u8ba4\u4f7f\u7528\u4e24\u53f0ECS\u5b9e\u4f8b\u4e2d\u5168\u90e8GPU\u3002 --dist-init-addr ${HEAD_NODE_ADDRESS}:20000 # \u586b\u5199master\u8282\u70b9\u7684\u5185\u7f51IP\u5730\u5740\u3002 --nnodes 2 # \u8282\u70b9\u603b\u7b97\uff0c\u9ed8\u8ba4\u4f7f\u7528\u4e24\u53f0ECS\u5b9e\u4f8b\u3002 --node-rank 0 # \u8282\u70b9\u5e8f\u53f7\uff0c\u9ed8\u8ba4\u4e3a0\u3002 --trust-remote-code \\ --host 0.0.0.0 \\ --port 8000 \\ --mem-fraction-static 0.9 # Gpu\u5360\u7528\u7387\uff0c\u8fc7\u9ad8\u53ef\u80fd\u5bfc\u81f4\u5176\u4ed6\u8fdb\u7a0b\u89e6\u53d1OOM\u3002\u53d6\u503c\u8303\u56f4:0~1 sglang\u90e8\u7f72worker\u8282\u70b9\u53c2\u8003\u811a\u672c ```shell docker run -d -t --net=host --gpus all \\ --entrypoint /bin/bash \\ --privileged \\ --ipc=host \\ --name llm-server \\ -v /root:/root \\ egs-registry.cn-hangzhou.cr.aliyuncs.com/egs/vllm:0.7.2-sglang0.4.3.post2-pytorch2.5-cuda12.4-20250224 \\ -c \"pip install sglang==0.4.3 && # \u53ef\u81ea\u5b9a\u4e49\u7248\u672c\uff0c\u5fc5\u987b\u4e0emaster\u8282\u70b9\u4fdd\u6301\u4e00\u81f4 export NCCL_IB_DISABLE=0 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_DEBUG=INFO && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_NET_GDR_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_P2P_LEVEL=5 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export NCCL_IB_GID_INDEX=1 && # \u91c7\u7528\u5f39\u6027RDMA\u8fdb\u884c\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u4e0d\u5efa\u8bae\u6539\u53d8 export GLOO_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 export NCCL_SOCKET_IFNAME=eth0 && # \u91c7\u7528vpc\u8fdb\u884c\u7f51\u7edc\u901a\u4fe1\u6240\u9700\u73af\u5883\u53d8\u91cf\uff0c\u52ff\u5220\u6539 python3 -m sglang.launch_server \\ --model-path /root/llm-model/${ModelName} \\ --served-model-name ${ModelName} \\ --tp 16 \\ # \u5f53\u524dsglang\u4e0d\u652f\u6301\u6d41\u7ebf\u5e76\u884c\uff0c\u9ed8\u8ba4\u4f7f\u7528\u4e24\u53f0ECS\u5b9e\u4f8b\u4e2d\u5168\u90e8GPU\u3002 --dist-init-addr ${HEAD_NODE_ADDRESS}:20000 # \u586b\u5199master\u8282\u70b9\u7684\u5185\u7f51IP\u5730\u5740\u3002 --nnodes 2 # \u8282\u70b9\u603b\u7b97\uff0c\u9ed8\u8ba4\u4f7f\u7528\u4e24\u53f0ECS\u5b9e\u4f8b\u3002 --node-rank 1 # \u8282\u70b9\u5e8f\u53f7\uff0c\u9ed8\u8ba4\u4e3a1\u3002 --trust-remote-code \\ --host 0.0.0.0 \\ --port 8000 \\ --mem-fraction-static 0.9 # Gpu\u5360\u7528\u7387\uff0c\u8fc7\u9ad8\u53ef\u80fd\u5bfc\u81f4\u5176\u4ed6\u8fdb\u7a0b\u89e6\u53d1OOM\u3002\u53d6\u503c\u8303\u56f4:0~1","title":"\u81ea\u5b9a\u4e49\u6a21\u578b\u90e8\u7f72\u53c2\u6570"},{"location":"index-ecs-two/#api","text":"\u590d\u5236Api\u8c03\u7528\u793a\u4f8b\uff0c\u5728\u8d44\u6e90\u6807\u7b7e\u9875\u7684ECS\u5b9e\u4f8b\u4e2d\u7c98\u8d34Api\u8c03\u7528\u793a\u4f8b\u5373\u53ef\u3002\u4e5f\u53ef\u5728\u540c\u4e00VPC\u5185\u7684\u5176\u4ed6ECS\u4e2d\u8bbf\u95ee\u3002","title":"\u5185\u7f51API\u8bbf\u95ee"},{"location":"index-ecs-two/#api_1","text":"\u590d\u5236Api\u8c03\u7528\u793a\u4f8b\uff0c\u5728\u672c\u5730\u7ec8\u7aef\u4e2d\u7c98\u8d34Api\u8c03\u7528\u793a\u4f8b\u5373\u53ef\u3002","title":"\u516c\u7f51API\u8bbf\u95ee"},{"location":"index-ecs-two/#chatbox-vllm-api","text":"\u8bbf\u95ee Chatbox \u4e0b\u8f7d\u5730\u5740 \u4e0b\u8f7d\u5e76\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u672c\u65b9\u6848\u4ee5 macOS M3 \u4e3a\u4f8b\u3002 \u8fd0\u884c\u5e76\u914d\u7f6e vLLM API \uff0c\u5355\u51fb\u8bbe\u7f6e\u3002 \u5728\u5f39\u51fa\u7684\u770b\u677f\u4e2d\u6309\u7167\u5982\u4e0b\u8868\u683c\u8fdb\u884c\u914d\u7f6e\u3002 \u9879\u76ee \u8bf4\u660e \u793a\u4f8b\u503c \u6a21\u578b\u63d0\u4f9b\u65b9 \u4e0b\u62c9\u9009\u62e9\u6a21\u578b\u63d0\u4f9b\u65b9\u3002 \u6dfb\u52a0\u81ea\u5b9a\u4e49\u63d0\u4f9b\u65b9 \u540d\u79f0 \u586b\u5199\u5b9a\u4e49\u6a21\u578b\u63d0\u4f9b\u65b9\u540d\u79f0\u3002 vLLM API API \u57df\u540d \u586b\u5199\u6a21\u578b\u670d\u52a1\u8c03\u7528\u5730\u5740\u3002 http:// :8000 API \u8def\u5f84 \u586b\u5199 API \u8def\u5f84\u3002 /v1/chat/completions \u7f51\u7edc\u517c\u5bb9\u6027 \u70b9\u51fb\u5f00\u542f\u6539\u5584\u7f51\u7edc\u517c\u5bb9\u6027 \u5f00\u542f API \u5bc6\u94a5 \u586b\u5199\u6a21\u578b\u670d\u52a1\u8c03\u7528 API \u5bc6\u94a5\u3002 \u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\u540e\uff0c\u5728\u670d\u52a1\u5b9e\u4f8b\u9875\u9762\u53ef\u83b7\u53d6Api_Key \u6a21\u578b \u586b\u5199\u8c03\u7528\u7684\u6a21\u578b\u3002 deepseek-ai/DeepSeek-R1 \u4fdd\u5b58\u914d\u7f6e\u3002\u5728\u6587\u672c\u8f93\u5165\u6846\u4e2d\u53ef\u4ee5\u8fdb\u884c\u5bf9\u8bdd\u4ea4\u4e92\u3002\u8f93\u5165\u95ee\u9898\u4f60\u662f\u8c01\uff1f\u6216\u8005\u5176\u4ed6\u6307\u4ee4\u540e\uff0c\u8c03\u7528\u6a21\u578b\u670d\u52a1\u83b7\u5f97\u76f8\u5e94\u7684\u54cd\u5e94\u3002","title":"\u4f7f\u7528 Chatbox \u5ba2\u6237\u7aef\u914d\u7f6e vLLM API \u8fdb\u884c\u5bf9\u8bdd(\u53ef\u9009)"},{"location":"index-ecs-two/#_8","text":"","title":"\u6027\u80fd\u6d4b\u8bd5"},{"location":"index-ecs-two/#_9","text":"\u524d\u63d0\u6761\u4ef6\uff1a 1. \u65e0\u6cd5\u76f4\u63a5\u6d4b\u8bd5\u5e26api-key\u7684\u6a21\u578b\u670d\u52a1\uff1b2. \u9700\u8981\u516c\u7f51\u3002","title":"\u538b\u6d4b\u8fc7\u7a0b(\u4f9b\u53c2\u8003)"},{"location":"index-ecs-two/#_10","text":"\u8fdc\u7a0b\u8fde\u63a5\uff0c\u767b\u5165worker\u8282\u70b9\uff08\u547d\u540d\u4e3allm-xxxx-worker\uff09\u3002 \u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5c06\u6a21\u578b\u670d\u52a1\u505c\u6b62\u3002 ```shell sudo docker stop vllm sudo docker rm vllm \u8bf7\u53c2\u8003\u672c\u6587\u6863\u4e2d\u7684 \u67e5\u8be2\u6a21\u578b\u90e8\u7f72\u53c2\u6570 \u90e8\u5206\uff0c\u83b7\u53d6worker\u8282\u70b9\u6a21\u578b\u90e8\u7f72\u5b9e\u9645\u6267\u884c\u7684\u811a\u672c\u3002 \u53bb\u6389\u811a\u672c\u4e2d\u7684--api-key\u53c2\u6570\uff0c\u5728ECS\u5b9e\u4f8b\u4e2d\u6267\u884c\u5269\u4f59\u811a\u672c\u3002\u6267\u884cdocker logs vllm\u3002\u82e5\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u6a21\u578b\u670d\u52a1\u91cd\u65b0\u90e8\u7f72\u6210\u529f\u3002","title":"\u91cd\u65b0\u90e8\u7f72\u6a21\u578b\u670d\u52a1"},{"location":"index-ecs-two/#_11","text":"\u4ee5Deepseek-R1\u4e3a\u4f8b\uff0c\u6a21\u578b\u670d\u52a1\u90e8\u7f72\u5b8c\u6210\u540e\uff0cssh\u767b\u5f55ECS\u5b9e\u4f8b\u3002\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5373\u53ef\u5f97\u5230\u6a21\u578b\u670d\u52a1\u6027\u80fd\u6d4b\u8bd5\u7ed3\u679c\u3002\u53ef\u6839\u636e\u53c2\u6570\u8bf4\u660e\u81ea\u884c\u4fee\u6539\u3002 ```shell yum install -y git-lfs git lfs install git lfs clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git git lfs clone https://github.com/vllm-project/vllm.git docker exec vllm bash -c \" pip install pandas datasets && python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model /root/llm-model/deepseek-ai/DeepSeek-R1 \\ --served-model-name deepseek-ai/DeepSeek-R1 \\ --sonnet-input-len 1024 \\ # \u6700\u5927\u8f93\u5165\u957f\u5ea6 --sonnet-output-len 4096 \\ # \u6700\u5927\u8f93\u51fa\u957f\u5ea6 --sonnet-prefix-len 50 \\ # \u524d\u7f00\u957f\u5ea6 --num-prompts 400 \\ # \u4ece\u6570\u636e\u96c6\u4e2d\u968f\u673a\u9009\u53d6\u6216\u6309\u987a\u5e8f\u5904\u7406 400 \u4e2a prompt \u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002 --request-rate 20 \\ # \u6a21\u62df\u6bcf\u79d2 20 \u4e2a\u5e76\u53d1\u8bf7\u6c42\u7684\u538b\u529b\u6d4b\u8bd5\uff0c\u6301\u7eed20\u79d2\uff0c\u5171400\u4e2a\u8bf7\u6c42\u3002\u8bc4\u4f30\u6a21\u578b\u670d\u52a1\u5728\u8d1f\u8f7d\u4e0b\u7684\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u3002 --port 8000 \\ --trust-remote-code \\ --dataset-name sharegpt \\ --save-result \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \" ```","title":"\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5"},{"location":"index-ecs-two/#_12","text":"\u672c\u670d\u52a1\u65b9\u6848\u4e0b\uff0c\u9488\u5bf9Deepseek-R1\u548cV3\uff0c\u5206\u522b\u6d4b\u8bd5QPS\u4e3a75\u548c60\u60c5\u51b5\u4e0b\u6a21\u578b\u670d\u52a1\u7684\u63a8\u7406\u54cd\u5e94\u6027\u80fd\uff0c\u538b\u6d4b\u6301\u7eed\u65f6\u95f4\u5747\u4e3a20s\u3002","title":"\u6027\u80fd\u6d4b\u8bd5\u7ed3\u679c"},{"location":"index-ecs-two/#deepseek-r1","text":"","title":"Deepseek-R1"},{"location":"index-ecs-two/#qps75","text":"","title":"QPS\u4e3a75"},{"location":"index-ecs-two/#deepseek-v3","text":"","title":"Deepseek-V3"},{"location":"index-ecs-two/#qps60","text":"","title":"QPS\u4e3a60"},{"location":"ack-ch/index-ack/","text":"\u57fa\u4e8eACK\u96c6\u7fa4\u7684\u5927\u6a21\u578b\u90e8\u7f72\u6587\u6863 \u90e8\u7f72\u8bf4\u660e \u672c\u65b9\u6848\u901a\u8fc7\u963f\u91cc\u4e91\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u73b0\u5f00\u7bb1\u5373\u7528\u7684\u5927\u6a21\u578b\u63a8\u7406\u670d\u52a1\u90e8\u7f72\uff0c\u652f\u6301\u4ee5\u4e0b\u573a\u666f\uff1a - \u65b0\u5efaACK\u96c6\u7fa4 \uff1a\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u76f4\u63a5\u521b\u5efa\u4e00\u4e2aACK\u96c6\u7fa4\uff0c\u8ba1\u7b97\u5de2\u4f1a\u5728\u7528\u6237\u8d26\u53f7\u4e0b\u4e00\u952e\u521b\u5efaACK\u96c6\u7fa4\u548cOSS Bucket\uff0c\u5b8c\u6210\u4e86\u6a21\u578b\u4e0a\u4f20\u548cBucket\u7684\u6302\u8f7d\u540e\u4f1a\u81ea\u52a8\u90e8\u7f72\u6a21\u578b\uff0c\u6700\u540e\u4f1a\u81ea\u52a8\u521b\u5efa\u8d1f\u8f7d\u5747\u8861\u5b9e\u73b0\u5185\u3001\u516c\u7f51\u7684\u8bbf\u95ee\u3002 - \u9009\u62e9\u5df2\u6709ACK\u96c6\u7fa4 \uff1a\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528\u5df2\u6709\u7684ACK\u96c6\u7fa4\uff0c\u8ba1\u7b97\u5de2\u4f1a\u5728\u7528\u6237\u8d26\u53f7\u4e0b\u521b\u5efaOSS Bucket\uff08\u4e5f\u53ef\u4ee5\u9009\u62e9\u5df2\u6709\u7684Bucket\uff09\uff0c\u5b8c\u6210\u4e86\u6a21\u578b\u4e0a\u4f20\u548cBucket\u7684\u6302\u8f7d\u540e\u4f1a\u81ea\u52a8\u90e8\u7f72\u6a21\u578b\uff0c\u6700\u540e\u4f1a\u81ea\u52a8\u521b\u5efa\u8d1f\u8f7d\u5747\u8861\u5b9e\u73b0\u5185\u3001\u516c\u7f51\u7684\u8bbf\u95ee\u3002 \u672c\u65b9\u6848\u57fa\u4e8e\u4ee5\u4e0b\u6838\u5fc3\u7ec4\u4ef6\uff1a vLLM \uff1a\u63d0\u4f9b\u9ad8\u6027\u80fd\u5e76\u884c\u63a8\u7406\u80fd\u529b\uff0c\u652f\u6301\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u7684LLM\u63a8\u7406\uff08\u652f\u6301Qwen\u3001DeepSeek\u5168\u7cfb\u5217\u6a21\u578b\uff09 SGLang : SGLang \u662f\u4e00\u4e2a\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u670d\u52a1\u6846\u67b6\u3002 ACK\u96c6\u7fa4 \uff1a\u63d0\u4f9b\u5168\u6258\u7ba1\u7684Kubernetes\u73af\u5883 A10/L20/GU8TF GPU\u52a0\u901f \uff1a\u652f\u6301\u591a\u79cd\u7b97\u529b\u89c4\u683c\uff0c\u6ee1\u8db3\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u7684\u63a8\u7406\u9700\u6c42 \u90e8\u7f72\u540e\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u79c1\u6709/\u516c\u7f51API\u8c03\u7528\u6a21\u578b\u670d\u52a1\uff0c\u8d44\u6e90\u5229\u7528\u7387\u63d0\u5347\u6570\u500d\uff0c\u5f00\u53d1\u8005\u65e0\u9700\u5173\u6ce8\u5e95\u5c42\u5bb9\u5668\u7f16\u6392\u4e0e\u8d44\u6e90\u8c03\u5ea6\uff0c\u4ec5\u9700\u5728\u8ba1\u7b97\u5de2\u63a7\u5236\u53f0\u9875\u9762\u9009\u62e9\u6a21\u578b\u5373\u53ef\u5b8c\u6210\u4e00\u952e\u90e8\u7f72\u3002 \u6574\u4f53\u67b6\u6784 \u8ba1\u8d39\u8bf4\u660e \u8d44\u6e90\u7c7b\u578b \u8ba1\u8d39\u6a21\u5f0f \u5173\u952e\u914d\u7f6e\u8bf4\u660e ACK\u96c6\u7fa4 \u6309\u91cf\u4ed8\u8d39 \u6839\u636e\u6240\u9009GPU\u7c7b\u578b\u548c\u6570\u91cf\u8ba1\u8d39 ECS\u8df3\u677f\u673a \u6309\u91cf\u4ed8\u8d39 ecs.u1-c1m2.xlarge\uff084C8G\uff09\uff0c\u7528\u4e8e\u96c6\u7fa4\u7ba1\u7406\uff0c\u90e8\u7f72\u5b8c\u6210\u540e\u53ef\u5b89\u5168\u91ca\u653e OSS\u5b58\u50a8 \u6309\u91cf\u4ed8\u8d39 \u5b58\u50a8\u6a21\u578b\u6587\u4ef6\uff0c\u5efa\u8bae\u9009\u62e9\u4e0e\u96c6\u7fa4\u540c\u5730\u57df\u7684\u5b58\u50a8\u7c7b\u578b NAT\u7f51\u5173 \u6309\u91cf\u4ed8\u8d39 \u5f53\u5f00\u542f\u516c\u7f51\u8bbf\u95ee\u65f6\u81ea\u52a8\u521b\u5efa\uff0c\u6309\u4f7f\u7528\u65f6\u957f\u548c\u5e26\u5bbd\u8ba1\u8d39 RAM\u8d26\u53f7\u6240\u9700\u6743\u9650 \u90e8\u7f72\u5b9e\u4f8b\u9700\u8981\u5bf9\u90e8\u5206\u963f\u91cc\u4e91\u8d44\u6e90\u8fdb\u884c\u8bbf\u95ee\u548c\u521b\u5efa\u64cd\u4f5c\u3002\u56e0\u6b64\u60a8\u7684\u8d26\u53f7\u9700\u8981\u5305\u542b\u5982\u4e0b\u8d44\u6e90\u7684\u6743\u9650\u3002\u4e14\u9700\u8981\u5f00\u901aACK\u670d\u52a1\uff0c\u5f00\u901a\u540e\u53ef\u4ee5\u5728ACK\u63a7\u5236\u53f0\u53f3\u4e0a\u89d2\u770b\u5230\uff1a \u5f00\u901a\u72b6\u6001\uff1aGPU \u6309\u91cf\u4ed8\u8d39\u5df2\u5f00\u901a, GPU \u5bb9\u91cf\u9884\u7559\u5df2\u5f00\u901a, CPU \u6309\u91cf\u4ed8\u8d39\u5df2\u5f00\u901a \u3002 \u6743\u9650\u7b56\u7565\u540d\u79f0 \u5907\u6ce8 AliyunECSFullAccess \u7ba1\u7406\u4e91\u670d\u52a1\u5668\u670d\u52a1\uff08ECS\uff09\u7684\u6743\u9650 AliyunVPCFullAccess \u7ba1\u7406\u4e13\u6709\u7f51\u7edc\uff08VPC\uff09\u7684\u6743\u9650 AliyunROSFullAccess \u7ba1\u7406\u8d44\u6e90\u7f16\u6392\u670d\u52a1\uff08ROS\uff09\u7684\u6743\u9650 AliyunCSFullAccess \u7ba1\u7406\u5bb9\u5668\u670d\u52a1\uff08CS\uff09\u7684\u6743\u9650 AliyunComputeNestUserFullAccess \u7ba1\u7406\u8ba1\u7b97\u5de2\u670d\u52a1\uff08ComputeNest\uff09\u7684\u7528\u6237\u4fa7\u6743\u9650 AliyunOSSFullAccess \u7ba1\u7406\u7f51\u7edc\u5bf9\u8c61\u5b58\u50a8\u670d\u52a1\uff08OSS\uff09\u7684\u6743\u9650 \u90e8\u7f72\u6d41\u7a0b \u5355\u51fb \u90e8\u7f72\u94fe\u63a5 \u3002\u6839\u636e\u754c\u9762\u63d0\u793a\u586b\u5199\u53c2\u6570\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u8be2\u4ef7\u660e\u7ec6\uff0c\u786e\u8ba4\u53c2\u6570\u540e\u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u3002 \u8fd9\u91cc\u4e5f\u53ef\u4ee5\u9009\u62e9\u5df2\u6709ACK\u96c6\u7fa4\uff0c\u5982\u679c\u9009\u62e9\u5df2\u6709\u96c6\u7fa4\uff0c\u8bf7\u786e\u4fdd\u96c6\u7fa4\u8282\u70b9\u8fd8\u6709GPU\u8d44\u6e90\uff0c\u5426\u5219\u4f1a\u8c03\u5ea6\u5931\u8d25\uff0c\u5982\u4e0b\u6240\u793a\uff1a \u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u540e\u53ef\u4ee5\u4e5f\u770b\u5230\u4ef7\u683c\u9884\u89c8\uff0c\u968f\u540e\u70b9\u51fb \u7acb\u5373\u90e8\u7f72 \uff0c\u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u3002 \u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u540e\u5c31\u53ef\u4ee5\u5f00\u59cb\u4f7f\u7528\u670d\u52a1\uff0c\u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u67e5\u770b\u5982\u4f55\u79c1\u7f51\u8bbf\u95ee\u6307\u5bfc\u3002\u5982\u679c\u9009\u62e9\u4e86 \u652f\u6301\u516c\u7f51\u8bbf\u95ee \uff0c\u5219\u80fd\u770b\u5230\u516c\u7f51\u8bbf\u95ee\u6307\u5bfc\u3002 \u4f7f\u7528\u8bf4\u660e \u79c1\u7f51API\u8bbf\u95ee \u5728\u548c\u670d\u52a1\u5668\u540c\u4e00VPC\u5185\u7684ECS\u4e2d\u8bbf\u95ee\u6982\u89c8\u9875\u7684 \u79c1\u7f51API\u5730\u5740 \u3002\u8bbf\u95ee\u793a\u4f8b\u5982\u4e0b\uff1a # \u79c1\u7f51\u6709\u8ba4\u8bc1\u8bf7\u6c42\uff0c\u6d41\u5f0f\u8bbf\u95ee\uff0c\u82e5\u60f3\u5173\u95ed\u6d41\u5f0f\u8bbf\u95ee\uff0c\u5220\u9664stream\u5373\u53ef\u3002 curl http://{$PrivateIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${API_KEY}\" \\ -d '{ \"model\": \"ds\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ], \"max_tokens\": 1024, \"temperature\": 0, \"top_p\": 0.9, \"seed\": 10, \"stream\": true }' \u516c\u7f51API\u8bbf\u95ee \u5982\u679c\u60f3\u901a\u8fc7\u516c\u7f51\u8bbf\u95eeAPI\u5730\u5740\uff0c\u90e8\u7f72\u65f6\u5982\u679c\u9009\u62e9\u4e86 \u652f\u6301\u516c\u7f51\u8bbf\u95ee \uff0c\u5219\u76f4\u63a5\u901a\u8fc7\u516c\u7f51IP\u8bbf\u95ee\u5373\u53ef\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a curl http://${PublicIp}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"ds\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ], \"max_tokens\": 1024, \"temperature\": 0, \"top_p\": 0.9, \"seed\": 10, \"stream\": true }' \u5982\u679c\u672a\u9009\u62e9 \u652f\u6301\u516c\u7f51\u8bbf\u95ee \uff0c\u5219\u9700\u8981\u624b\u52a8\u5728\u96c6\u7fa4\u4e2d\u521b\u5efa\u4e00\u4e2a LoadBalance \uff0c\u793a\u4f8b\u5982\u4e0b\uff08deepseek-r1\uff0c\u5982\u679c\u662fqwq-32b\uff0clabels.app\u9700\u8981\u6539\u4e3aqwq-32b)\uff1a apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type: \"internet\" service.beta.kubernetes.io/alibaba-cloud-loadbalancer-ip-version: ipv4 labels: app: deepseek-r1 name: svc-public namespace: llm-model spec: externalTrafficPolicy: Local ports: - name: serving port: 8000 protocol: TCP targetPort: 8000 selector: app: deepseek-r1 type: LoadBalancer \u624b\u52a8\u91cd\u65b0\u90e8\u7f72\u6a21\u578b \u5bf9\u4e8e\u4e0d\u66f4\u6362\u6a21\u578b\u3001\u4ec5\u6539\u53d8\u90e8\u7f72\u53c2\u6570\u7684\u60c5\u51b5\uff0c\u53ef\u4ee5\u53c2\u8003\u5982\u4e0b\u8bf4\u660e\u91cd\u65b0\u90e8\u7f72\u6a21\u578b\uff1a \u901a\u8fc7\u8df3\u677f\u673a\u4e0a\u6267\u884ckubectl apply\u547d\u4ee4\u6216\u8005\u76f4\u63a5\u5728\u63a7\u5236\u53f0\u624b\u52a8\u8f93\u5165\u6a21\u677f\u6765\u91cd\u65b0\u90e8\u7f72\u3002 \u8df3\u677f\u673a\u65b9\u5f0f \u8fdb\u5165\u8ba1\u7b97\u5de2\u63a7\u5236\u53f0\u670d\u52a1\u5b9e\u4f8b\u7684\u8d44\u6e90\u754c\u9762\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u7684ECS\u8df3\u677f\u673a\uff0c\u6267\u884c \u8fdc\u7a0b\u8fde\u63a5 \uff0c\u9009\u62e9\u514d\u5bc6\u767b\u5f55\u3002 \u8fdb\u5165\u8df3\u677f\u673a\u540e\u6267\u884c\u547d\u4ee4 bash su root # \u4fee\u6539\u90e8\u7f72\u53c2\u6570 vi /model.yaml # \u5982\u679c\u9700\u8981\u66f4\u6539\u6a21\u578b\u53c2\u6570\uff0c\u4fee\u6539\u4e86model.yaml\u540e\u76f4\u63a5\u6267\u884capply\u547d\u4ee4\u5373\u53ef kubectl apply -f /model.yaml \u63a7\u5236\u53f0\u65b9\u5f0f \u8fdb\u5165\u8ba1\u7b97\u5de2\u63a7\u5236\u53f0\uff0c\u70b9\u51fb \u670d\u52a1\u5b9e\u4f8b \uff0c\u70b9\u51fb \u8d44\u6e90 \uff0c\u627e\u5230\u5bf9\u5e94\u7684ACK\u5b9e\u4f8b\uff0c\u70b9\u51fb\u8fdb\u5165\u3002 \u8fdb\u5165ACK\u63a7\u5236\u53f0\u540e\u70b9\u51fb \u5de5\u4f5c\u8d1f\u8f7d \uff0c\u67e5\u770b \u65e0\u72b6\u6001 \uff0c\u4ee5Qwen3-8B\u4e3a\u4f8b\uff1a\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u7684Deployment\u3002 \u70b9\u51fb\u8be5Deployment\u540e\u8fdb\u5165\u8be6\u60c5\u9875\u9762\uff0c\u70b9\u51fb\u7f16\u8f91\u53ef\u4ee5\u4fee\u6539\u4e00\u4e9b\u57fa\u672c\u53c2\u6570\uff0c\u6216\u8005\u70b9\u51fb\u67e5\u770byaml\u4fee\u6539\u540e\u66f4\u65b0\u3002 \u5bf9\u4e8e\u66f4\u6362\u6a21\u578b\u7684\u60c5\u51b5\uff0c\u53ef\u4ee5\u53c2\u8003\u5982\u4e0b\u6587\u6863\uff1a ACS\u96c6\u7fa4\u5f62\u6001\u7684LLM\u5927\u6a21\u578b\u63a8\u7406\u955c\u50cf\u4f7f\u7528\u6307\u5bfc_PG1\u963f\u91cc\u4e91\u4ea7\u54c1-\u963f\u91cc\u4e91\u5e2e\u52a9\u4e2d\u5fc3 \u4f7f\u7528ACS GPU\u7b97\u529b\u6784\u5efaDeepSeek\u6ee1\u8840\u7248\u6a21\u578b\u63a8\u7406\u670d\u52a1_\u5bb9\u5668\u8ba1\u7b97\u670d\u52a1(ACS)-\u963f\u91cc\u4e91\u5e2e\u52a9\u4e2d\u5fc3 \u8fdb\u9636\u6559\u7a0b \u9664\u4e86\u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\u65f6\u53ef\u4ee5\u9009\u62e9 Fluid\u914d\u7f6e \uff0c\u4e5f\u53ef\u4ee5\u540e\u7eed\u81ea\u5b9a\u4e49\u914d\u7f6eFluid\u5b9e\u73b0\u6a21\u578b\u52a0\u901f Fluid \u662f\u4e00\u79cd\u57fa\u4e8e Kubernetes \u539f\u751f\u7684\u5206\u5e03\u5f0f\u6570\u636e\u96c6\u7f16\u6392\u548c\u52a0\u901f\u5f15\u64ce\uff0c\u65e8\u5728\u4f18\u5316\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\uff08\u5982AI\u63a8\u7406\u3001\u5927\u6a21\u578b\u8bad\u7ec3\u7b49\u573a\u666f\uff09\u7684\u6027\u80fd\u3002\u5982\u679c\u670d\u52a1\u9700\u8981\u5728\u5f39\u6027\u4f38\u7f29\u65f6\u5feb\u901f\u542f\u52a8\uff0c \u53ef\u4ee5\u8003\u8651\u90e8\u7f72Fluid\uff0c\u5177\u4f53\u53ef\u4ee5\u53c2\u8003\u6587\u6863\uff1a Fluid \u3002 \u7ecf\u6d4b\u8bd5\uff0c\u91c7\u7528Fluid\u7684\u52a0\u901f\uff0c\u6839\u636e\u7f13\u5b58\u5927\u5c0f\uff0c\u6a21\u578b\u52a0\u8f7d\u901f\u5ea6\u53ef\u4ee5\u7f29\u77ed\u81f350%\uff0c\u5728\u5e94\u5bf9\u4e00\u4e9b\u5f39\u6027\u4f38\u7f29\u7684\u573a\u666f\u4e0b\uff0c\u53ef\u4ee5\u5feb\u901f\u52a0\u8f7d\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002\u5982\u4e0b\u6240\u793a\uff0c\u53ef\u4ee5\u4ec5\u4fee\u6539\u5177\u4f53\u7684BucketName\u3001ModelName\u548c\u5177\u4f53\u7684JindoRuntime\u53c2\u6570\uff1a apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: llm-model namespace: llm-model spec: placement: Shared mounts: - mountPoint: oss://${BucketName}/llm-model options: fs.oss.endpoint: oss-${RegionId}-internal.aliyuncs.com name: models path: \"/\" encryptOptions: - name: fs.oss.accessKeyId valueFrom: secretKeyRef: name: oss-secret key: akId - name: fs.oss.accessKeySecret valueFrom: secretKeyRef: name: oss-secret key: akSecret --- apiVersion: data.fluid.io/v1alpha1 kind: JindoRuntime metadata: name: llm-model namespace: llm-model spec: networkmode: ContainerNetwork replicas: ${JindoRuntimeReplicas} # \u8bbe\u7f6e\u526f\u672c\u6570,\u6839\u636e\u5b9e\u9645\u7684\u6a21\u578b\u78c1\u76d8\u5360\u7528\u8fdb\u884c\u8bbe\u7f6e master: podMetadata: labels: alibabacloud.com/compute-class: performance alibabacloud.com/compute-qos: default worker: podMetadata: labels: alibabacloud.com/compute-class: performance alibabacloud.com/compute-qos: default annotations: kubernetes.io/resource-type: serverless resources: requests: cpu: 16 memory: 128Gi limits: cpu: 16 memory: 128Gi tieredstore: levels: - mediumtype: MEM path: /dev/shm volumeType: emptyDir quota: 128Gi high: \"0.99\" low: \"0.95\" --- apiVersion: data.fluid.io/v1alpha1 kind: DataLoad metadata: name: llm-model namespace: llm-model spec: dataset: name: llm-model namespace: llm-model loadMetadata: true Benchmark \u672c\u670d\u52a1\u57fa\u91c7\u7528vllm\u81ea\u5e26\u7684benchmark\u8fdb\u884c\u6d4b\u8bd5\uff0c\u91c7\u7528\u7684\u538b\u6d4b\u6570\u636e\u96c6\uff1a https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split/files \uff0c \u6574\u4f53\u538b\u6d4b\u6d41\u7a0b\uff1a \u521b\u5efa\u4e00\u4e2aDeployment\uff0c\u4f7f\u7528vllm-benchmark\u955c\u50cf\u3002\u5728\u5bb9\u5668\u4e2d\u6267\u884c\u6570\u636e\u96c6\u4e0b\u8f7d\u3001\u538b\u6d4b\u64cd\u4f5c \u4f7f\u7528\u4e0b\u9762\u7684yaml\u521b\u5efaDeployment\u524d\u9700\u8981\u66ff\u6362\u90e8\u5206\u53c2\u6570 \u66ff\u6362\u53c2\u6570 \u53c2\u6570\u542b\u4e49 \u53c2\u6570\u503c\u793a\u4f8b/\u8bf4\u660e $POD_IP \u8fd0\u884c deepseek-r1 \u7684 Pod IP kubectl get pod -n llm-model -l app=$(kubectl get deployment -n llm-model -l app -o jsonpath='{.items[0].spec.template.metadata.labels.app}') -o jsonpath='{.items[0].status.podIP}' $API_KEY \u670d\u52a1\u8ba4\u8bc1\u5bc6\u94a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u4e2d\u83b7\u53d6\uff08\u5f62\u5982 sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \uff09 $MODEL_PATH \u6a21\u578b\u5b58\u50a8\u8def\u5f84 QwQ-32b: /llm-model/Qwen/QwQ-32B Qwen3-32b: /llm-model/Qwen/Qwen3-32B Qwen3-235b-A22b: /llm-model/Qwen/Qwen3-235B-A22B DeepSeek-R1_671b: /llm-model/deepseek-ai/DeepSeek-R1 DeepSeek-R1_32b: /llm-model/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1_70b: /llm-model/deepseek-ai/DeepSeek-R1-Distill-Llama-70B $SERVED_MODEL_NAME \u670d\u52a1\u90e8\u7f72\u7684\u6a21\u578b\u540d\u79f0 QwQ-32b: Qwen/QwQ-32B Qwen3-32b: Qwen/Qwen3-32B Qwen3-235b-A22b: Qwen/Qwen3-235B-A22B DeepSeek-R1_671b: deepseek-ai/DeepSeek-R1 DeepSeek-R1_32b: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1_70b: deepseek-ai/DeepSeek-R1-Distill-Llama-70B apiVersion: apps/v1 kind: Deployment metadata: name: vllm-benchmark namespace: llm-model labels: app: vllm-benchmark spec: replicas: 1 selector: matchLabels: app: vllm-benchmark template: metadata: labels: app: vllm-benchmark spec: volumes: - name: llm-model persistentVolumeClaim: claimName: llm-model containers: - name: vllm-benchmark image: kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/vllm-benchmark:v1 command: - \"sh\" - \"-c\" - | # \u5b89\u88c5\u4f9d\u8d56 yum install -y epel-release && \\ yum install -y git git-lfs && \\ git lfs install && # \u4e0b\u8f7d\u6570\u636e\u96c6 git clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git /root/ShareGPT_V3_unfiltered_cleaned_split # \u6267\u884c\u57fa\u51c6\u6d4b\u8bd5 export OPENAI_API_KEY=$API_KEY python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model $MODEL_PATH \\ --served-model-name $SERVED_MODEL_NAME \\ --trust-remote-code \\ --dataset-name sharegpt \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \\ --sonnet-input-len 1024 \\ --sonnet-output-len 6 \\ --sonnet-prefix-len 50 \\ --num-prompts 200 \\ --request-rate 1 \\ --host $POD_IP \\ --port 8000 \\ --endpoint /v1/completions \\ --save-result # \u4fdd\u6301\u5bb9\u5668\u8fd0\u884c sleep inf volumeMounts: - mountPath: /llm-model name: llm-model \u76f4\u63a5\u5728ACK\u63a7\u5236\u53f0\u67e5\u770b\u5bb9\u5668\u65e5\u5fd7\u6216\u8005\u8fdb\u5165\u5bb9\u5668\u67e5\u770b\u5bb9\u5668\u65e5\u5fd7 \u6d4b\u8bd5\u7ed3\u679c\u793a\u4f8b\uff1a plaintext ============ Serving Benchmark Result ============ Successful requests: 200 Benchmark duration (s): 272.15 Total input tokens: 43390 Total generated tokens: 39980 Request throughput (req/s): 0.73 Output token throughput (tok/s): 146.91 Total Token throughput (tok/s): 306.34 ---------------Time to First Token---------------- Mean TTFT (ms): 246.46 Median TTFT (ms): 244.58 P99 TTFT (ms): 342.11 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 130.30 Median TPOT (ms): 130.12 P99 TPOT (ms): 139.09 ---------------Inter-token Latency---------------- Mean ITL (ms): 129.89 Median ITL (ms): 125.40 P99 ITL (ms): 173.20 ==================================================","title":"\u57fa\u4e8eACK\u96c6\u7fa4\u7684\u5927\u6a21\u578b\u90e8\u7f72\u6587\u6863"},{"location":"ack-ch/index-ack/#ack","text":"","title":"\u57fa\u4e8eACK\u96c6\u7fa4\u7684\u5927\u6a21\u578b\u90e8\u7f72\u6587\u6863"},{"location":"ack-ch/index-ack/#_1","text":"\u672c\u65b9\u6848\u901a\u8fc7\u963f\u91cc\u4e91\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u73b0\u5f00\u7bb1\u5373\u7528\u7684\u5927\u6a21\u578b\u63a8\u7406\u670d\u52a1\u90e8\u7f72\uff0c\u652f\u6301\u4ee5\u4e0b\u573a\u666f\uff1a - \u65b0\u5efaACK\u96c6\u7fa4 \uff1a\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u76f4\u63a5\u521b\u5efa\u4e00\u4e2aACK\u96c6\u7fa4\uff0c\u8ba1\u7b97\u5de2\u4f1a\u5728\u7528\u6237\u8d26\u53f7\u4e0b\u4e00\u952e\u521b\u5efaACK\u96c6\u7fa4\u548cOSS Bucket\uff0c\u5b8c\u6210\u4e86\u6a21\u578b\u4e0a\u4f20\u548cBucket\u7684\u6302\u8f7d\u540e\u4f1a\u81ea\u52a8\u90e8\u7f72\u6a21\u578b\uff0c\u6700\u540e\u4f1a\u81ea\u52a8\u521b\u5efa\u8d1f\u8f7d\u5747\u8861\u5b9e\u73b0\u5185\u3001\u516c\u7f51\u7684\u8bbf\u95ee\u3002 - \u9009\u62e9\u5df2\u6709ACK\u96c6\u7fa4 \uff1a\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528\u5df2\u6709\u7684ACK\u96c6\u7fa4\uff0c\u8ba1\u7b97\u5de2\u4f1a\u5728\u7528\u6237\u8d26\u53f7\u4e0b\u521b\u5efaOSS Bucket\uff08\u4e5f\u53ef\u4ee5\u9009\u62e9\u5df2\u6709\u7684Bucket\uff09\uff0c\u5b8c\u6210\u4e86\u6a21\u578b\u4e0a\u4f20\u548cBucket\u7684\u6302\u8f7d\u540e\u4f1a\u81ea\u52a8\u90e8\u7f72\u6a21\u578b\uff0c\u6700\u540e\u4f1a\u81ea\u52a8\u521b\u5efa\u8d1f\u8f7d\u5747\u8861\u5b9e\u73b0\u5185\u3001\u516c\u7f51\u7684\u8bbf\u95ee\u3002 \u672c\u65b9\u6848\u57fa\u4e8e\u4ee5\u4e0b\u6838\u5fc3\u7ec4\u4ef6\uff1a vLLM \uff1a\u63d0\u4f9b\u9ad8\u6027\u80fd\u5e76\u884c\u63a8\u7406\u80fd\u529b\uff0c\u652f\u6301\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u7684LLM\u63a8\u7406\uff08\u652f\u6301Qwen\u3001DeepSeek\u5168\u7cfb\u5217\u6a21\u578b\uff09 SGLang : SGLang \u662f\u4e00\u4e2a\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u670d\u52a1\u6846\u67b6\u3002 ACK\u96c6\u7fa4 \uff1a\u63d0\u4f9b\u5168\u6258\u7ba1\u7684Kubernetes\u73af\u5883 A10/L20/GU8TF GPU\u52a0\u901f \uff1a\u652f\u6301\u591a\u79cd\u7b97\u529b\u89c4\u683c\uff0c\u6ee1\u8db3\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u7684\u63a8\u7406\u9700\u6c42 \u90e8\u7f72\u540e\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u79c1\u6709/\u516c\u7f51API\u8c03\u7528\u6a21\u578b\u670d\u52a1\uff0c\u8d44\u6e90\u5229\u7528\u7387\u63d0\u5347\u6570\u500d\uff0c\u5f00\u53d1\u8005\u65e0\u9700\u5173\u6ce8\u5e95\u5c42\u5bb9\u5668\u7f16\u6392\u4e0e\u8d44\u6e90\u8c03\u5ea6\uff0c\u4ec5\u9700\u5728\u8ba1\u7b97\u5de2\u63a7\u5236\u53f0\u9875\u9762\u9009\u62e9\u6a21\u578b\u5373\u53ef\u5b8c\u6210\u4e00\u952e\u90e8\u7f72\u3002","title":"\u90e8\u7f72\u8bf4\u660e"},{"location":"ack-ch/index-ack/#_2","text":"","title":"\u6574\u4f53\u67b6\u6784"},{"location":"ack-ch/index-ack/#_3","text":"\u8d44\u6e90\u7c7b\u578b \u8ba1\u8d39\u6a21\u5f0f \u5173\u952e\u914d\u7f6e\u8bf4\u660e ACK\u96c6\u7fa4 \u6309\u91cf\u4ed8\u8d39 \u6839\u636e\u6240\u9009GPU\u7c7b\u578b\u548c\u6570\u91cf\u8ba1\u8d39 ECS\u8df3\u677f\u673a \u6309\u91cf\u4ed8\u8d39 ecs.u1-c1m2.xlarge\uff084C8G\uff09\uff0c\u7528\u4e8e\u96c6\u7fa4\u7ba1\u7406\uff0c\u90e8\u7f72\u5b8c\u6210\u540e\u53ef\u5b89\u5168\u91ca\u653e OSS\u5b58\u50a8 \u6309\u91cf\u4ed8\u8d39 \u5b58\u50a8\u6a21\u578b\u6587\u4ef6\uff0c\u5efa\u8bae\u9009\u62e9\u4e0e\u96c6\u7fa4\u540c\u5730\u57df\u7684\u5b58\u50a8\u7c7b\u578b NAT\u7f51\u5173 \u6309\u91cf\u4ed8\u8d39 \u5f53\u5f00\u542f\u516c\u7f51\u8bbf\u95ee\u65f6\u81ea\u52a8\u521b\u5efa\uff0c\u6309\u4f7f\u7528\u65f6\u957f\u548c\u5e26\u5bbd\u8ba1\u8d39","title":"\u8ba1\u8d39\u8bf4\u660e"},{"location":"ack-ch/index-ack/#ram","text":"\u90e8\u7f72\u5b9e\u4f8b\u9700\u8981\u5bf9\u90e8\u5206\u963f\u91cc\u4e91\u8d44\u6e90\u8fdb\u884c\u8bbf\u95ee\u548c\u521b\u5efa\u64cd\u4f5c\u3002\u56e0\u6b64\u60a8\u7684\u8d26\u53f7\u9700\u8981\u5305\u542b\u5982\u4e0b\u8d44\u6e90\u7684\u6743\u9650\u3002\u4e14\u9700\u8981\u5f00\u901aACK\u670d\u52a1\uff0c\u5f00\u901a\u540e\u53ef\u4ee5\u5728ACK\u63a7\u5236\u53f0\u53f3\u4e0a\u89d2\u770b\u5230\uff1a \u5f00\u901a\u72b6\u6001\uff1aGPU \u6309\u91cf\u4ed8\u8d39\u5df2\u5f00\u901a, GPU \u5bb9\u91cf\u9884\u7559\u5df2\u5f00\u901a, CPU \u6309\u91cf\u4ed8\u8d39\u5df2\u5f00\u901a \u3002 \u6743\u9650\u7b56\u7565\u540d\u79f0 \u5907\u6ce8 AliyunECSFullAccess \u7ba1\u7406\u4e91\u670d\u52a1\u5668\u670d\u52a1\uff08ECS\uff09\u7684\u6743\u9650 AliyunVPCFullAccess \u7ba1\u7406\u4e13\u6709\u7f51\u7edc\uff08VPC\uff09\u7684\u6743\u9650 AliyunROSFullAccess \u7ba1\u7406\u8d44\u6e90\u7f16\u6392\u670d\u52a1\uff08ROS\uff09\u7684\u6743\u9650 AliyunCSFullAccess \u7ba1\u7406\u5bb9\u5668\u670d\u52a1\uff08CS\uff09\u7684\u6743\u9650 AliyunComputeNestUserFullAccess \u7ba1\u7406\u8ba1\u7b97\u5de2\u670d\u52a1\uff08ComputeNest\uff09\u7684\u7528\u6237\u4fa7\u6743\u9650 AliyunOSSFullAccess \u7ba1\u7406\u7f51\u7edc\u5bf9\u8c61\u5b58\u50a8\u670d\u52a1\uff08OSS\uff09\u7684\u6743\u9650","title":"RAM\u8d26\u53f7\u6240\u9700\u6743\u9650"},{"location":"ack-ch/index-ack/#_4","text":"\u5355\u51fb \u90e8\u7f72\u94fe\u63a5 \u3002\u6839\u636e\u754c\u9762\u63d0\u793a\u586b\u5199\u53c2\u6570\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u8be2\u4ef7\u660e\u7ec6\uff0c\u786e\u8ba4\u53c2\u6570\u540e\u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u3002 \u8fd9\u91cc\u4e5f\u53ef\u4ee5\u9009\u62e9\u5df2\u6709ACK\u96c6\u7fa4\uff0c\u5982\u679c\u9009\u62e9\u5df2\u6709\u96c6\u7fa4\uff0c\u8bf7\u786e\u4fdd\u96c6\u7fa4\u8282\u70b9\u8fd8\u6709GPU\u8d44\u6e90\uff0c\u5426\u5219\u4f1a\u8c03\u5ea6\u5931\u8d25\uff0c\u5982\u4e0b\u6240\u793a\uff1a \u70b9\u51fb \u4e0b\u4e00\u6b65\uff1a\u786e\u8ba4\u8ba2\u5355 \u540e\u53ef\u4ee5\u4e5f\u770b\u5230\u4ef7\u683c\u9884\u89c8\uff0c\u968f\u540e\u70b9\u51fb \u7acb\u5373\u90e8\u7f72 \uff0c\u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u3002 \u7b49\u5f85\u90e8\u7f72\u5b8c\u6210\u540e\u5c31\u53ef\u4ee5\u5f00\u59cb\u4f7f\u7528\u670d\u52a1\uff0c\u8fdb\u5165\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u67e5\u770b\u5982\u4f55\u79c1\u7f51\u8bbf\u95ee\u6307\u5bfc\u3002\u5982\u679c\u9009\u62e9\u4e86 \u652f\u6301\u516c\u7f51\u8bbf\u95ee \uff0c\u5219\u80fd\u770b\u5230\u516c\u7f51\u8bbf\u95ee\u6307\u5bfc\u3002","title":"\u90e8\u7f72\u6d41\u7a0b"},{"location":"ack-ch/index-ack/#_5","text":"","title":"\u4f7f\u7528\u8bf4\u660e"},{"location":"ack-ch/index-ack/#api","text":"\u5728\u548c\u670d\u52a1\u5668\u540c\u4e00VPC\u5185\u7684ECS\u4e2d\u8bbf\u95ee\u6982\u89c8\u9875\u7684 \u79c1\u7f51API\u5730\u5740 \u3002\u8bbf\u95ee\u793a\u4f8b\u5982\u4e0b\uff1a # \u79c1\u7f51\u6709\u8ba4\u8bc1\u8bf7\u6c42\uff0c\u6d41\u5f0f\u8bbf\u95ee\uff0c\u82e5\u60f3\u5173\u95ed\u6d41\u5f0f\u8bbf\u95ee\uff0c\u5220\u9664stream\u5373\u53ef\u3002 curl http://{$PrivateIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${API_KEY}\" \\ -d '{ \"model\": \"ds\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ], \"max_tokens\": 1024, \"temperature\": 0, \"top_p\": 0.9, \"seed\": 10, \"stream\": true }'","title":"\u79c1\u7f51API\u8bbf\u95ee"},{"location":"ack-ch/index-ack/#api_1","text":"\u5982\u679c\u60f3\u901a\u8fc7\u516c\u7f51\u8bbf\u95eeAPI\u5730\u5740\uff0c\u90e8\u7f72\u65f6\u5982\u679c\u9009\u62e9\u4e86 \u652f\u6301\u516c\u7f51\u8bbf\u95ee \uff0c\u5219\u76f4\u63a5\u901a\u8fc7\u516c\u7f51IP\u8bbf\u95ee\u5373\u53ef\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a curl http://${PublicIp}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"ds\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ], \"max_tokens\": 1024, \"temperature\": 0, \"top_p\": 0.9, \"seed\": 10, \"stream\": true }' \u5982\u679c\u672a\u9009\u62e9 \u652f\u6301\u516c\u7f51\u8bbf\u95ee \uff0c\u5219\u9700\u8981\u624b\u52a8\u5728\u96c6\u7fa4\u4e2d\u521b\u5efa\u4e00\u4e2a LoadBalance \uff0c\u793a\u4f8b\u5982\u4e0b\uff08deepseek-r1\uff0c\u5982\u679c\u662fqwq-32b\uff0clabels.app\u9700\u8981\u6539\u4e3aqwq-32b)\uff1a apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type: \"internet\" service.beta.kubernetes.io/alibaba-cloud-loadbalancer-ip-version: ipv4 labels: app: deepseek-r1 name: svc-public namespace: llm-model spec: externalTrafficPolicy: Local ports: - name: serving port: 8000 protocol: TCP targetPort: 8000 selector: app: deepseek-r1 type: LoadBalancer","title":"\u516c\u7f51API\u8bbf\u95ee"},{"location":"ack-ch/index-ack/#_6","text":"\u5bf9\u4e8e\u4e0d\u66f4\u6362\u6a21\u578b\u3001\u4ec5\u6539\u53d8\u90e8\u7f72\u53c2\u6570\u7684\u60c5\u51b5\uff0c\u53ef\u4ee5\u53c2\u8003\u5982\u4e0b\u8bf4\u660e\u91cd\u65b0\u90e8\u7f72\u6a21\u578b\uff1a \u901a\u8fc7\u8df3\u677f\u673a\u4e0a\u6267\u884ckubectl apply\u547d\u4ee4\u6216\u8005\u76f4\u63a5\u5728\u63a7\u5236\u53f0\u624b\u52a8\u8f93\u5165\u6a21\u677f\u6765\u91cd\u65b0\u90e8\u7f72\u3002 \u8df3\u677f\u673a\u65b9\u5f0f \u8fdb\u5165\u8ba1\u7b97\u5de2\u63a7\u5236\u53f0\u670d\u52a1\u5b9e\u4f8b\u7684\u8d44\u6e90\u754c\u9762\uff0c\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u7684ECS\u8df3\u677f\u673a\uff0c\u6267\u884c \u8fdc\u7a0b\u8fde\u63a5 \uff0c\u9009\u62e9\u514d\u5bc6\u767b\u5f55\u3002 \u8fdb\u5165\u8df3\u677f\u673a\u540e\u6267\u884c\u547d\u4ee4 bash su root # \u4fee\u6539\u90e8\u7f72\u53c2\u6570 vi /model.yaml # \u5982\u679c\u9700\u8981\u66f4\u6539\u6a21\u578b\u53c2\u6570\uff0c\u4fee\u6539\u4e86model.yaml\u540e\u76f4\u63a5\u6267\u884capply\u547d\u4ee4\u5373\u53ef kubectl apply -f /model.yaml \u63a7\u5236\u53f0\u65b9\u5f0f \u8fdb\u5165\u8ba1\u7b97\u5de2\u63a7\u5236\u53f0\uff0c\u70b9\u51fb \u670d\u52a1\u5b9e\u4f8b \uff0c\u70b9\u51fb \u8d44\u6e90 \uff0c\u627e\u5230\u5bf9\u5e94\u7684ACK\u5b9e\u4f8b\uff0c\u70b9\u51fb\u8fdb\u5165\u3002 \u8fdb\u5165ACK\u63a7\u5236\u53f0\u540e\u70b9\u51fb \u5de5\u4f5c\u8d1f\u8f7d \uff0c\u67e5\u770b \u65e0\u72b6\u6001 \uff0c\u4ee5Qwen3-8B\u4e3a\u4f8b\uff1a\u53ef\u4ee5\u770b\u5230\u5bf9\u5e94\u7684Deployment\u3002 \u70b9\u51fb\u8be5Deployment\u540e\u8fdb\u5165\u8be6\u60c5\u9875\u9762\uff0c\u70b9\u51fb\u7f16\u8f91\u53ef\u4ee5\u4fee\u6539\u4e00\u4e9b\u57fa\u672c\u53c2\u6570\uff0c\u6216\u8005\u70b9\u51fb\u67e5\u770byaml\u4fee\u6539\u540e\u66f4\u65b0\u3002 \u5bf9\u4e8e\u66f4\u6362\u6a21\u578b\u7684\u60c5\u51b5\uff0c\u53ef\u4ee5\u53c2\u8003\u5982\u4e0b\u6587\u6863\uff1a ACS\u96c6\u7fa4\u5f62\u6001\u7684LLM\u5927\u6a21\u578b\u63a8\u7406\u955c\u50cf\u4f7f\u7528\u6307\u5bfc_PG1\u963f\u91cc\u4e91\u4ea7\u54c1-\u963f\u91cc\u4e91\u5e2e\u52a9\u4e2d\u5fc3 \u4f7f\u7528ACS GPU\u7b97\u529b\u6784\u5efaDeepSeek\u6ee1\u8840\u7248\u6a21\u578b\u63a8\u7406\u670d\u52a1_\u5bb9\u5668\u8ba1\u7b97\u670d\u52a1(ACS)-\u963f\u91cc\u4e91\u5e2e\u52a9\u4e2d\u5fc3","title":"\u624b\u52a8\u91cd\u65b0\u90e8\u7f72\u6a21\u578b"},{"location":"ack-ch/index-ack/#_7","text":"\u9664\u4e86\u90e8\u7f72\u670d\u52a1\u5b9e\u4f8b\u65f6\u53ef\u4ee5\u9009\u62e9 Fluid\u914d\u7f6e \uff0c\u4e5f\u53ef\u4ee5\u540e\u7eed\u81ea\u5b9a\u4e49\u914d\u7f6eFluid\u5b9e\u73b0\u6a21\u578b\u52a0\u901f Fluid \u662f\u4e00\u79cd\u57fa\u4e8e Kubernetes \u539f\u751f\u7684\u5206\u5e03\u5f0f\u6570\u636e\u96c6\u7f16\u6392\u548c\u52a0\u901f\u5f15\u64ce\uff0c\u65e8\u5728\u4f18\u5316\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\uff08\u5982AI\u63a8\u7406\u3001\u5927\u6a21\u578b\u8bad\u7ec3\u7b49\u573a\u666f\uff09\u7684\u6027\u80fd\u3002\u5982\u679c\u670d\u52a1\u9700\u8981\u5728\u5f39\u6027\u4f38\u7f29\u65f6\u5feb\u901f\u542f\u52a8\uff0c \u53ef\u4ee5\u8003\u8651\u90e8\u7f72Fluid\uff0c\u5177\u4f53\u53ef\u4ee5\u53c2\u8003\u6587\u6863\uff1a Fluid \u3002 \u7ecf\u6d4b\u8bd5\uff0c\u91c7\u7528Fluid\u7684\u52a0\u901f\uff0c\u6839\u636e\u7f13\u5b58\u5927\u5c0f\uff0c\u6a21\u578b\u52a0\u8f7d\u901f\u5ea6\u53ef\u4ee5\u7f29\u77ed\u81f350%\uff0c\u5728\u5e94\u5bf9\u4e00\u4e9b\u5f39\u6027\u4f38\u7f29\u7684\u573a\u666f\u4e0b\uff0c\u53ef\u4ee5\u5feb\u901f\u52a0\u8f7d\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002\u5982\u4e0b\u6240\u793a\uff0c\u53ef\u4ee5\u4ec5\u4fee\u6539\u5177\u4f53\u7684BucketName\u3001ModelName\u548c\u5177\u4f53\u7684JindoRuntime\u53c2\u6570\uff1a apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: llm-model namespace: llm-model spec: placement: Shared mounts: - mountPoint: oss://${BucketName}/llm-model options: fs.oss.endpoint: oss-${RegionId}-internal.aliyuncs.com name: models path: \"/\" encryptOptions: - name: fs.oss.accessKeyId valueFrom: secretKeyRef: name: oss-secret key: akId - name: fs.oss.accessKeySecret valueFrom: secretKeyRef: name: oss-secret key: akSecret --- apiVersion: data.fluid.io/v1alpha1 kind: JindoRuntime metadata: name: llm-model namespace: llm-model spec: networkmode: ContainerNetwork replicas: ${JindoRuntimeReplicas} # \u8bbe\u7f6e\u526f\u672c\u6570,\u6839\u636e\u5b9e\u9645\u7684\u6a21\u578b\u78c1\u76d8\u5360\u7528\u8fdb\u884c\u8bbe\u7f6e master: podMetadata: labels: alibabacloud.com/compute-class: performance alibabacloud.com/compute-qos: default worker: podMetadata: labels: alibabacloud.com/compute-class: performance alibabacloud.com/compute-qos: default annotations: kubernetes.io/resource-type: serverless resources: requests: cpu: 16 memory: 128Gi limits: cpu: 16 memory: 128Gi tieredstore: levels: - mediumtype: MEM path: /dev/shm volumeType: emptyDir quota: 128Gi high: \"0.99\" low: \"0.95\" --- apiVersion: data.fluid.io/v1alpha1 kind: DataLoad metadata: name: llm-model namespace: llm-model spec: dataset: name: llm-model namespace: llm-model loadMetadata: true","title":"\u8fdb\u9636\u6559\u7a0b"},{"location":"ack-ch/index-ack/#benchmark","text":"\u672c\u670d\u52a1\u57fa\u91c7\u7528vllm\u81ea\u5e26\u7684benchmark\u8fdb\u884c\u6d4b\u8bd5\uff0c\u91c7\u7528\u7684\u538b\u6d4b\u6570\u636e\u96c6\uff1a https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split/files \uff0c \u6574\u4f53\u538b\u6d4b\u6d41\u7a0b\uff1a \u521b\u5efa\u4e00\u4e2aDeployment\uff0c\u4f7f\u7528vllm-benchmark\u955c\u50cf\u3002\u5728\u5bb9\u5668\u4e2d\u6267\u884c\u6570\u636e\u96c6\u4e0b\u8f7d\u3001\u538b\u6d4b\u64cd\u4f5c \u4f7f\u7528\u4e0b\u9762\u7684yaml\u521b\u5efaDeployment\u524d\u9700\u8981\u66ff\u6362\u90e8\u5206\u53c2\u6570 \u66ff\u6362\u53c2\u6570 \u53c2\u6570\u542b\u4e49 \u53c2\u6570\u503c\u793a\u4f8b/\u8bf4\u660e $POD_IP \u8fd0\u884c deepseek-r1 \u7684 Pod IP kubectl get pod -n llm-model -l app=$(kubectl get deployment -n llm-model -l app -o jsonpath='{.items[0].spec.template.metadata.labels.app}') -o jsonpath='{.items[0].status.podIP}' $API_KEY \u670d\u52a1\u8ba4\u8bc1\u5bc6\u94a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u4e2d\u83b7\u53d6\uff08\u5f62\u5982 sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \uff09 $MODEL_PATH \u6a21\u578b\u5b58\u50a8\u8def\u5f84 QwQ-32b: /llm-model/Qwen/QwQ-32B Qwen3-32b: /llm-model/Qwen/Qwen3-32B Qwen3-235b-A22b: /llm-model/Qwen/Qwen3-235B-A22B DeepSeek-R1_671b: /llm-model/deepseek-ai/DeepSeek-R1 DeepSeek-R1_32b: /llm-model/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1_70b: /llm-model/deepseek-ai/DeepSeek-R1-Distill-Llama-70B $SERVED_MODEL_NAME \u670d\u52a1\u90e8\u7f72\u7684\u6a21\u578b\u540d\u79f0 QwQ-32b: Qwen/QwQ-32B Qwen3-32b: Qwen/Qwen3-32B Qwen3-235b-A22b: Qwen/Qwen3-235B-A22B DeepSeek-R1_671b: deepseek-ai/DeepSeek-R1 DeepSeek-R1_32b: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1_70b: deepseek-ai/DeepSeek-R1-Distill-Llama-70B apiVersion: apps/v1 kind: Deployment metadata: name: vllm-benchmark namespace: llm-model labels: app: vllm-benchmark spec: replicas: 1 selector: matchLabels: app: vllm-benchmark template: metadata: labels: app: vllm-benchmark spec: volumes: - name: llm-model persistentVolumeClaim: claimName: llm-model containers: - name: vllm-benchmark image: kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/vllm-benchmark:v1 command: - \"sh\" - \"-c\" - | # \u5b89\u88c5\u4f9d\u8d56 yum install -y epel-release && \\ yum install -y git git-lfs && \\ git lfs install && # \u4e0b\u8f7d\u6570\u636e\u96c6 git clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git /root/ShareGPT_V3_unfiltered_cleaned_split # \u6267\u884c\u57fa\u51c6\u6d4b\u8bd5 export OPENAI_API_KEY=$API_KEY python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model $MODEL_PATH \\ --served-model-name $SERVED_MODEL_NAME \\ --trust-remote-code \\ --dataset-name sharegpt \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \\ --sonnet-input-len 1024 \\ --sonnet-output-len 6 \\ --sonnet-prefix-len 50 \\ --num-prompts 200 \\ --request-rate 1 \\ --host $POD_IP \\ --port 8000 \\ --endpoint /v1/completions \\ --save-result # \u4fdd\u6301\u5bb9\u5668\u8fd0\u884c sleep inf volumeMounts: - mountPath: /llm-model name: llm-model \u76f4\u63a5\u5728ACK\u63a7\u5236\u53f0\u67e5\u770b\u5bb9\u5668\u65e5\u5fd7\u6216\u8005\u8fdb\u5165\u5bb9\u5668\u67e5\u770b\u5bb9\u5668\u65e5\u5fd7 \u6d4b\u8bd5\u7ed3\u679c\u793a\u4f8b\uff1a plaintext ============ Serving Benchmark Result ============ Successful requests: 200 Benchmark duration (s): 272.15 Total input tokens: 43390 Total generated tokens: 39980 Request throughput (req/s): 0.73 Output token throughput (tok/s): 146.91 Total Token throughput (tok/s): 306.34 ---------------Time to First Token---------------- Mean TTFT (ms): 246.46 Median TTFT (ms): 244.58 P99 TTFT (ms): 342.11 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 130.30 Median TPOT (ms): 130.12 P99 TPOT (ms): 139.09 ---------------Inter-token Latency---------------- Mean ITL (ms): 129.89 Median ITL (ms): 125.40 P99 ITL (ms): 173.20 ==================================================","title":"Benchmark"},{"location":"en/index-acs-en/","text":"vllm Large Model Deployment Guide on ACK Cluster Deployment Overview This solution provides an out-of-the-box deployment of high-performance large model inference services using Alibaba Cloud ComputeNest. It is based on the following core components: VLLM : Provides high-performance parallel inference capabilities, supporting low-latency and high-throughput LLM inference (e.g., Qwen, DeepSeek, etc.). ACK Cluster : A managed Kubernetes environment supporting Serverless workloads. After deployment, users can invoke model services via private/public APIs. Resource utilization is improved by several times, and developers need not manage underlying container orchestration or resource scheduling\u2014only selecting the model on the ComputeNest console is required for one-click deployment. The service supports various models and GPU types during deployment, including: QwQ32B DeepSeek-R1-Distill-Qwen-32B , GPU: P16EN DeepSeek-R1-Distill-Llama-70B , GPU: P16EN Deepseek Full-Blooded Version (671B, fp8) , GPU: GU8TF Deepseek Full-Blooded Version (671B, fp8) , GPU: P16EN Architecture Overview Cost Explanation The costs for this service on Alibaba Cloud primarily include: ACK Cluster Fees Jump Server (ECS) Fees Notes: This ECS instance is used to deploy and manage the K8s cluster. The /root directory contains the K8s YAML resource files used for deployment. Parameters can be modified and re-deployed directly afterward. The ECS can be released after deployment if no longer needed. OSS Fees Billing Method : Pay-as-you-go (hourly) or subscription (prepaid). Estimated costs are visible in real-time during instance creation. Required RAM Account Permissions The account deploying the instance requires permissions to access and manage Alibaba Cloud resources. The following policies must be included: Permission Policy Name Notes AliyunECSFullAccess Permissions to manage ECS (Elastic Compute Service) AliyunVPCFullAccess Permissions to manage VPC (Virtual Private Cloud) AliyunROSFullAccess Permissions to manage ROS (Resource Orchestration Service) AliyunCSFullAccess Permissions to manage ACK (Container Service) AliyunComputeNestUserFullAccess Permissions to manage ComputeNest (user-side) AliyunOSSFullAccess Permissions to manage OSS (Object Storage Service) Important : Contact PDSA to add GPU to your whitelist before deployment. Deployment Steps Click the * Deployment Link . Follow the interface prompts to fill in parameters and view cost estimates. Confirm parameters and click Next: Confirm Order *. Click Next: Confirm Order to preview costs. Then click Deploy Now and wait for completion. After deployment, access the service. Navigate to the instance details to view private network access instructions. If \"Support Public Network Access\" was selected, public access instructions will also be available. Usage Guide Private Network API Access Access the Private API address from an ECS instance within the same VPC. Example: shell # Private API request with authentication and streaming (remove \"stream\" to disable streaming) curl http://$PrivateIP:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY\" \\ -d '{ \"model\": \"ds\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to your daughter from the future (2035), encouraging her to study science and technology to become a leader in this field. She is currently in 3rd grade.\" } ], \"max_tokens\": 1024, \"temperature\": 0, \"top_p\": 0.9, \"seed\": 10, \"stream\": true }' Public Network API Access If \"Support Public Network Access\" was selected during deployment, use the public IP directly: curl http://$PublicIp:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"ds\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to your daughter from the future (2035), encouraging her to study science and technology to become a leader in this field. She is currently in 3rd grade.\" } ], \"max_tokens\": 1024, \"temperature\": 0, \"top_p\": 0.9, \"seed\": 10, \"stream\": true }' If public access was not enabled, manually create a LoadBalancer in the cluster. Example (for DeepSeek-R1; adjust labels.app for QwQ-32B): apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type: \"internet\" service.beta.kubernetes.io/alibaba-cloud-loadbalancer-ip-version: ipv4 labels: app: deepseek-r1 name: svc-public namespace: llm-model spec: externalTrafficPolicy: Local ports: - name: serving port: 8000 protocol: TCP targetPort: 8000 selector: app: deepseek-r1 type: LoadBalancer Re-deploying Models Models can be re-deployed using kubectl apply on the jump server or by manually updating templates in the console. Using the Jump Server In the ComputeNest console's instance resources page, locate the ECS jump server and connect via **Remote Connection ** (password-free login). Execute commands on the jump server: ```bash [root@iZ0jl6qbv1gs36mzvvl1gaZ ~]# cd /root [root@iZ0jl6qbv1gs36mzvvl1gaZ ~]# ls download.log kubectl llm-k8s-resource llm-k8s-resource.tar.gz llm-model logtail.sh ossutil-2.1.0-linux-amd64 ossutil-2.1.0-linux-amd64.zip [root@iZ0jl6qbv1gs36mzvvl1gaZ ~]# cd llm-k8s-resource/ [root@iZ0jl6qbv1gs36mzvvl1gaZ llm-k8s-resource]# ll total 28 -rw-r--r-- 1 root root 2594 Apr 16 10:04 model.yaml -rw-r--r-- 1 502 games 930 Apr 16 10:04 pre-deploy-application.yaml -rw-r--r-- 1 502 games 426 Apr 16 10:21 private-service.yaml -rw-r--r-- 1 502 games 456 Apr 16 10:21 public-service.yaml -rw-r--r-- 1 502 games 2586 Apr 14 17:30 qwq-application.yaml To deploy QwQ32B, execute: kubectl apply -f /root/llm-k8s-resource/qwq-application.yaml ``` Using the Console Enter the ComputeNest console, click Service Instances , then Resources to find the ACK cluster. Enter its console. In the ACK console, navigate to Workloads > Stateful . For example, view the QwQ-32B Deployment: Click the Deployment to view details, then edit parameters or update YAML directly. Advanced Tutorials Customizing Fluid for Model Acceleration Fluid is a Kubernetes-native engine for orchestrating and accelerating distributed datasets, optimizing performance for data-intensive applications like AI inference and large model training. To accelerate elastic scaling scenarios: Deploy Fluid following the guide: Fluid Documentation . Modify parameters such as BucketName , ModelName , and JindoRuntime settings in the YAML below: yaml apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: llm-model namespace: llm-model spec: placement: Shared mounts: - mountPoint: oss://${BucketName}/llm-model options: fs.oss.endpoint: oss-${RegionId}-internal.aliyuncs.com name: models path: \"/\" encryptOptions: - name: fs.oss.accessKeyId valueFrom: secretKeyRef: name: oss-secret key: akId - name: fs.oss.accessKeySecret valueFrom: secretKeyRef: name: oss-secret key: akSecret --- apiVersion: data.fluid.io/v1alpha1 kind: JindoRuntime metadata: name: llm-model namespace: llm-model spec: networkmode: ContainerNetwork replicas: ${JindoRuntimeReplicas} # set replicas according to the actual model disk usage master: podMetadata: labels: alibabacloud.com/compute-class: performance alibabacloud.com/compute-qos: default worker: podMetadata: labels: alibabacloud.com/compute-class: performance alibabacloud.com/compute-qos: default annotations: kubernetes.io/resource-type: serverless resources: requests: cpu: 16 memory: 128Gi limits: cpu: 16 memory: 128Gi tieredstore: levels: - mediumtype: MEM path: /dev/shm volumeType: emptyDir quota: 128Gi high: \"0.99\" low: \"0.95\" --- apiVersion: data.fluid.io/v1alpha1 kind: DataLoad metadata: name: llm-model namespace: llm-model spec: dataset: name: llm-model namespace: llm-model loadMetadata: true Benchmark Results This service uses VLLM's built-in benchmark tool for testing. The test dataset is available at https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split/files . Benchmark Workflow Create a Deployment using the vllm-benchmark image to download the dataset and run tests: Parameter Description Example/Value $POD_IP Pod IP running deepseek-r1 kubectl get pod -n llm-model -l app=$(kubectl get deployment -n llm-model -l app -o jsonpath='{.items[0].spec.template.metadata.labels.app}') -o jsonpath='{.items[0].status.podIP}' $API_KEY Service authentication key Obtained from service instance details page (format: sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx ) $MODEL_PATH Model storage path QwQ-32b: /llm-model/Qwen/QwQ-32B Qwen3-32b: /llm-model/Qwen/Qwen3-32B Qwen3-235b-A22b: /llm-model/Qwen/Qwen3-235B-A22B DeepSeek-R1_671b: /llm-model/deepseek-ai/DeepSeek-R1 DeepSeek-R1_32b: /llm-model/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1_70b: /llm-model/deepseek-ai/DeepSeek-R1-Distill-Llama-70B $SERVED_MODEL_NAME Deployed model name QwQ-32b: qwq-32b Qwen3-32b: qwen3 Qwen3-235b-A22b: qwen3 DeepSeek-R1_671b: deepseek-r1 DeepSeek-R1_32b: deepseek-r1 DeepSeek-R1_70b: deepseek-r1 ```yaml apiVersion: apps/v1 kind: Deployment metadata: name: vllm-benchmark namespace: llm-model labels: app: vllm-benchmark spec: replicas: 1 selector: matchLabels: app: vllm-benchmark template: metadata: labels: app: vllm-benchmark spec: volumes: - name: llm-model persistentVolumeClaim: claimName: llm-model containers: - name: vllm-benchmark image: kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/vllm-benchmark:v1 command: - \"sh\" - \"-c\" - | # Install dependencies yum install -y epel-release && \\ yum install -y git git-lfs && \\ git lfs install && # Download the dataset git clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git /root/ShareGPT_V3_unfiltered_cleaned_split # Run the benchmark export OPENAI_API_KEY=$API_KEY python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model $MODEL_PATH \\ --served-model-name $SERVED_MODEL_NAME \\ --trust-remote-code \\ --dataset-name sharegpt \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \\ --sonnet-input-len 1024 \\ --sonnet-output-len 6 \\ --sonnet-prefix-len 50 \\ --num-prompts 200 \\ --request-rate 1 \\ --host $POD_IP \\ --port 8000 \\ --endpoint /v1/completions \\ --save-result # Keep the container running sleep inf volumeMounts: - mountPath: /llm-model name: llm-model ``` View logs in the ACK console or directly in the container, this is a Sample Benchmark Results: ```plaintext =========== Serving Benchmark Result ============ Successful requests: 200 Benchmark duration (s): 272.15 Total input tokens: 43390 Total generated tokens: 39980 Request throughput (req/s): 0.73 Output token throughput (tok/s): 146.91 Total Token throughput (tok/s): 306.34 ---------------Time to First Token---------------- Mean TTFT (ms): 246.46 Median TTFT (ms): 244.58 P99 TTFT (ms): 342.11 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 130.30 Median TPOT (ms): 130.12 P99 TPOT (ms): 139.09 ---------------Inter-token Latency---------------- Mean ITL (ms): 129.89 Median ITL (ms): 125.40 P99 ITL (ms): 173.20 =================================================","title":"vllm Large Model Deployment Guide on ACK Cluster"},{"location":"en/index-acs-en/#vllm-large-model-deployment-guide-on-ack-cluster","text":"","title":"vllm Large Model Deployment Guide on ACK Cluster"},{"location":"en/index-acs-en/#deployment-overview","text":"This solution provides an out-of-the-box deployment of high-performance large model inference services using Alibaba Cloud ComputeNest. It is based on the following core components: VLLM : Provides high-performance parallel inference capabilities, supporting low-latency and high-throughput LLM inference (e.g., Qwen, DeepSeek, etc.). ACK Cluster : A managed Kubernetes environment supporting Serverless workloads. After deployment, users can invoke model services via private/public APIs. Resource utilization is improved by several times, and developers need not manage underlying container orchestration or resource scheduling\u2014only selecting the model on the ComputeNest console is required for one-click deployment. The service supports various models and GPU types during deployment, including: QwQ32B DeepSeek-R1-Distill-Qwen-32B , GPU: P16EN DeepSeek-R1-Distill-Llama-70B , GPU: P16EN Deepseek Full-Blooded Version (671B, fp8) , GPU: GU8TF Deepseek Full-Blooded Version (671B, fp8) , GPU: P16EN","title":"Deployment Overview"},{"location":"en/index-acs-en/#architecture-overview","text":"","title":"Architecture Overview"},{"location":"en/index-acs-en/#cost-explanation","text":"The costs for this service on Alibaba Cloud primarily include: ACK Cluster Fees Jump Server (ECS) Fees Notes: This ECS instance is used to deploy and manage the K8s cluster. The /root directory contains the K8s YAML resource files used for deployment. Parameters can be modified and re-deployed directly afterward. The ECS can be released after deployment if no longer needed. OSS Fees Billing Method : Pay-as-you-go (hourly) or subscription (prepaid). Estimated costs are visible in real-time during instance creation.","title":"Cost Explanation"},{"location":"en/index-acs-en/#required-ram-account-permissions","text":"The account deploying the instance requires permissions to access and manage Alibaba Cloud resources. The following policies must be included: Permission Policy Name Notes AliyunECSFullAccess Permissions to manage ECS (Elastic Compute Service) AliyunVPCFullAccess Permissions to manage VPC (Virtual Private Cloud) AliyunROSFullAccess Permissions to manage ROS (Resource Orchestration Service) AliyunCSFullAccess Permissions to manage ACK (Container Service) AliyunComputeNestUserFullAccess Permissions to manage ComputeNest (user-side) AliyunOSSFullAccess Permissions to manage OSS (Object Storage Service) Important : Contact PDSA to add GPU to your whitelist before deployment.","title":"Required RAM Account Permissions"},{"location":"en/index-acs-en/#deployment-steps","text":"Click the * Deployment Link . Follow the interface prompts to fill in parameters and view cost estimates. Confirm parameters and click Next: Confirm Order *. Click Next: Confirm Order to preview costs. Then click Deploy Now and wait for completion. After deployment, access the service. Navigate to the instance details to view private network access instructions. If \"Support Public Network Access\" was selected, public access instructions will also be available.","title":"Deployment Steps"},{"location":"en/index-acs-en/#usage-guide","text":"","title":"Usage Guide"},{"location":"en/index-acs-en/#private-network-api-access","text":"Access the Private API address from an ECS instance within the same VPC. Example: shell # Private API request with authentication and streaming (remove \"stream\" to disable streaming) curl http://$PrivateIP:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $API_KEY\" \\ -d '{ \"model\": \"ds\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to your daughter from the future (2035), encouraging her to study science and technology to become a leader in this field. She is currently in 3rd grade.\" } ], \"max_tokens\": 1024, \"temperature\": 0, \"top_p\": 0.9, \"seed\": 10, \"stream\": true }'","title":"Private Network API Access"},{"location":"en/index-acs-en/#public-network-api-access","text":"If \"Support Public Network Access\" was selected during deployment, use the public IP directly: curl http://$PublicIp:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"ds\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to your daughter from the future (2035), encouraging her to study science and technology to become a leader in this field. She is currently in 3rd grade.\" } ], \"max_tokens\": 1024, \"temperature\": 0, \"top_p\": 0.9, \"seed\": 10, \"stream\": true }' If public access was not enabled, manually create a LoadBalancer in the cluster. Example (for DeepSeek-R1; adjust labels.app for QwQ-32B): apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type: \"internet\" service.beta.kubernetes.io/alibaba-cloud-loadbalancer-ip-version: ipv4 labels: app: deepseek-r1 name: svc-public namespace: llm-model spec: externalTrafficPolicy: Local ports: - name: serving port: 8000 protocol: TCP targetPort: 8000 selector: app: deepseek-r1 type: LoadBalancer","title":"Public Network API Access"},{"location":"en/index-acs-en/#re-deploying-models","text":"Models can be re-deployed using kubectl apply on the jump server or by manually updating templates in the console.","title":"Re-deploying Models"},{"location":"en/index-acs-en/#using-the-jump-server","text":"In the ComputeNest console's instance resources page, locate the ECS jump server and connect via **Remote Connection ** (password-free login). Execute commands on the jump server: ```bash [root@iZ0jl6qbv1gs36mzvvl1gaZ ~]# cd /root [root@iZ0jl6qbv1gs36mzvvl1gaZ ~]# ls download.log kubectl llm-k8s-resource llm-k8s-resource.tar.gz llm-model logtail.sh ossutil-2.1.0-linux-amd64 ossutil-2.1.0-linux-amd64.zip [root@iZ0jl6qbv1gs36mzvvl1gaZ ~]# cd llm-k8s-resource/ [root@iZ0jl6qbv1gs36mzvvl1gaZ llm-k8s-resource]# ll total 28 -rw-r--r-- 1 root root 2594 Apr 16 10:04 model.yaml -rw-r--r-- 1 502 games 930 Apr 16 10:04 pre-deploy-application.yaml -rw-r--r-- 1 502 games 426 Apr 16 10:21 private-service.yaml -rw-r--r-- 1 502 games 456 Apr 16 10:21 public-service.yaml -rw-r--r-- 1 502 games 2586 Apr 14 17:30 qwq-application.yaml","title":"Using the Jump Server"},{"location":"en/index-acs-en/#to-deploy-qwq32b-execute","text":"kubectl apply -f /root/llm-k8s-resource/qwq-application.yaml ```","title":"To deploy QwQ32B, execute:"},{"location":"en/index-acs-en/#using-the-console","text":"Enter the ComputeNest console, click Service Instances , then Resources to find the ACK cluster. Enter its console. In the ACK console, navigate to Workloads > Stateful . For example, view the QwQ-32B Deployment: Click the Deployment to view details, then edit parameters or update YAML directly.","title":"Using the Console"},{"location":"en/index-acs-en/#advanced-tutorials","text":"","title":"Advanced Tutorials"},{"location":"en/index-acs-en/#customizing-fluid-for-model-acceleration","text":"Fluid is a Kubernetes-native engine for orchestrating and accelerating distributed datasets, optimizing performance for data-intensive applications like AI inference and large model training. To accelerate elastic scaling scenarios: Deploy Fluid following the guide: Fluid Documentation . Modify parameters such as BucketName , ModelName , and JindoRuntime settings in the YAML below: yaml apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: llm-model namespace: llm-model spec: placement: Shared mounts: - mountPoint: oss://${BucketName}/llm-model options: fs.oss.endpoint: oss-${RegionId}-internal.aliyuncs.com name: models path: \"/\" encryptOptions: - name: fs.oss.accessKeyId valueFrom: secretKeyRef: name: oss-secret key: akId - name: fs.oss.accessKeySecret valueFrom: secretKeyRef: name: oss-secret key: akSecret --- apiVersion: data.fluid.io/v1alpha1 kind: JindoRuntime metadata: name: llm-model namespace: llm-model spec: networkmode: ContainerNetwork replicas: ${JindoRuntimeReplicas} # set replicas according to the actual model disk usage master: podMetadata: labels: alibabacloud.com/compute-class: performance alibabacloud.com/compute-qos: default worker: podMetadata: labels: alibabacloud.com/compute-class: performance alibabacloud.com/compute-qos: default annotations: kubernetes.io/resource-type: serverless resources: requests: cpu: 16 memory: 128Gi limits: cpu: 16 memory: 128Gi tieredstore: levels: - mediumtype: MEM path: /dev/shm volumeType: emptyDir quota: 128Gi high: \"0.99\" low: \"0.95\" --- apiVersion: data.fluid.io/v1alpha1 kind: DataLoad metadata: name: llm-model namespace: llm-model spec: dataset: name: llm-model namespace: llm-model loadMetadata: true","title":"Customizing Fluid for Model Acceleration"},{"location":"en/index-acs-en/#benchmark-results","text":"This service uses VLLM's built-in benchmark tool for testing. The test dataset is available at https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split/files .","title":"Benchmark Results"},{"location":"en/index-acs-en/#benchmark-workflow","text":"Create a Deployment using the vllm-benchmark image to download the dataset and run tests: Parameter Description Example/Value $POD_IP Pod IP running deepseek-r1 kubectl get pod -n llm-model -l app=$(kubectl get deployment -n llm-model -l app -o jsonpath='{.items[0].spec.template.metadata.labels.app}') -o jsonpath='{.items[0].status.podIP}' $API_KEY Service authentication key Obtained from service instance details page (format: sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx ) $MODEL_PATH Model storage path QwQ-32b: /llm-model/Qwen/QwQ-32B Qwen3-32b: /llm-model/Qwen/Qwen3-32B Qwen3-235b-A22b: /llm-model/Qwen/Qwen3-235B-A22B DeepSeek-R1_671b: /llm-model/deepseek-ai/DeepSeek-R1 DeepSeek-R1_32b: /llm-model/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1_70b: /llm-model/deepseek-ai/DeepSeek-R1-Distill-Llama-70B $SERVED_MODEL_NAME Deployed model name QwQ-32b: qwq-32b Qwen3-32b: qwen3 Qwen3-235b-A22b: qwen3 DeepSeek-R1_671b: deepseek-r1 DeepSeek-R1_32b: deepseek-r1 DeepSeek-R1_70b: deepseek-r1 ```yaml apiVersion: apps/v1 kind: Deployment metadata: name: vllm-benchmark namespace: llm-model labels: app: vllm-benchmark spec: replicas: 1 selector: matchLabels: app: vllm-benchmark template: metadata: labels: app: vllm-benchmark spec: volumes: - name: llm-model persistentVolumeClaim: claimName: llm-model containers: - name: vllm-benchmark image: kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/vllm-benchmark:v1 command: - \"sh\" - \"-c\" - | # Install dependencies yum install -y epel-release && \\ yum install -y git git-lfs && \\ git lfs install && # Download the dataset git clone https://www.modelscope.cn/datasets/gliang1001/ShareGPT_V3_unfiltered_cleaned_split.git /root/ShareGPT_V3_unfiltered_cleaned_split # Run the benchmark export OPENAI_API_KEY=$API_KEY python3 /root/vllm/benchmarks/benchmark_serving.py \\ --backend vllm \\ --model $MODEL_PATH \\ --served-model-name $SERVED_MODEL_NAME \\ --trust-remote-code \\ --dataset-name sharegpt \\ --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json \\ --sonnet-input-len 1024 \\ --sonnet-output-len 6 \\ --sonnet-prefix-len 50 \\ --num-prompts 200 \\ --request-rate 1 \\ --host $POD_IP \\ --port 8000 \\ --endpoint /v1/completions \\ --save-result # Keep the container running sleep inf volumeMounts: - mountPath: /llm-model name: llm-model ``` View logs in the ACK console or directly in the container, this is a Sample Benchmark Results: ```plaintext =========== Serving Benchmark Result ============ Successful requests: 200 Benchmark duration (s): 272.15 Total input tokens: 43390 Total generated tokens: 39980 Request throughput (req/s): 0.73 Output token throughput (tok/s): 146.91 Total Token throughput (tok/s): 306.34 ---------------Time to First Token---------------- Mean TTFT (ms): 246.46 Median TTFT (ms): 244.58 P99 TTFT (ms): 342.11 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 130.30 Median TPOT (ms): 130.12 P99 TPOT (ms): 139.09 ---------------Inter-token Latency---------------- Mean ITL (ms): 129.89 Median ITL (ms): 125.40 P99 ITL (ms): 173.20 =================================================","title":"Benchmark Workflow"}]}